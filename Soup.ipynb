{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tei_doc = \"../papers-xmls/Learning_to_Optimize_Combinatorial_Functions.tei.xml\"\n",
    "with open(tei_doc, 'r') as tei:\n",
    "    soup = BeautifulSoup(tei, 'lxml')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title level=\"a\" type=\"main\">Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis</title>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learning a Lexicon and Translation Model from Phoneme Lattices'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<abstract>\n",
       "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We consider the task of fine-grained sentiment analysis from the perspective of multiple instance learning (MIL). Our neural model is trained on document sentiment labels, and learns to predict the sentiment of text segments, i.e. sentences or elementary discourse units (EDUs), without segment-level supervision. We introduce an attention-based polarity scoring method for identifying positive and negative text snippets and a new dataset which we call SPOT (as shorthand for Segment-level POlariTy annotations) for evaluating MILstyle sentiment models like ours. Experimental results demonstrate superior performance against multiple baselines, whereas a judgement elicitation study shows that EDU-level opinion extraction produces more informative summaries than sentence-based alternatives.</p></div>\n",
       "</abstract>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We consider the task of fine-grained sentiment analysis from the perspective of multiple instance learning (MIL). Our neural model is trained on document sentiment labels, and learns to predict the sentiment of text segments, i.e. sentences or elementary discourse units (EDUs), without segment-level supervision. We introduce an attention-based polarity scoring method for identifying positive and negative text snippets and a new dataset which we call SPOT (as shorthand for Segment-level POlariTy annotations) for evaluating MILstyle sentiment models like ours. Experimental results demonstrate superior performance against multiple baselines, whereas a judgement elicitation study shows that EDU-level opinion extraction produces more informative summaries than sentence-based alternatives.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.abstract.getText(separator=' ', strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False, False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_text = soup.abstract.getText(separator=' ', strip=True)\n",
    "'movement' in abstract_text.lower(), 'ecology' in abstract_text.lower(), 'computer' in abstract_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tei(tei_file):\n",
    "    with open(tei_file, 'r') as tei:\n",
    "        soup = BeautifulSoup(tei, 'lxml')\n",
    "        return soup\n",
    "    raise RuntimeError('Cannot generate a soup from the input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elem_to_text(elem, default=''):\n",
    "    if elem:\n",
    "        return elem.getText()\n",
    "    else:\n",
    "        return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alan Turing authored many influential publications in computer science.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Person:\n",
    "    firstname: str\n",
    "    middlename: str\n",
    "    surname: str\n",
    "\n",
    "turing_author = Person(firstname='Alan', middlename='M', surname='Turing')\n",
    "\n",
    "f\"{turing_author.firstname} {turing_author.surname} authored many influential publications in computer science.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEIFile(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.soup = read_tei(filename)\n",
    "        self._text = None\n",
    "        self._title = ''\n",
    "        self._abstract = ''\n",
    "\n",
    "    @property\n",
    "    def doi(self):\n",
    "        idno_elem = self.soup.find('idno', type='DOI')\n",
    "        if not idno_elem:\n",
    "            return ''\n",
    "        else:\n",
    "            return idno_elem.getText()\n",
    "\n",
    "    @property\n",
    "    def title(self):\n",
    "        if not self._title:\n",
    "            self._title = self.soup.title.getText()\n",
    "        return self._title\n",
    "\n",
    "    @property\n",
    "    def abstract(self):\n",
    "        if not self._abstract:\n",
    "            abstract = self.soup.abstract.getText(separator=' ', strip=True)\n",
    "            self._abstract = abstract\n",
    "        return self._abstract\n",
    "\n",
    "    @property\n",
    "    def authors(self):\n",
    "        authors_in_header = self.soup.analytic.find_all('author')\n",
    "\n",
    "        result = []\n",
    "        for author in authors_in_header:\n",
    "            persname = author.persname\n",
    "            if not persname:\n",
    "                continue\n",
    "            firstname = elem_to_text(persname.find(\"forename\", type=\"first\"))\n",
    "            middlename = elem_to_text(persname.find(\"forename\", type=\"middle\"))\n",
    "            surname = elem_to_text(persname.surname)\n",
    "            person = Person(firstname, middlename, surname)\n",
    "            result.append(person)\n",
    "        return result\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        divs_text = []\n",
    "        if not self._text:\n",
    "            \n",
    "            for div in self.soup.body.find_all(\"div\")[1:]:\n",
    "                # div is neither an appendix nor references, just plain text.\n",
    "                if not div.get(\"type\"):\n",
    "                    div_text = div.get_text(separator=' ', strip=True)\n",
    "                    divs_text.append(div_text)\n",
    "\n",
    "            plain_text = \" \".join(divs_text)\n",
    "            self._text = divs_text\n",
    "        return self._text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The authors of the paper entitled 'Attention Is All You Need' are [Person(firstname='Ashish', middlename='', surname='Vaswani'), Person(firstname='Google', middlename='', surname='Brain'), Person(firstname='Noam', middlename='', surname='Shazeer'), Person(firstname='Google', middlename='', surname='Brain'), Person(firstname='Niki', middlename='', surname='Parmar'), Person(firstname='Jakob', middlename='', surname='Uszkoreit'), Person(firstname='Llion', middlename='', surname='Jones'), Person(firstname='Aidan', middlename='N', surname='Gomez'), Person(firstname='Łukasz', middlename='', surname='Kaiser'), Person(firstname='Google', middlename='', surname='Brain'), Person(firstname='Illia', middlename='', surname='Polosukhin')]\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tei = TEIFile('sample.tei.xml')\n",
    "f\"The authors of the paper entitled '{tei.title}' are {tei.authors}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Introduction Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5] . Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13] . Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h t , as a function of the previous hidden state h t−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [18] and conditional computation [26] , while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16] . In all but a few cases [22] , however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.',\n",
       " 'Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20] , ByteNet [15] and ConvS2S [8] , all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [11] . In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19] . End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28] . To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8] .',\n",
       " 'Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29] . Here, the encoder maps an input sequence of symbol representations (x 1 , ..., x n ) to a sequence of continuous representations z = (z 1 , ..., z n ). Given z, the decoder then generates an output sequence (y 1 , ..., y m ) of symbols one element at a time. At each step the model is auto-regressive [9] , consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1 , respectively.',\n",
       " 'Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [10] around each of the two sub-layers, followed by layer normalization [1] . That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d model = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.',\n",
       " 'Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. query with all keys, divide each by √ d k , and apply a softmax function to obtain the weights on the values.',\n",
       " 'Scaled Dot-Product Attention In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as: Attention(Q, K, V ) = softmax( QK T √ d k )V (1) The two most commonly used attention functions are additive attention [2] , and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1 √ d k . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of d k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d k [3] . We suspect that for large values of d k , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4 . To counteract this effect, we scale the dot products by 1 √ d k .',\n",
       " 'Multi-Head Attention Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k , d k and d v dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2 . Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. MultiHead(Q, K, V ) = Concat(head 1 , ..., head h )W O where head i = Attention(QW Q i , KW K i , V W V i ) Where the projections are parameter matrices W Q i ∈ R dmodel×d k , W K i ∈ R dmodel×d k , W V i ∈ R dmodel×dv and W O ∈ R hdv×dmodel . In this work we employ h = 8 parallel attention layers, or heads. For each of these we use d k = d v = d model /h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.',\n",
       " 'Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8 ].',\n",
       " 'Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d model = 512, and the inner-layer has dimensionality d f f = 2048.',\n",
       " 'Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model . We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [24] . In the embedding layers, we multiply those weights by √ d model .',\n",
       " 'Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the Table 1 : Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n 2 · d) O(1) O(1) Recurrent O(n · d 2 ) O(n) O(n) Convolutional O(k · n · d 2 ) O(1) O(log k (n)) Self-Attention (restricted) O(r · n · d) O(1) O(n/r) bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [8] . In this work, we use sine and cosine functions of different frequencies: where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P E pos+k can be represented as a linear function of P E pos . We also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.',\n",
       " 'Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1 , ..., x n ) to another sequence of equal length (z 1 , ..., z n ), with x i , z i ∈ R d , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [11] . Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1 , a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(log k (n)) in the case of dilated convolutions [15] , increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6] , however, decrease the complexity considerably, to O(k · n · d + n · d 2 ). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.',\n",
       " 'Training This section describes the training regime for our models.',\n",
       " 'Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3] , which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [31] . Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.',\n",
       " 'Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, (described on the  bottom line of table 3) , step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).',\n",
       " 'Optimizer We used the Adam optimizer [17] with β 1 = 0.9, β 2 = 0.98 and = 10 −9 . We varied the learning rate over the course of training, according to the formula: lrate = d −0.5 model · min(step_num −0.5 , step_num · warmup_steps −1.5 ) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.',\n",
       " 'Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0.1. Label Smoothing During training, we employed label smoothing of value ls = 0.1 [30] . This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.',\n",
       " 'Results',\n",
       " 'Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2 ) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3 . Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P drop = 0.1, instead of 0.3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [31] . These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [31] .',\n",
       " 'Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3 . In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size d k hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [8] , and observe nearly identical results to the base model.',\n",
       " 'Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.',\n",
       " 'Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tei.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_datas = [head.get_text() for head in soup.find_all('head')]\n",
    "head_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div xmlns=\"http://www.tei-c.org/ns/1.0\">Background<p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU <ref target=\"#b19\" type=\"bibr\">[20]</ref>, ByteNet <ref target=\"#b14\" type=\"bibr\">[15]</ref> and ConvS2S <ref target=\"#b7\" type=\"bibr\">[8]</ref>, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions <ref target=\"#b10\" type=\"bibr\">[11]</ref>. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.</p><p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations <ref target=\"#b3\" type=\"bibr\">[4,</ref><ref target=\"#b21\" type=\"bibr\">22,</ref><ref target=\"#b22\" type=\"bibr\">23,</ref><ref target=\"#b18\" type=\"bibr\">19]</ref>.</p><p>End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks <ref target=\"#b27\" type=\"bibr\">[28]</ref>.</p><p>To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as <ref target=\"#b13\" type=\"bibr\">[14,</ref><ref target=\"#b14\" type=\"bibr\">15]</ref> and <ref target=\"#b7\" type=\"bibr\">[8]</ref>.</p></div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('div')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs_text=[]\n",
    "for div in soup.body.find_all(\"div\")[1:]:\n",
    "    # div is neither an appendix nor references, just plain text.\n",
    "    if not div.get(\"type\"):\n",
    "        div_text = div.get_text(separator=' ', strip=True)\n",
    "        divs_text.append(div_text)\n",
    "        #rint(divs_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Acknowledgements'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_text.split(' ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Introduction Combinatorial optimization aims to optimize an objective function over a set of feasible solutions defined on a discrete space. Numerous real-life decision-making problems can be formulated as combinatorial optimization problems (Korte et al. 2012; Trevisan 2011) . In the last decade, development of time-efficient algorithms for combinatorial optimization problems paved the way for these algorithms to be widely utilized in industry, including, but not limited to, in resource allocation (Angalakudati et al. 2014 ), efficient energy scheduling (Ngueveu, Artigues, and Lopez 2016), price optimization (Ferreira, Lee, and Simchi-Levi 2015) , sales promotion planning (Cohen et al. 2017) , etc. The last decade has, in parallel, witnessed a tremendous growth in machine learning (ML) methods, which can produce very accurate predictions by leveraging historical and contextual data. In real-world applications, not all parameters of an optimization problem are known at the time of Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. execution and predictive ML models can be used for estimation of those parameters from historical data. For instance, Cohen et al. first predicted future demand of products using an ML model and then use the predicted demand to compute the optimal promotion pricing scheme over the products through non-linear integer programming. When predictive ML is followed by optimization, it is often assumed that improvements in the quality of the predictions (with respect to some suitable evaluation metric) will result in better optimization outcomes. However, ML models make errors and the impact of prediction errors is not uniform throughout the underlying solution space, for example, overestimating the highest-valued prediction might not change a maximization problem outcome, while underestimating it can. Hence, a better prediction model may not ensure a better outcome in the optimization stage. In this regard, Ifrim, OSullivan, and Simonis (2012) experienced that a better predictive model does not always translate to optimized energy-saving schedules. The alternative is to take the effect of the errors on the optimization outcome into account during learning. In the context of linear programming problems, Elmachtoub and Grigas proposed an approach, called \"Smart Predict and Optimize\" (SPO), for training ML models by minimizing a convex surrogate loss function which considers the outcome of the optimization stage. Specifically they consider optimization problems where predictions occur as weights that are linear in the objective. In this work, we build on that approach and consider discrete combinatorial optimization problems. Indeed, the SPO loss is valid for any optimization problem with a linear objective over predictions, where the constraints implicitly define a convex region. Furthermore, any black box optimization method can be used as only its outcome is used to compute the (sub)gradient of the loss. The main challenge is the computational cost of the repeated solving of the optimization problem during training, namely once for every evaluation of the loss function on an instance. For NP-hard problems, this may quickly become infeasible. In order to scale up to large problem instances, we investigate the importance of finding the optimal discrete solution for learning, showing that continuous relax-ations are highly informative for learning. Furthermore, we investigate how to speed up the learning by transfer learning from easier-to-learn models as well as method for speeding up the solving by warmstarting from earlier solutions. Our approach outperforms the state-of-the-art Melding approach (Wilder, Dilkina, and Tambe 2019) in most cases, and for the first time we are able to show the applicability of predict-and-optimize on large scale combinatorial instances, namely from the ICON energy-cost aware scheduling challenge (Simonis et al. ).',\n",
       " 'Related Work Predict-and-optimize problems arise in many applications. Current practice is to use a two-stage approach where the ML models are trained independent of the optimization problem. As a consequence, the ML models do not account for the optimization tasks (Wang et al. 2006; Mukhopadhyay et al. 2017) . In recent years there is a growing interest in decision-focused learning (Elmachtoub and Grigas 2017; Demirović et al. 2019; , that aims to couple ML and decision making. In the context of portfolio optimization, Bengio (1997) report a deep learning model fails to improve future profit when trained with respect to a standard ML loss function, but a profit-driven loss function turns out to be effective. Kao, Roy, and Yan (2009) consider an unconstrained quadratic optimisation problem, where the predicted values appear linearly with the objective. They train a linear model with respect to a combination of prediction error and optimization loss. They do not mention how this can be applied to optimization problems with constraints. A number of works aim to exploit the fact that the KKT conditions of a quadratic program (QP) define a system of linear equations around the optimal points. For instance, Donti, Amos, and Kolter (2017) propose a framework which computes the gradient of the solution of the QP with respect to the predictions by applying the implicit function theorem to differentiate through the KKT conditions around the optimal point. Wilder, Dilkina, and Tambe (2019) use the same approach, and propose its use for linear programs by adding a small quadratic term to convert it into a concave QP. They also propose a specialisation of it for submodular functions. Our work builds on the SPO approach of Elmachtoub and Grigas (2017) , where the authors provide a framework to train an ML model, which learns with respect to the error in the optimization problem. This is investigated for linear optimization problems with a convex feasible region. We will use the approach for discrete combinatorial problems with a linear objective. They are computationally expensive to solve, e.g. often N P-hard. The decision variables and search space of these problems is discrete, meaning gradients can not be computed in a straightforward manner. However, the SPO approach remains applicable as we will see. Demirović et al. (2019) investigate the predic-tion+optimisation problem for the knapsack problem, and prove that optimizing over predictions are as valid as stochastic optimisation over learned distributions, in case the predictions are used as weights in a linear objective. They further investigate possible learning approaches, and classified them into three groups: indirect approaches, which do not use knowledge of the optimisation problem; semi-direct approaches, which encode knowledge of the optimisation problem, such as the importance of ranking and direct approaches which encode or use the optimisation problem in the learning in some way (Demirović et al. 2019) . Our approach is a direct approach and we examine how to combine the best of such techniques in order to scale to large and hard combinatorial problems.',\n",
       " \"Problem Formulation and Approach Optimizing a parameterized problem Traditional optimization algorithms work under the assumption that all the parameters are known precisely. But in a predict-and-optimize setup we assume some parameters of the optimization problem are not known. We formalize a combinatorial optimization problem as follows : v * (θ) ≡ arg min v f (v, θ) s.t. C(v, θ) (1) where θ defines the set of parameters (coefficients) of the optimization problem, v are the decision variables, f (v, θ) is an objective to be minimized, and C(v, θ) is a (set of) constraints that determine(s) the feasible region; hence v * () is an oracle that returns the optimal solution. Consider, the 0-1 knapsack, where a set of items, with their values and wights, are provided. The objective is to select a subset of items respecting a capacity constraint on the sum of weights so that the total value of the subset is maximized. The parameter set θ of the problem consists of the value and weight of each item and the total capacity. The decision variable set consists of 0-1 decision variable for each item, f is a linear sum of the variables and the item values and C describing the capacity constraint. We decompose θ = θ s ∪ θ u where θ s are the set of parameters that are observed (e.g. the weights and capacity of a knapsack) and θ u are the set of unobserved parameters (the value of the knapsack items). To predict the unobserved parameters θ u , some attributes correlated with them are observed. We are equipped with a training dataset D : {(x 1 , θ u1 ), ..., (x n , θ un )} where x i 's are vectors of attributes correlated to θ ui . An ML model m is trained to generate a predictionθ u = m(x; ω). The model m is characterized by a set of learning parameters ω. E.g. in linear regression the ω encompasses the slope and the intercept of the regression line. Once the predictedθ u are obtained, θ = θ s ∪θ u is used for the optimization problem. To ease notation, we will writeθ = (θ s ,θ u ), containing both the observed parameters and predicted parameters. Recall from Equation (1) that v * (θ) is the optimal solution using parametersθ. Then, the objective value of this solution is f (v * (θ), θ). Whereas if the actual θ is known a priori, one could obtain the actual optimal solution v * (θ). The difference in using the predicted instead of the actual values is hence measured by regret(θ,θ) ≡ f (v * (θ), θ) − f (v * (θ), θ) (2) Ideally the aim of the training should be to generate predictions which minimize this regret on unseen data.\",\n",
       " 'Two Stage Learning First we formalize a two-stage approach, which is widely used in industry and where the prediction and the optimization are performed in a decoupled manner. First the predictive model is trained with the objective of minimizing the expected loss for a suitable choice of loss function L(θ u ,θ u ) For regression, if we use squared-error as the loss function, model parameters ω are estimated by minimizing the Mean Squared Error (MSE) on the training data: L M SE = 1 n n i=1 (θ ui −θ ui ) 2 2 (3) Algorithm 1: Stochastic Batch gradient descent for the two-stage learning for regression tasks (batchsize:N ) and learning rate α repeat Sample N training datapoints for i in 1, ..., N do predictθ ui using current ω ∇L i ← (θ ui − θ ui ) //gradient of L M SE end ∇L = N i=1 ∇Li N ω ← ω − α * ∇L * ∂θu ∂ω until convergence; Training by gradient descent The process of estimating ω to minimize the loss function is executed through stochastic gradient descent algorithms 1 ,where at each epoch, ω are updated after calculating the gradient ∇L of the loss function with respect to the predictions as shown in Algorithm 1. The advantage of the two-stage approach is that training by gradient descent is straightforward. In the prediction stage the objective is to minimize the MSE-loss to generate accurate predictions without considering their impact on the final solution. Clearly this does not require solving the optimization problem during training. Model validation and early stopping with regret It is common practice to perform model validation on a separate validation set while training the model. Early stopping (Bishop 2006) is the practice of choosing the epoch with the smallest loss on the validation set as the final output of the training procedure. The objective is to avoid overfitting on training data. The performance on the validation set is also used to select hyperparameters of the ML models (Bergstra and Bengio 2012) . Considering the final task, in our setup, is minimizing the regret, we modify the two stage learning by measuring regret on the validation set for early stopping and hyperparameter selection. We call this the MSE-r approach. It is more computationally expensive than MSE given that computing regret on the validation data for every epoch requires solving the optimization problems each time.',\n",
       " 'Smart Predict then Optimize (SPO) The major drawback of the two-stage approach is it does not aim to minimize the regret, but minimizes the error between θ u andθ u directly. As Figure 1 shows, minimizing loss between θ u andθ u does not necessarily result in minimization of the regret. Early stopping with regret can avoid worsening results, but can not improve the learning. The SPO framework proposed by Elmachtoub and Grigas addresses this by integrating prediction and optimization. Note, to minimize regret(θ,θ) directly we have to find the gradient of it with respect toθ which requires differentiating the argmin operator v * (θ) in Eq. 1. This differentiation may not be feasible as v * (θ) can be discontinuous inθ and exponential in size. Consequently we can not train an ML model to minimize the regret through gradient descent. The SPO framework integrates the optimization problem into the loop of gradient descent algorithms in a clever way. In their work, Elmachtoub and Grigas consider an optimization problem with a convex feasible region S and a linear objective : v * (θ) ≡ arg min v∈S θ v (4) where the cost vector θ is not known beforehand. Following Eq. (2), the regret for such a problem when using predicted valuesθ instead of actual θ is: θ (v * (θ) − v * (θ)) , which as discussed is not differentiable. To make it differentiable, they use a convex surrogate upper bound of the regret function, which they name the SPO+ loss function L SP O+ (θ,θ). The gradient of L SP O+ (θ,θ) may not exist as it also involves the argmin operator. However, they have shown that v * (θ) − v * (2θ − θ) is a subgradient of L SP O+ (θ,θ), that is g(θ,θ) = v * (θ) − v * (2θ − θ), g(θ,θ) ∈ ∇L SP O+ (θ,θ) (5) The subgadient formulation is the key to bring the optimization problem into the loop of gradient descent as shown in algorithm 2. The difference between algorithm 1 and algorithm 2 is in their (sub)gradients. In Algorithm 1, the MSE gradient is the signed difference between the actual values Algorithm 2: Stochastic Batch gradient descent for the SPO approach for regression tasks (batchsize:N ) and learning rate α repeat Sample N training datapoints for i in 1, ..., N do predictθ ui using current ω compute v * (2θ − θ) ∇L i ← v * (θ) − v * (2θ − θ) //sub-gradient end ∇L = N i=1 ∇Li N ω ← ω − α * ∇L * ∂θu ∂ω ; until convergence; and predicted ones; in Algorithm 2 the SPO subgradient is the difference of an optimization solution obtained using the actual parameter values and another solution obtained using a convex combination of the predicted values and the true values. For the 0-1 knapsack problem, the solution of a knapsack instance is a 0-1 vector of length equal to the size of the set, where 1 represents the corresponding item is selected. In this case, the subgradient is the element-wise difference between the two solutions and if the solution using the transformed predicted values is the same as the solution using actual values, all entries of the subgradient are zero. In essence, the non-zero entries in the subgradient indicate places where the two solutions contradict. Note, to compute the subgradient for this SPO approach, the optimization problem v * (2θ − θ) needs to be solved for each training instance, while v * (θ) can be precomputed and cached. Moreover, one training instance typically contains multiple predictions. For example, if we consider a 0-1 knapsack problem with 10 items, then one training instance always contains 10 value predictions, one for each item. Furthermore, the dataset may contain thousands of training instances of 10 values each. Hence, one iteration over the training data (one epoch) requires solving hundreds of knapsack problems. As an ML model is trained over several epochs, clearly the training process is computationally expensive.',\n",
       " 'Combinatorial problems and scaling up We observe that the SPO approach and its corresponding loss function places no restriction on the type of oracle v * () used. Given that our target task is to minimize the regret of the combinatorial problem, an oracle that solves the combinatorial optimisation problem is the most natural choice. We call this approach SPO-full. Weaker oracles Repeatedly solving combinatorial problems is computationally expensive. Hence for large and hard problems, it is necessary to look at ways to reduce the solving time. As there is no restriction on the oracle used, we consider using weaker oracles during training. NP-hard problems that have a polynomial (bounded) approximation algorithm could use the approximation algorithm in the loss as a proxy instead. For example, in case of knapsack, the greedy algorithm (Dantzig 1957) . For mixed integer programming (MIP) formulations, a natural weaker oracle to use is the continuous relaxation of the problem. While disregarding the discrete part, relaxations can often identify what part of the problem is trivial (variable assignments close to 0 or 1) from what part is non-trivial. For example for knapsack, the continuous relaxation leads to very similar solutions compared to the greedy algorithm. Note that we always use the same oracle for v * (θ) and v * (2θ − θ) when computing the loss. We call the approach of using the continuous relaxation as oracle v * () SPO-relax. In case of weak MIP relaxations, one can also use a cutting plane algorithm in the root node and use the resulting tighter relaxation thereof (Ferber et al. 2019) . Other weaker oracles could also be used, for example setting a time-limit on an any-time solver and using the best solution found, or a node-limit on search algorithms. In case of mixed integer programming, we can also set a gap tolerance, which means the solver does not have to prove optimality. We call this SPO-gap. For stability of the learning, it is recommended that the solution returned by the oracle does not vary much when called with (near) identical input. Apart from changing what is being solved, we also investigate ways to warmstart the learning, and to warmstart across solver calls: Warmstarting the learning We consider warmstarting the learning by transfer learning (Pratt and Jennings 1996) , that is, to train the model with an easy to compute loss function, and then continue training it with a more difficult one. In our case, we can pre-train the model using MSE as loss, which means the predictions will already be more informed when we start using an SPO loss afterwards. More elaborate learning schemes are possible, such as curriculum learning (Thrun and Pratt 2012; Pratt and Jennings 1996) where we gradually move from easier to harder to compute loss functions, e.g. by moving from SPO-relax to SPO-gap for decreasing gaps to SPO-full. As we will see later, this is not needed for the cases we studied. Warmstarting the solving When computing the loss, we must solve both v * (θ) using the true values θ, and v * (2θ−θ). Furthermore, we know that an optimal solution to v * (θ) is also a valid (but potentially suboptimal) solution to v * (2θ − θ) as only the coefficients of the objective differ. Furthermore, if this is an optimal solution to the latter than we would achieve 0-regret, hence we can expect the solution of v * (θ) to be of decent quality for v * (2θ − θ) too. We hence want to aide the solving of v * (2θ − θ) by using the optimal solution of v * (θ). One way to do this for CP solvers is solution-guided search (Demirovíc, Chu, and Stuckey 2018) . For MIP/LP we can use warmstart-ing (Yildirim and Wright 2002; Zeilinger, Jones, and Morari 2011) , that is, to use the previous solution as starting point for MIP. In case of linear programming (and hence the relaxation), we can reuse the basis of the solution. An alternative is to use the true solution to compute a bound on the objective function. Indeed, as the solution to S = v * (θ) is valid for v * (2θ − θ) and has an objective value of f (S, (2θ − θ)). Hence, we can use this as a bound on the objective and potentially cut away a large part of the search space. While the true solutions v * (θ) can be cached, we must compute this solution once for each training instance (x, θ) i , which may already take significant time for large problems. We observe that only the objective changes between calls to the oracle v * , and hence any previously computed solution is also a candidate solution for the other calls. We can hence use warmstarting for any solver call after the first, and from any previously computed solution so far.',\n",
       " 'Experimental Evaluation We consider three types of combinatorial problems: unweighted and weighted knapsack and energy-cost aware scheduling. Below we briefly discuss the problem formulations: Unweighted/weighted knapsack problem The knapsack problem can be formalized as argmax X V X s.t. W X ≤ c. The values V will be predicted from data and weights W and capacity c are given. In the unweighted knapsack, all weights W are 1 and the problem is polynomial time solvable. Weighted knapsacks are NP-hard and it is known that the computational difficulty increases with the correlation between weights and values (Pisinger 2005) . We generated mildly correlated knapsacks as follows: for each of the 48 half-hour slots we assign a weight w i by sampling from the set {3, 5, 7}, then we multiply each profit value v i by its corresponding weight and include some randomness by adding Gaussian noise ξ ∼ N(0, 25) to each v i before multiplying by weight. In the unweighted case, we consider 9 different capacity values c, namely from 5 to 45 increasing by 5. For the weighted knapsack experiment, we consider 7 different capacity values from 30 to 210 increasing by 30. Energy-cost aware scheduling It is a resourceconstrained job scheduling problem where the goal is to minimize (predicted) energy cost, it is described in the CSPLib as problem 059 (Simonis et al. ) . In summary, we are given a number of machines and have to schedule a given number of tasks, where each task has a duration, an earliest start and a latest end, resource requirement and a power usage. Each machine has a resource capacity constraint. We omit startup/shutdown costs. No task is allowed to stretch over midnight between two days and cannot be interrupted once started, nor migrate to another machine. Time is discretized in t timeslots and a schedule has to be made over all timeslots at once. For each timeslot, a (predicted) energy cost is given and the objective is to minimize the total energy cost of running the tasks on the machines. We consider two variants of the problem: easy instances consisting of 30 minute timeslots (e.g. 48 timeslots per day), and hard instances as used in the ICON energy challenge consisting of 5 minute timeslots (288 timeslots per day). The easy instances have 3 machines and respectively 10, 15 and 20 tasks. The hard instances each have 10 machines and 200 tasks. Data Our data is drawn from the Irish Single Electricity Market Operator (SEMO) (Ifrim, OSullivan, and Simonis 2012) . This dataset consists of historical energy price data at 30-minute intervals starting from Midnight 1st November, 2011 to 31st December, 2013. Each instance of the data has calendar attributes; day-ahead estimates of weather characteristics; SEMO day-ahead forecasted energy-load, windenergy production and prices; and actual wind-speed, temperature, CO 2 intensity and price. Of the actual attributes, we keep only the actual price, and use it as a target for prediction. For the hard scheduling instances, each 5-minute timeslots have the same price as the 30-minute timeslot it belongs to, following the ICON challenge. Experimental setup For all our experiments, we use a linear model without any hidden layer as the underlying predictive model. Note, the SPO approach is a model-free approach and it is compatible wih any deep neural network; but earlier work (Ifrim, OSullivan, and Simonis 2012) showed accuracy in predictions is not effective for the downstream optimization task. For the experiments, we divide our data into three sets: training (70%), validation (10%) and test (20%), and evaluate the performance by measuring regret on the test set. Training, validation and test data covers 552, 60 and 177 days of energy data respectively. Our model is trained by batch gradient descent, where each batch corresponds to one day, that is, 48 consecutive training instances namely one for each half hour of that day. This batch together forms one set of parameters of one optimisation instance, e.g. knapsack values or schedule costs. The learning rate and momentum for each model are selected through a grid search based on the regret on the validation dataset. The best combination of parameters is then used in the experiments shown. Solving the knapsack instances takes sub-second time per optimisation instance, solving the easy scheduling problems takes 0.1 to 2 seconds per optimisation instance and for the hard scheduling problems solving just the relaxation already costs 30 to 150 seconds per optimisation instance. For the latter, this means that merely evaluating regret on the test-set of 177 optimisation instances takes about 3 hours. With 552 training instances, one epoch of training requires 9 hours. For all experiments except the hard instances, we repeat it 10 times and report on the mean and standard deviation. We use the Gurobi optimization-solver for solving the combinatorial problems, the nn module in Pytorch to implement the predictive models and the optim module in Pytorch for training the models with corresponding loss functions. Experiments were run on Intel(R) Xeon(R) CPU E3-1225 v5 @ 3.30GHz processors with 32GB memory 2 .',\n",
       " 'RQ1: exact versus weaker oracles The first research question is what the loss in accuracy of solving the full discrete problems (SPO-full) versus solving only the relaxation (SPO-relax) during training is. Together with this, we look at what the gain is in terms of reductionin-regret over time. We visualise both through the learning curves as evaluated on the test set. For all methods, we compute the exact regret on the test-set, e.g. by fully solving the instances with the predictions. Figure 2 shows the learning curves over epochs, where one epoch is one iteration over all instances of the training data, for three problem instances. We also include the regret of MSE-r as baseline. In all three case we see that MSE-r is worse, and stagnates or slightly decreases in performance over the epochs. This validates the use of MSE-r where the best epoch is chosen retrospectively based on a validation set. It also validates that the SPO-surrogate loss function captures the essence of the intended loss, namely regret. We also see, surprisingly, that SPO-relax achieves very similar performance to SPO-full on all problem instances in our experiments. This means that even though SPO can reason over exact discrete solutions, reasoning over these continuous relaxation solutions is sufficient for the loss function to guide the learning to where it matters. The real advantage of SPO-relax over SPO-full is evident from Figure 3 . Here we present the test regret not against',\n",
       " 'Instance Baseline MSE-warmstart Warmstart from earlier basis 1 6.5 (1.5) sec 8 (0.5) sec 1.5 (0.2) sec 2 7 (1.5) sec 6 (1.0) sec 1 (0.2) sec 3 10 (0.5) sec 12 (1.0) sec 2.5 (0.1) sec show SPO-relax runs, and hence converges, much quicker in time than SPO. This is thanks to the fact that solving the continuous relaxation can be done in polynomial time while the discrete problem is worst-case exponential. SPO-relax is slower than MSE-r but the difference in quality clearly justifies it. In the subsequent experiments, we will use SPO-relax only.',\n",
       " 'RQ2 benefits of warmstarting As baseline we use the standard SPO-relax approach. We test warmstarting the learning by first training the network with MSE as loss function for 6 epochs, after which we continue learning with SPO-relax. We indicate this approach by MSE-warmstart. We summarizes the effect of warmstarting in Table 1 , We observe that warmstarting from MSE results in a slightly faster start in the initial seconds, but this has no benefit, nor penalty over the longer run. Warmstarting from an earlier basis, after the MIP pre-solving, did result in runtime improvements overall. We also attempted warmstarting by adding objective cuts, but this slowed down the solving of the relaxation, often doubling it, because more iterations were needed.',\n",
       " 'RQ3: SPO versus QPTL Next, we compare our approach against the state-of-the-art QP approach (QPTL) of Wilder, Dilkina, and Tambe (2019) which proposes to transform the discrete linear integer program into a continuous QP by taking the continuous relaxation and a squared L2-norm of the decision variables ||x|| 2 2 . This makes the problem quadratic and twice differentiable allowing them to use the differntiable QP solver (Donti, Amos, and Kolter 2017) . Figure 4 shows the average regret on all unweighted and weighted knapsack instances and easy scheduling instances. We can see that for unweighted knapsack, SPO-relax almost always outperforms the other methods, while QPTL performs worse than MSE-r. For weighted knapsacks, SPOrelax is best for all but the lower capacities. For these lower capacities, QPTL is better though its results worsen for higher capacities. The same narrative is reflected in Figure 5 . In Figure 5c (weighted knapsack, capacity:60) QPTL converges to a better solution than SPO. In all other cases SPO-relax produces better quality of solution and in most cases QPTL converges slower than SPO-relax. The poor quality of QPTL at higher capacities may stem from the squared norm which favors sparse solutions, while at high capacities, most items will be included and the best solutions are those that identify which items not to include. On two energy-scheduling instances SPO-relax performs better whereas for the other instance, the regrets of SPOrelax and QPTL are similar. From Figure 5f and 5e, we can see, again, SPO-relax converges faster than QPTL.',\n",
       " 'RQ4: Suitability on large, hard optimisation instances While SPO-relax performs well across the combinatorial instances used so far, these are still toy-level problems with relatively few decision variables that can be solved in a few seconds. We will use the large-scale optimization instances of the ICON challenge, for which no exact solutions are known. Hence, for this experiment we will report the regret when solving the relaxation of the problem for the test instances, rather than solving the discrete problem during testing as in the previous experiments. We impose a timelimit of 6 hours on the total time budget that SPO-relax can spend on calling the solver. This includes the time to compute and cache the ground-truth solution of Table 2 , for 5 hard scheduling instances. First, we show the test (relaxed) regret after 2, 4, 6 and 8 MSE-r epochs. The results show that the test regret slightly decreases over the epochs; thereafter, we observed, regret tends to increase. With SPO-relax, in 6 hours, it was possible to train only on 300 to 450 different instances, which is only 50 to 80% of the training instances. Table 2 shows even for a limited solving budget of 6 hour and without MSE-warmstarting, it already outperforms the MSE learned models. This shows that even on very large and hard instances that are computationally expensive to solve, training with SPOrelax on a limited time-budget is better than training in a two-stage approach with a non optimisation-directed loss.',\n",
       " 'Conclusions and future work Smart \"Predict and Optimize\" methods have shown to be able to learn from, and improve task loss. Extending these techniques to be applicable beyond toy problems, more specifically hard combinatorial problems, is essential to the applicability of this promising idea. SPO is able to outperform QPTL and lends itself to a wide applicability as it allows for the use of black-box oracles in its loss computation. We investigated the use of weaker oracles and showed that for the problems studied, learning with SPO loss while solving the relaxation leads to equal performance as solving the discrete combinatorial problem. We have shown how this opens the way to solve larger and more complex combinatorial problems, for which solving the exact solution may not be possible, let alone to do so repeatedly. In case of problems with weaker relaxations, one could consider adding cutting planes prior to solving (Ferber et al. 2019) . Moreover, further improvements could be achieved by exploiting the fact that all previously computed solutions are valid candidates. So far we have only used this for warmstarting the solver. Our work hence encourages more research into the use of weak oracles and relaxation methods, especially those that can benefit from repeated solving. One interesting direction are local search methods and other iterative refinement methods, as they can improve the solutions during the loss computation. With respect to exact methods, knowledge compilation methods such as (relaxed) BDDs could offer both a runtime improvement from faster repeat solving and employing a relaxation.',\n",
       " 'Acknowledgments We would like to thank the anonymous reviewers for the valuable comments and suggestions. This research is supported by Data-driven logistics (FWO-S007318N).']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div xmlns=\"http://www.tei-c.org/ns/1.0\">Background<p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU <ref target=\"#b19\" type=\"bibr\">[20]</ref>, ByteNet <ref target=\"#b14\" type=\"bibr\">[15]</ref> and ConvS2S <ref target=\"#b7\" type=\"bibr\">[8]</ref>, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions <ref target=\"#b10\" type=\"bibr\">[11]</ref>. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.</p><p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations <ref target=\"#b3\" type=\"bibr\">[4,</ref><ref target=\"#b21\" type=\"bibr\">22,</ref><ref target=\"#b22\" type=\"bibr\">23,</ref><ref target=\"#b18\" type=\"bibr\">19]</ref>.</p><p>End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks <ref target=\"#b27\" type=\"bibr\">[28]</ref>.</p><p>To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as <ref target=\"#b13\" type=\"bibr\">[14,</ref><ref target=\"#b14\" type=\"bibr\">15]</ref> and <ref target=\"#b7\" type=\"bibr\">[8]</ref>.</p></div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('div')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<html>\\n <body>\\n  <tei xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemalocation=\"http://www.tei-c.org/ns/1.0 /home/varun/Desktop/NLP/Projects/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd\">\\n   <teiheader xml:lang=\"en\">\\n    <filedesc>\\n     <titlestmt>\\n      <title level=\"a\" type=\"main\">\\n       Attention Is All You Need\\n      </title>\\n     </titlestmt>\\n     <publicationstmt>\\n      <publisher>\\n      </publisher>\\n      <availability status=\"unknown\">\\n       <licence>\\n       </licence>\\n      </availability>\\n     </publicationstmt>\\n     <sourcedesc>\\n      <biblstruct>\\n       <analytic>\\n        <author>\\n         <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n          <forename type=\"first\">\\n           Ashish\\n          </forename>\\n          <surname>\\n           Vaswani\\n          </surname>\\n         </persname>\\n         <email>\\n          avaswani@google.com\\n         </email>\\n         <affiliation key=\"aff0\">\\n          <orgname key=\"dep1\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep2\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep3\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname type=\"institution\">\\n           University of Toronto\\n          </orgname>\\n         </affiliation>\\n        </author>\\n        <author>\\n         <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n          <forename type=\"first\">\\n           Google\\n          </forename>\\n          <surname>\\n           Brain\\n          </surname>\\n         </persname>\\n         <affiliation key=\"aff0\">\\n          <orgname key=\"dep1\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep2\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep3\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname type=\"institution\">\\n           University of Toronto\\n          </orgname>\\n         </affiliation>\\n        </author>\\n        <author>\\n         <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n          <forename type=\"first\">\\n           Noam\\n          </forename>\\n          <surname>\\n           Shazeer\\n          </surname>\\n         </persname>\\n         <affiliation key=\"aff0\">\\n          <orgname key=\"dep1\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep2\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep3\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname type=\"institution\">\\n           University of Toronto\\n          </orgname>\\n         </affiliation>\\n        </author>\\n        <author>\\n         <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n          <forename type=\"first\">\\n           Google\\n          </forename>\\n          <surname>\\n           Brain\\n          </surname>\\n         </persname>\\n         <affiliation key=\"aff0\">\\n          <orgname key=\"dep1\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep2\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep3\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname type=\"institution\">\\n           University of Toronto\\n          </orgname>\\n         </affiliation>\\n        </author>\\n        <author>\\n         <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n          <forename type=\"first\">\\n           Niki\\n          </forename>\\n          <surname>\\n           Parmar\\n          </surname>\\n         </persname>\\n         <email>\\n          nikip@google.com\\n         </email>\\n         <affiliation key=\"aff0\">\\n          <orgname key=\"dep1\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep2\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep3\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname type=\"institution\">\\n           University of Toronto\\n          </orgname>\\n         </affiliation>\\n        </author>\\n        <author>\\n         <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n          <forename type=\"first\">\\n           Jakob\\n          </forename>\\n          <surname>\\n           Uszkoreit\\n          </surname>\\n         </persname>\\n         <affiliation key=\"aff0\">\\n          <orgname key=\"dep1\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep2\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep3\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname type=\"institution\">\\n           University of Toronto\\n          </orgname>\\n         </affiliation>\\n        </author>\\n        <author>\\n         <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n          <forename type=\"first\">\\n           Llion\\n          </forename>\\n          <surname>\\n           Jones\\n          </surname>\\n         </persname>\\n         <affiliation key=\"aff0\">\\n          <orgname key=\"dep1\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep2\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep3\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname type=\"institution\">\\n           University of Toronto\\n          </orgname>\\n         </affiliation>\\n        </author>\\n        <author>\\n         <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n          <forename type=\"first\">\\n           Aidan\\n          </forename>\\n          <forename type=\"middle\">\\n           N\\n          </forename>\\n          <surname>\\n           Gomez\\n          </surname>\\n         </persname>\\n         <affiliation key=\"aff0\">\\n          <orgname key=\"dep1\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep2\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep3\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname type=\"institution\">\\n           University of Toronto\\n          </orgname>\\n         </affiliation>\\n        </author>\\n        <author>\\n         <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n          <forename type=\"first\">\\n           Łukasz\\n          </forename>\\n          <surname>\\n           Kaiser\\n          </surname>\\n         </persname>\\n         <email>\\n          lukaszkaiser@google.com\\n         </email>\\n         <affiliation key=\"aff0\">\\n          <orgname key=\"dep1\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep2\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep3\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname type=\"institution\">\\n           University of Toronto\\n          </orgname>\\n         </affiliation>\\n        </author>\\n        <author>\\n         <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n          <forename type=\"first\">\\n           Google\\n          </forename>\\n          <surname>\\n           Brain\\n          </surname>\\n         </persname>\\n         <affiliation key=\"aff0\">\\n          <orgname key=\"dep1\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep2\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep3\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname type=\"institution\">\\n           University of Toronto\\n          </orgname>\\n         </affiliation>\\n        </author>\\n        <author>\\n         <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n          <forename type=\"first\">\\n           Illia\\n          </forename>\\n          <surname>\\n           Polosukhin\\n          </surname>\\n         </persname>\\n         <email>\\n          illia.polosukhin@gmail.com\\n         </email>\\n         <affiliation key=\"aff0\">\\n          <orgname key=\"dep1\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep2\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname key=\"dep3\" type=\"department\">\\n           Google Research\\n          </orgname>\\n          <orgname type=\"institution\">\\n           University of Toronto\\n          </orgname>\\n         </affiliation>\\n        </author>\\n        <title level=\"a\" type=\"main\">\\n         Attention Is All You Need\\n        </title>\\n       </analytic>\\n       <monogr>\\n        <imprint>\\n         <date>\\n         </date>\\n        </imprint>\\n       </monogr>\\n      </biblstruct>\\n     </sourcedesc>\\n    </filedesc>\\n    <encodingdesc>\\n     <appinfo>\\n      <application ident=\"GROBID\" version=\"0.5.6\" when=\"2020-04-20T09:13+0000\">\\n       <ref target=\"https://github.com/kermitt2/grobid\">\\n        GROBID - A machine learning software for extracting information from scholarly documents\\n       </ref>\\n      </application>\\n     </appinfo>\\n    </encodingdesc>\\n    <profiledesc>\\n     <abstract>\\n      <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n       <p>\\n        The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. * Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. † Work performed while at Google Brain.\\n       </p>\\n      </div>\\n     </abstract>\\n    </profiledesc>\\n   </teiheader>\\n   <text xml:lang=\"en\">\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Introduction\\n     <p>\\n      Recurrent neural networks, long short-term memory\\n      <ref target=\"#b11\" type=\"bibr\">\\n       [12]\\n      </ref>\\n      and gated recurrent\\n      <ref target=\"#b6\" type=\"bibr\">\\n       [7]\\n      </ref>\\n      neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation\\n      <ref target=\"#b28\" type=\"bibr\">\\n       [29,\\n      </ref>\\n      <ref target=\"#b1\" type=\"bibr\">\\n       2,\\n      </ref>\\n      <ref target=\"#b4\" type=\"bibr\">\\n       5]\\n      </ref>\\n      . Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures\\n      <ref target=\"#b30\" type=\"bibr\">\\n       [31,\\n      </ref>\\n      <ref target=\"#b20\" type=\"bibr\">\\n       21,\\n      </ref>\\n      <ref target=\"#b12\" type=\"bibr\">\\n       13]\\n      </ref>\\n      .\\n     </p>\\n     <p>\\n      Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h t , as a function of the previous hidden state h t−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks\\n      <ref target=\"#b17\" type=\"bibr\">\\n       [18]\\n      </ref>\\n      and conditional computation\\n      <ref target=\"#b25\" type=\"bibr\">\\n       [26]\\n      </ref>\\n      , while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\\n     </p>\\n     <p>\\n      Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences\\n      <ref target=\"#b1\" type=\"bibr\">\\n       [2,\\n      </ref>\\n      <ref target=\"#b15\" type=\"bibr\">\\n       16]\\n      </ref>\\n      . In all but a few cases\\n      <ref target=\"#b21\" type=\"bibr\">\\n       [22]\\n      </ref>\\n      , however, such attention mechanisms are used in conjunction with a recurrent network.\\n     </p>\\n     <p>\\n      In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Background\\n     <p>\\n      The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n      <ref target=\"#b19\" type=\"bibr\">\\n       [20]\\n      </ref>\\n      , ByteNet\\n      <ref target=\"#b14\" type=\"bibr\">\\n       [15]\\n      </ref>\\n      and ConvS2S\\n      <ref target=\"#b7\" type=\"bibr\">\\n       [8]\\n      </ref>\\n      , all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions\\n      <ref target=\"#b10\" type=\"bibr\">\\n       [11]\\n      </ref>\\n      . In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\\n     </p>\\n     <p>\\n      Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations\\n      <ref target=\"#b3\" type=\"bibr\">\\n       [4,\\n      </ref>\\n      <ref target=\"#b21\" type=\"bibr\">\\n       22,\\n      </ref>\\n      <ref target=\"#b22\" type=\"bibr\">\\n       23,\\n      </ref>\\n      <ref target=\"#b18\" type=\"bibr\">\\n       19]\\n      </ref>\\n      .\\n     </p>\\n     <p>\\n      End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks\\n      <ref target=\"#b27\" type=\"bibr\">\\n       [28]\\n      </ref>\\n      .\\n     </p>\\n     <p>\\n      To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as\\n      <ref target=\"#b13\" type=\"bibr\">\\n       [14,\\n      </ref>\\n      <ref target=\"#b14\" type=\"bibr\">\\n       15]\\n      </ref>\\n      and\\n      <ref target=\"#b7\" type=\"bibr\">\\n       [8]\\n      </ref>\\n      .\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Model Architecture\\n     <p>\\n      Most competitive neural sequence transduction models have an encoder-decoder structure\\n      <ref target=\"#b4\" type=\"bibr\">\\n       [5,\\n      </ref>\\n      <ref target=\"#b1\" type=\"bibr\">\\n       2,\\n      </ref>\\n      <ref target=\"#b28\" type=\"bibr\">\\n       29]\\n      </ref>\\n      . Here, the encoder maps an input sequence of symbol representations (x 1 , ..., x n ) to a sequence of continuous representations z = (z 1 , ..., z n ). Given z, the decoder then generates an output sequence (y 1 , ..., y m ) of symbols one element at a time. At each step the model is auto-regressive\\n      <ref target=\"#b8\" type=\"bibr\">\\n       [9]\\n      </ref>\\n      , consuming the previously generated symbols as additional input when generating the next.\\n     </p>\\n     <p>\\n      The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of\\n      <ref target=\"#fig_0\" type=\"figure\">\\n       Figure 1\\n      </ref>\\n      , respectively.\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Encoder and Decoder Stacks\\n     <p>\\n      Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection\\n      <ref target=\"#b9\" type=\"bibr\">\\n       [10]\\n      </ref>\\n      around each of the two sub-layers, followed by layer normalization\\n      <ref target=\"#b0\" type=\"bibr\">\\n       [1]\\n      </ref>\\n      . That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d model = 512.\\n     </p>\\n     <p>\\n      Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Attention\\n     <p>\\n      An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. query with all keys, divide each by √ d k , and apply a softmax function to obtain the weights on the values.\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Scaled Dot-Product Attention\\n     <p>\\n      In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\\n     </p>\\n     <formula xml:id=\"formula_0\">\\n      Attention(Q, K, V ) = softmax( QK T √ d k )V\\n      <label>\\n       (1)\\n      </label>\\n     </formula>\\n     <p>\\n      The two most commonly used attention functions are additive attention\\n      <ref target=\"#b1\" type=\"bibr\">\\n       [2]\\n      </ref>\\n      , and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1\\n     </p>\\n     <formula xml:id=\"formula_1\">\\n      √ d k .\\n     </formula>\\n     <p>\\n      Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n     </p>\\n     <p>\\n      While for small values of d k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d k\\n      <ref target=\"#b2\" type=\"bibr\">\\n       [3]\\n      </ref>\\n      . We suspect that for large values of d k , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4 . To counteract this effect, we scale the dot products by 1\\n     </p>\\n     <formula xml:id=\"formula_2\">\\n      √ d k .\\n     </formula>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Multi-Head Attention\\n     <p>\\n      Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k , d k and d v dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in\\n      <ref target=\"#fig_1\" type=\"figure\">\\n       Figure 2\\n      </ref>\\n      .\\n     </p>\\n     <p>\\n      Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n     </p>\\n     <formula xml:id=\"formula_3\">\\n      MultiHead(Q, K, V ) = Concat(head 1 , ..., head h )W O where head i = Attention(QW Q i , KW K i , V W V i )\\n     </formula>\\n     <p>\\n      Where the projections are parameter matrices\\n     </p>\\n     <formula xml:id=\"formula_4\">\\n      W Q i ∈ R dmodel×d k , W K i ∈ R dmodel×d k , W V i ∈ R dmodel×dv and W O ∈ R hdv×dmodel .\\n     </formula>\\n     <p>\\n      In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\n     </p>\\n     <formula xml:id=\"formula_5\">\\n      d k = d v = d model /h = 64.\\n     </formula>\\n     <p>\\n      Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Applications of Attention in our Model\\n     <p>\\n      The Transformer uses multi-head attention in three different ways:\\n     </p>\\n     <p>\\n      • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n      <ref target=\"#b30\" type=\"bibr\">\\n       [31,\\n      </ref>\\n      <ref target=\"#b1\" type=\"bibr\">\\n       2,\\n      </ref>\\n      <ref target=\"#b7\" type=\"bibr\">\\n       8\\n      </ref>\\n      ].\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Position-wise Feed-Forward Networks\\n     <p>\\n      In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\\n     </p>\\n     <formula xml:id=\"formula_6\">\\n      FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2\\n      <label>\\n       (2)\\n      </label>\\n     </formula>\\n     <p>\\n      While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d model = 512, and the inner-layer has dimensionality d f f = 2048.\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Embeddings and Softmax\\n     <p>\\n      Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model . We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to\\n      <ref target=\"#b23\" type=\"bibr\">\\n       [24]\\n      </ref>\\n      . In the embedding layers, we multiply those weights by √ d model .\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Positional Encoding\\n     <p>\\n      Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n      <ref type=\"table\">\\n       Table 1\\n      </ref>\\n      : Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\\n     </p>\\n     <p>\\n      Layer Type Complexity per Layer Sequential Maximum Path Length\\n     </p>\\n     <formula xml:id=\"formula_7\">\\n      Operations Self-Attention O(n 2 · d) O(1) O(1) Recurrent O(n · d 2 ) O(n) O(n) Convolutional O(k · n · d 2 ) O(1) O(log k (n)) Self-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n     </formula>\\n     <p>\\n      bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed\\n      <ref target=\"#b7\" type=\"bibr\">\\n       [8]\\n      </ref>\\n      .\\n     </p>\\n     <p>\\n      In this work, we use sine and cosine functions of different frequencies: where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P E pos+k can be represented as a linear function of P E pos .\\n     </p>\\n     <p>\\n      We also experimented with using learned positional embeddings\\n      <ref target=\"#b7\" type=\"bibr\">\\n       [8]\\n      </ref>\\n      instead, and found that the two versions produced nearly identical results (see\\n      <ref target=\"#tab_2\" type=\"table\">\\n       Table 3\\n      </ref>\\n      row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Why Self-Attention\\n     <p>\\n      In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1 , ..., x n ) to another sequence of equal length (z 1 , ..., z n ), with x i , z i ∈ R d , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\n     </p>\\n     <p>\\n      One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\\n     </p>\\n     <p>\\n      The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies\\n      <ref target=\"#b10\" type=\"bibr\">\\n       [11]\\n      </ref>\\n      . Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\n     </p>\\n     <p>\\n      As noted in\\n      <ref type=\"table\">\\n       Table 1\\n      </ref>\\n      , a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece\\n      <ref target=\"#b30\" type=\"bibr\">\\n       [31]\\n      </ref>\\n      and byte-pair\\n      <ref target=\"#b24\" type=\"bibr\">\\n       [25]\\n      </ref>\\n      representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\\n     </p>\\n     <p>\\n      A single convolutional layer with kernel width k &lt; n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(log k (n)) in the case of dilated convolutions\\n      <ref target=\"#b14\" type=\"bibr\">\\n       [15]\\n      </ref>\\n      , increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions\\n      <ref target=\"#b5\" type=\"bibr\">\\n       [6]\\n      </ref>\\n      , however, decrease the complexity considerably, to O(k · n · d + n · d 2 ). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\\n     </p>\\n     <p>\\n      As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Training\\n     <p>\\n      This section describes the training regime for our models.\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Training Data and Batching\\n     <p>\\n      We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding\\n      <ref target=\"#b2\" type=\"bibr\">\\n       [3]\\n      </ref>\\n      , which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary\\n      <ref target=\"#b30\" type=\"bibr\">\\n       [31]\\n      </ref>\\n      . Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Hardware and Schedule\\n     <p>\\n      We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,\\n      <ref target=\"#tab_2\" type=\"table\">\\n       (described on the  bottom line of table 3)\\n      </ref>\\n      , step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Optimizer\\n     <p>\\n      We used the Adam optimizer\\n      <ref target=\"#b16\" type=\"bibr\">\\n       [17]\\n      </ref>\\n      with β 1 = 0.9, β 2 = 0.98 and = 10 −9 . We varied the learning rate over the course of training, according to the formula:\\n     </p>\\n     <formula xml:id=\"formula_8\">\\n      lrate = d −0.5 model · min(step_num −0.5 , step_num · warmup_steps −1.5 )\\n      <label>\\n       (3)\\n      </label>\\n     </formula>\\n     <p>\\n      This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Regularization\\n     <p>\\n      We employ three types of regularization during training:\\n     </p>\\n     <p>\\n      Residual Dropout We apply dropout\\n      <ref target=\"#b26\" type=\"bibr\">\\n       [27]\\n      </ref>\\n      to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0.1. Label Smoothing During training, we employed label smoothing of value ls = 0.1\\n      <ref target=\"#b29\" type=\"bibr\">\\n       [30]\\n      </ref>\\n      . This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Results\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Machine Translation\\n     <p>\\n      On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in\\n      <ref target=\"#tab_0\" type=\"table\">\\n       Table 2\\n      </ref>\\n      ) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of\\n      <ref target=\"#tab_2\" type=\"table\">\\n       Table 3\\n      </ref>\\n      . Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\\n     </p>\\n     <p>\\n      On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P drop = 0.1, instead of 0.3.\\n     </p>\\n     <p>\\n      For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6\\n      <ref target=\"#b30\" type=\"bibr\">\\n       [31]\\n      </ref>\\n      . These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible\\n      <ref target=\"#b30\" type=\"bibr\">\\n       [31]\\n      </ref>\\n      .\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Model Variations\\n     <p>\\n      To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in\\n      <ref target=\"#tab_2\" type=\"table\">\\n       Table 3\\n      </ref>\\n      .\\n     </p>\\n     <p>\\n      In\\n      <ref target=\"#tab_2\" type=\"table\">\\n       Table 3\\n      </ref>\\n      rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In\\n      <ref target=\"#tab_2\" type=\"table\">\\n       Table 3\\n      </ref>\\n      rows (B), we observe that reducing the attention key size d k hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings\\n      <ref target=\"#b7\" type=\"bibr\">\\n       [8]\\n      </ref>\\n      , and observe nearly identical results to the base model.\\n     </p>\\n    </div>\\n    <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Conclusion\\n     <p>\\n      In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\n     </p>\\n     <p>\\n      For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\\n     </p>\\n     <p>\\n      We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\\n     </p>\\n     <p>\\n      The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\\n     </p>\\n    </div>\\n    <figure xml:id=\"fig_0\" xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Figure 1 :\\n     <label>\\n      1\\n     </label>\\n     <figdesc>\\n      The Transformer -model architecture.\\n     </figdesc>\\n    </figure>\\n    <figure xml:id=\"fig_1\" xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Figure 2 :\\n     <label>\\n      2\\n     </label>\\n     <figdesc>\\n      We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension d k , and values of dimension d v . We compute the dot products of the (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\n     </figdesc>\\n    </figure>\\n    <figure xml:id=\"fig_2\" xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     •\\n     <label>\\n     </label>\\n     <figdesc>\\n      The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. SeeFigure 2.\\n     </figdesc>\\n    </figure>\\n    <figure xml:id=\"fig_3\" xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     P\\n     <label>\\n     </label>\\n     <figdesc>\\n      E (pos,2i) = sin(pos/10000 2i/dmodel ) P E (pos,2i+1) = cos(pos/10000 2i/dmodel )\\n     </figdesc>\\n    </figure>\\n    <figure type=\"table\" validated=\"true\" xml:id=\"tab_0\" xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Table 2 :\\n     <label>\\n      2\\n     </label>\\n     <figdesc>\\n      The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n     </figdesc>\\n     <table>\\n      Model \\nBLEU \\nTraining Cost (FLOPs) \\n\\nEN-DE EN-FR \\nEN-DE \\nEN-FR \\nByteNet [15] \\n23.75 \\nDeep-Att + PosUnk [32] \\n39.2 \\n1.0 · 10 20 \\nGNMT + RL [31] \\n24.6 \\n39.92 \\n2.3 · 10 19 1.4 · 10 20 \\nConvS2S [8] \\n25.16 \\n40.46 \\n9.6 · 10 18 1.5 · 10 20 \\nMoE [26] \\n26.03 \\n40.56 \\n2.0 · 10 19 1.2 · 10 20 \\nDeep-Att + PosUnk Ensemble [32] \\n40.4 \\n8.0 · 10 20 \\nGNMT + RL Ensemble [31] \\n26.30 \\n41.16 \\n1.8 · 10 20 1.1 · 10 21 \\nConvS2S Ensemble [8] \\n26.36 \\n41.29 \\n7.7 · 10 19 1.2 · 10 21 \\nTransformer (base model) \\n27.3 \\n38.1 \\n3.3 · 10 18 \\nTransformer (big) \\n28.4 \\n41.0 \\n2.3 · 10 19\\n     </table>\\n    </figure>\\n    <figure type=\"table\" validated=\"false\" xml:id=\"tab_1\" xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Table 2\\n     <label>\\n      2\\n     </label>\\n     <figdesc>\\n      summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5 .\\n     </figdesc>\\n     <table>\\n     </table>\\n    </figure>\\n    <figure type=\"table\" validated=\"true\" xml:id=\"tab_2\" xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     Table 3 :\\n     <label>\\n      3\\n     </label>\\n     <figdesc>\\n      Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\\n     </figdesc>\\n     <table>\\n      N d model \\nd ff \\nh \\nd k \\nd v \\nP drop \\n\\nls \\n\\ntrain \\nPPL BLEU params \\nsteps (dev) (dev) \\n×10 6 \\nbase 6 \\n512 \\n2048 8 \\n64 \\n64 \\n0.1 \\n0.1 100K 4.92 \\n25.8 \\n65 \\n\\n(A) \\n\\n1 512 512 \\n5.29 \\n24.9 \\n4 128 128 \\n5.00 \\n25.5 \\n16 32 \\n32 \\n4.91 \\n25.8 \\n32 16 \\n16 \\n5.01 \\n25.4 \\n\\n(B) \\n16 \\n5.16 \\n25.1 \\n58 \\n32 \\n5.01 \\n25.4 \\n60 \\n\\n(C) \\n\\n2 \\n6.11 \\n23.7 \\n36 \\n4 \\n5.19 \\n25.3 \\n50 \\n8 \\n4.88 \\n25.5 \\n80 \\n256 \\n32 \\n32 \\n5.75 \\n24.5 \\n28 \\n1024 \\n128 128 \\n4.66 \\n26.0 \\n168 \\n1024 \\n5.12 \\n25.4 \\n53 \\n4096 \\n4.75 \\n26.2 \\n90 \\n\\n(D) \\n\\n0.0 \\n5.77 \\n24.6 \\n0.2 \\n4.95 \\n25.5 \\n0.0 \\n4.67 \\n25.3 \\n0.2 \\n5.47 \\n25.7 \\n(E) \\npositional embedding instead of sinusoids \\n4.92 \\n25.7 \\nbig \\n6 \\n1024 4096 16 \\n0.3 \\n300K 4.33 \\n26.4 \\n213\\n     </table>\\n    </figure>\\n    <note n=\"4\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q · k = d k i=1 qiki, has mean 0 and variance d k .\\n    </note>\\n    <note n=\"5\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">\\n     We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n    </note>\\n    <back>\\n     <div type=\"acknowledgement\">\\n      <div xmlns=\"http://www.tei-c.org/ns/1.0\">\\n       <p>\\n        Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\\n       </p>\\n      </div>\\n     </div>\\n     <div type=\"references\">\\n      <listbibl>\\n       <biblstruct xml:id=\"b0\">\\n        <monogr>\\n         <title>\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Jimmy\\n           </forename>\\n           <forename type=\"middle\">\\n            Lei\\n           </forename>\\n           <surname>\\n            Ba\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Jamie\\n           </forename>\\n           <forename type=\"middle\">\\n            Ryan\\n           </forename>\\n           <surname>\\n            Kiros\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Geoffrey\\n           </forename>\\n           <forename type=\"middle\">\\n            E\\n           </forename>\\n           <surname>\\n            Hinton\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno type=\"arXiv\">\\n          arXiv:1607.06450\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2016\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n        <note type=\"report_type\">\\n         Layer normalization. arXiv preprint\\n        </note>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b1\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Neural machine translation by jointly learning to align and translate\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Dzmitry\\n           </forename>\\n           <surname>\\n            Bahdanau\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Kyunghyun\\n           </forename>\\n           <surname>\\n            Cho\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Yoshua\\n           </forename>\\n           <surname>\\n            Bengio\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno>\\n          abs/1409.0473\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2014\">\\n          </date>\\n          <publisher>\\n           CoRR\\n          </publisher>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b2\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Massive exploration of neural machine translation architectures\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Denny\\n           </forename>\\n           <surname>\\n            Britz\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Anna\\n           </forename>\\n           <surname>\\n            Goldie\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Minh-Thang\\n           </forename>\\n           <surname>\\n            Luong\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Quoc\\n           </forename>\\n           <forename type=\"middle\">\\n            V\\n           </forename>\\n           <surname>\\n            Le\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno>\\n          abs/1703.03906\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2017\">\\n          </date>\\n          <publisher>\\n           CoRR\\n          </publisher>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b3\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Long short-term memory-networks for machine reading\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Jianpeng\\n           </forename>\\n           <surname>\\n            Cheng\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Li\\n           </forename>\\n           <surname>\\n            Dong\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Mirella\\n           </forename>\\n           <surname>\\n            Lapata\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno type=\"arXiv\">\\n          arXiv:1601.06733\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2016\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n        <note type=\"report_type\">\\n         arXiv preprint\\n        </note>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b4\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Learning phrase representations using rnn encoder-decoder for statistical machine translation\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Kyunghyun\\n           </forename>\\n           <surname>\\n            Cho\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Caglar\\n           </forename>\\n           <surname>\\n            Bart Van Merrienboer\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Fethi\\n           </forename>\\n           <surname>\\n            Gulcehre\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Holger\\n           </forename>\\n           <surname>\\n            Bougares\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Yoshua\\n           </forename>\\n           <surname>\\n            Schwenk\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <surname>\\n            Bengio\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno>\\n          abs/1406.1078\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2014\">\\n          </date>\\n          <publisher>\\n           CoRR\\n          </publisher>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b5\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Xception: Deep learning with depthwise separable convolutions\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Francois\\n           </forename>\\n           <surname>\\n            Chollet\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno type=\"arXiv\">\\n          arXiv:1610.02357\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2016\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n        <note type=\"report_type\">\\n         arXiv preprint\\n        </note>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b6\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Empirical evaluation of gated recurrent neural networks on sequence modeling\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Junyoung\\n           </forename>\\n           <surname>\\n            Chung\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Çaglar\\n           </forename>\\n           <surname>\\n            Gülçehre\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Kyunghyun\\n           </forename>\\n           <surname>\\n            Cho\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Yoshua\\n           </forename>\\n           <surname>\\n            Bengio\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno>\\n          abs/1412.3555\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2014\">\\n          </date>\\n          <publisher>\\n           CoRR\\n          </publisher>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b7\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Convolutional sequence to sequence learning\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Jonas\\n           </forename>\\n           <surname>\\n            Gehring\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Michael\\n           </forename>\\n           <surname>\\n            Auli\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            David\\n           </forename>\\n           <surname>\\n            Grangier\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Denis\\n           </forename>\\n           <surname>\\n            Yarats\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Yann\\n           </forename>\\n           <forename type=\"middle\">\\n            N\\n           </forename>\\n           <surname>\\n            Dauphin\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno type=\"arXiv\">\\n          arXiv:1705.03122v2\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2017\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n        <note type=\"report_type\">\\n         arXiv preprint\\n        </note>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b8\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Generating sequences with recurrent neural networks\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Alex\\n           </forename>\\n           <surname>\\n            Graves\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno type=\"arXiv\">\\n          arXiv:1308.0850\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2013\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n        <note type=\"report_type\">\\n         arXiv preprint\\n        </note>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b9\">\\n        <analytic>\\n         <title level=\"a\" type=\"main\">\\n          Deep residual learning for image recognition\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Kaiming\\n           </forename>\\n           <surname>\\n            He\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Xiangyu\\n           </forename>\\n           <surname>\\n            Zhang\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Shaoqing\\n           </forename>\\n           <surname>\\n            Ren\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Jian\\n           </forename>\\n           <surname>\\n            Sun\\n           </surname>\\n          </persname>\\n         </author>\\n        </analytic>\\n        <monogr>\\n         <title level=\"m\">\\n          Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\n         </title>\\n         <meeting>\\n          the IEEE Conference on Computer Vision and Pattern Recognition\\n         </meeting>\\n         <imprint>\\n          <date type=\"published\" when=\"2016\">\\n          </date>\\n          <biblscope from=\"770\" to=\"778\" unit=\"page\">\\n          </biblscope>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b10\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Gradient flow in recurrent nets: the difficulty of learning long-term dependencies\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Sepp\\n           </forename>\\n           <surname>\\n            Hochreiter\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Yoshua\\n           </forename>\\n           <surname>\\n            Bengio\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Paolo\\n           </forename>\\n           <surname>\\n            Frasconi\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Jürgen\\n           </forename>\\n           <surname>\\n            Schmidhuber\\n           </surname>\\n          </persname>\\n         </author>\\n         <imprint>\\n          <date type=\"published\" when=\"2001\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b11\">\\n        <analytic>\\n         <title level=\"a\" type=\"main\">\\n          Long short-term memory\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Sepp\\n           </forename>\\n           <surname>\\n            Hochreiter\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Jürgen\\n           </forename>\\n           <surname>\\n            Schmidhuber\\n           </surname>\\n          </persname>\\n         </author>\\n        </analytic>\\n        <monogr>\\n         <title level=\"j\">\\n          Neural computation\\n         </title>\\n         <imprint>\\n          <biblscope unit=\"volume\">\\n           9\\n          </biblscope>\\n          <biblscope unit=\"issue\">\\n           8\\n          </biblscope>\\n          <biblscope from=\"1735\" to=\"1780\" unit=\"page\">\\n          </biblscope>\\n          <date type=\"published\" when=\"1997\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b12\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Exploring the limits of language modeling\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Rafal\\n           </forename>\\n           <surname>\\n            Jozefowicz\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Oriol\\n           </forename>\\n           <surname>\\n            Vinyals\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Mike\\n           </forename>\\n           <surname>\\n            Schuster\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Noam\\n           </forename>\\n           <surname>\\n            Shazeer\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Yonghui\\n           </forename>\\n           <surname>\\n            Wu\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno type=\"arXiv\">\\n          arXiv:1602.02410\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2016\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n        <note type=\"report_type\">\\n         arXiv preprint\\n        </note>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b13\">\\n        <analytic>\\n         <title level=\"a\" type=\"main\">\\n          Neural GPUs learn algorithms\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Łukasz\\n           </forename>\\n           <surname>\\n            Kaiser\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Ilya\\n           </forename>\\n           <surname>\\n            Sutskever\\n           </surname>\\n          </persname>\\n         </author>\\n        </analytic>\\n        <monogr>\\n         <title level=\"m\">\\n          International Conference on Learning Representations (ICLR)\\n         </title>\\n         <imprint>\\n          <date type=\"published\" when=\"2016\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b14\">\\n        <monogr>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Nal\\n           </forename>\\n           <surname>\\n            Kalchbrenner\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Lasse\\n           </forename>\\n           <surname>\\n            Espeholt\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Karen\\n           </forename>\\n           <surname>\\n            Simonyan\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Aaron\\n           </forename>\\n           <surname>\\n            Van Den Oord\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Alex\\n           </forename>\\n           <surname>\\n            Graves\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Koray\\n           </forename>\\n           <surname>\\n            Kavukcuoglu\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno type=\"arXiv\">\\n          arXiv:1610.10099v2\\n         </idno>\\n         <title level=\"m\">\\n          Neural machine translation in linear time\\n         </title>\\n         <imprint>\\n          <date type=\"published\" when=\"2017\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n        <note type=\"report_type\">\\n         arXiv preprint\\n        </note>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b15\">\\n        <analytic>\\n         <title level=\"a\" type=\"main\">\\n          Structured attention networks\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Yoon\\n           </forename>\\n           <surname>\\n            Kim\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Carl\\n           </forename>\\n           <surname>\\n            Denton\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Luong\\n           </forename>\\n           <surname>\\n            Hoang\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Alexander\\n           </forename>\\n           <forename type=\"middle\">\\n            M\\n           </forename>\\n           <surname>\\n            Rush\\n           </surname>\\n          </persname>\\n         </author>\\n        </analytic>\\n        <monogr>\\n         <title level=\"m\">\\n          International Conference on Learning Representations\\n         </title>\\n         <imprint>\\n          <date type=\"published\" when=\"2017\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b16\">\\n        <analytic>\\n         <title level=\"a\" type=\"main\">\\n          Adam: A method for stochastic optimization\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Diederik\\n           </forename>\\n           <surname>\\n            Kingma\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Jimmy\\n           </forename>\\n           <surname>\\n            Ba\\n           </surname>\\n          </persname>\\n         </author>\\n        </analytic>\\n        <monogr>\\n         <title level=\"m\">\\n          ICLR\\n         </title>\\n         <imprint>\\n          <date type=\"published\" when=\"2015\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b17\">\\n        <monogr>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Oleksii\\n           </forename>\\n           <surname>\\n            Kuchaiev\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Boris\\n           </forename>\\n           <surname>\\n            Ginsburg\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno type=\"arXiv\">\\n          arXiv:1703.10722\\n         </idno>\\n         <title level=\"m\">\\n          Factorization tricks for LSTM networks\\n         </title>\\n         <imprint>\\n          <date type=\"published\" when=\"2017\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n        <note type=\"report_type\">\\n         arXiv preprint\\n        </note>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b18\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          A structured self-attentive sentence embedding\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Zhouhan\\n           </forename>\\n           <surname>\\n            Lin\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Minwei\\n           </forename>\\n           <surname>\\n            Feng\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Cicero\\n           </forename>\\n           <surname>\\n            Nogueira\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Mo\\n           </forename>\\n           <surname>\\n            Santos\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Bing\\n           </forename>\\n           <surname>\\n            Yu\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Bowen\\n           </forename>\\n           <surname>\\n            Xiang\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Yoshua\\n           </forename>\\n           <surname>\\n            Zhou\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <surname>\\n            Bengio\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno type=\"arXiv\">\\n          arXiv:1703.03130\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2017\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n        <note type=\"report_type\">\\n         arXiv preprint\\n        </note>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b19\">\\n        <analytic>\\n         <title level=\"a\" type=\"main\">\\n          Can active memory replace attention?\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Samy\\n           </forename>\\n           <surname>\\n            Bengio\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Łukasz\\n           </forename>\\n           <surname>\\n            Kaiser\\n           </surname>\\n          </persname>\\n         </author>\\n        </analytic>\\n        <monogr>\\n         <title level=\"m\">\\n          Advances in Neural Information Processing Systems, (NIPS)\\n         </title>\\n         <imprint>\\n          <date type=\"published\" when=\"2016\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b20\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Effective approaches to attentionbased neural machine translation\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Minh-Thang\\n           </forename>\\n           <surname>\\n            Luong\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Hieu\\n           </forename>\\n           <surname>\\n            Pham\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Christopher D\\n           </forename>\\n           <surname>\\n            Manning\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno type=\"arXiv\">\\n          arXiv:1508.04025\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2015\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n        <note type=\"report_type\">\\n         arXiv preprint\\n        </note>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b21\">\\n        <analytic>\\n         <title level=\"a\" type=\"main\">\\n          A decomposable attention model\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Ankur\\n           </forename>\\n           <surname>\\n            Parikh\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Oscar\\n           </forename>\\n           <surname>\\n            Täckström\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Dipanjan\\n           </forename>\\n           <surname>\\n            Das\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Jakob\\n           </forename>\\n           <surname>\\n            Uszkoreit\\n           </surname>\\n          </persname>\\n         </author>\\n        </analytic>\\n        <monogr>\\n         <title level=\"m\">\\n          Empirical Methods in Natural Language Processing\\n         </title>\\n         <imprint>\\n          <date type=\"published\" when=\"2016\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b22\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          A deep reinforced model for abstractive summarization\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Romain\\n           </forename>\\n           <surname>\\n            Paulus\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Caiming\\n           </forename>\\n           <surname>\\n            Xiong\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Richard\\n           </forename>\\n           <surname>\\n            Socher\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno type=\"arXiv\">\\n          arXiv:1705.04304\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2017\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n        <note type=\"report_type\">\\n         arXiv preprint\\n        </note>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b23\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Using the output embedding to improve language models\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Ofir\\n           </forename>\\n           <surname>\\n            Press\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Lior\\n           </forename>\\n           <surname>\\n            Wolf\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno type=\"arXiv\">\\n          arXiv:1608.05859\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2016\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n        <note type=\"report_type\">\\n         arXiv preprint\\n        </note>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b24\">\\n        <monogr>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Rico\\n           </forename>\\n           <surname>\\n            Sennrich\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Barry\\n           </forename>\\n           <surname>\\n            Haddow\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Alexandra\\n           </forename>\\n           <surname>\\n            Birch\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno type=\"arXiv\">\\n          arXiv:1508.07909\\n         </idno>\\n         <title level=\"m\">\\n          Neural machine translation of rare words with subword units\\n         </title>\\n         <imprint>\\n          <date type=\"published\" when=\"2015\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n        <note type=\"report_type\">\\n         arXiv preprint\\n        </note>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b25\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Noam\\n           </forename>\\n           <surname>\\n            Shazeer\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Azalia\\n           </forename>\\n           <surname>\\n            Mirhoseini\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Krzysztof\\n           </forename>\\n           <surname>\\n            Maziarz\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Andy\\n           </forename>\\n           <surname>\\n            Davis\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Quoc\\n           </forename>\\n           <surname>\\n            Le\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Geoffrey\\n           </forename>\\n           <surname>\\n            Hinton\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Jeff\\n           </forename>\\n           <surname>\\n            Dean\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno type=\"arXiv\">\\n          arXiv:1701.06538\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2017\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n        <note type=\"report_type\">\\n         arXiv preprint\\n        </note>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b26\">\\n        <analytic>\\n         <title level=\"a\" type=\"main\">\\n          Dropout: a simple way to prevent neural networks from overfitting\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Nitish\\n           </forename>\\n           <surname>\\n            Srivastava\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Geoffrey\\n           </forename>\\n           <forename type=\"middle\">\\n            E\\n           </forename>\\n           <surname>\\n            Hinton\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Alex\\n           </forename>\\n           <surname>\\n            Krizhevsky\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Ilya\\n           </forename>\\n           <surname>\\n            Sutskever\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Ruslan\\n           </forename>\\n           <surname>\\n            Salakhutdinov\\n           </surname>\\n          </persname>\\n         </author>\\n        </analytic>\\n        <monogr>\\n         <title level=\"j\">\\n          Journal of Machine Learning Research\\n         </title>\\n         <imprint>\\n          <biblscope unit=\"volume\">\\n           15\\n          </biblscope>\\n          <biblscope unit=\"issue\">\\n           1\\n          </biblscope>\\n          <biblscope from=\"1929\" to=\"1958\" unit=\"page\">\\n          </biblscope>\\n          <date type=\"published\" when=\"2014\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b27\">\\n        <analytic>\\n         <title level=\"a\" type=\"main\">\\n          End-to-end memory networks\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Sainbayar\\n           </forename>\\n           <surname>\\n            Sukhbaatar\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Jason\\n           </forename>\\n           <surname>\\n            Weston\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Rob\\n           </forename>\\n           <surname>\\n            Fergus\\n           </surname>\\n          </persname>\\n         </author>\\n        </analytic>\\n        <monogr>\\n         <title level=\"m\">\\n          Advances in Neural Information Processing Systems\\n         </title>\\n         <editor>\\n          C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett\\n         </editor>\\n         <imprint>\\n          <publisher>\\n           Curran Associates, Inc\\n          </publisher>\\n          <date type=\"published\" when=\"2015\">\\n          </date>\\n          <biblscope unit=\"volume\">\\n           28\\n          </biblscope>\\n          <biblscope from=\"2440\" to=\"2448\" unit=\"page\">\\n          </biblscope>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b28\">\\n        <analytic>\\n         <title level=\"a\" type=\"main\">\\n          Sequence to sequence learning with neural networks\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Ilya\\n           </forename>\\n           <surname>\\n            Sutskever\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Oriol\\n           </forename>\\n           <surname>\\n            Vinyals\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Quoc Vv\\n           </forename>\\n           <surname>\\n            Le\\n           </surname>\\n          </persname>\\n         </author>\\n        </analytic>\\n        <monogr>\\n         <title level=\"m\">\\n          Advances in Neural Information Processing Systems\\n         </title>\\n         <imprint>\\n          <date type=\"published\" when=\"2014\">\\n          </date>\\n          <biblscope from=\"3104\" to=\"3112\" unit=\"page\">\\n          </biblscope>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b29\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Rethinking the inception architecture for computer vision\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Christian\\n           </forename>\\n           <surname>\\n            Szegedy\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Vincent\\n           </forename>\\n           <surname>\\n            Vanhoucke\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Sergey\\n           </forename>\\n           <surname>\\n            Ioffe\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Jonathon\\n           </forename>\\n           <surname>\\n            Shlens\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Zbigniew\\n           </forename>\\n           <surname>\\n            Wojna\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno>\\n          abs/1512.00567\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2015\">\\n          </date>\\n          <publisher>\\n           CoRR\\n          </publisher>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b30\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Google\\'s neural machine translation system: Bridging the gap between human and machine translation\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Yonghui\\n           </forename>\\n           <surname>\\n            Wu\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Mike\\n           </forename>\\n           <surname>\\n            Schuster\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Zhifeng\\n           </forename>\\n           <surname>\\n            Chen\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            V\\n           </forename>\\n           <surname>\\n            Quoc\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Mohammad\\n           </forename>\\n           <surname>\\n            Le\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Wolfgang\\n           </forename>\\n           <surname>\\n            Norouzi\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Maxim\\n           </forename>\\n           <surname>\\n            Macherey\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Yuan\\n           </forename>\\n           <surname>\\n            Krikun\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Qin\\n           </forename>\\n           <surname>\\n            Cao\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Klaus\\n           </forename>\\n           <surname>\\n            Gao\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <surname>\\n            Macherey\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno type=\"arXiv\">\\n          arXiv:1609.08144\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2016\">\\n          </date>\\n         </imprint>\\n        </monogr>\\n        <note type=\"report_type\">\\n         arXiv preprint\\n        </note>\\n       </biblstruct>\\n       <biblstruct xml:id=\"b31\">\\n        <monogr>\\n         <title level=\"m\" type=\"main\">\\n          Deep recurrent models with fast-forward connections for neural machine translation\\n         </title>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Jie\\n           </forename>\\n           <surname>\\n            Zhou\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Ying\\n           </forename>\\n           <surname>\\n            Cao\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Xuguang\\n           </forename>\\n           <surname>\\n            Wang\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Peng\\n           </forename>\\n           <surname>\\n            Li\\n           </surname>\\n          </persname>\\n         </author>\\n         <author>\\n          <persname xmlns=\"http://www.tei-c.org/ns/1.0\">\\n           <forename type=\"first\">\\n            Wei\\n           </forename>\\n           <surname>\\n            Xu\\n           </surname>\\n          </persname>\\n         </author>\\n         <idno>\\n          abs/1606.04199\\n         </idno>\\n         <imprint>\\n          <date type=\"published\" when=\"2016\">\\n          </date>\\n          <publisher>\\n           CoRR\\n          </publisher>\\n         </imprint>\\n        </monogr>\\n       </biblstruct>\\n      </listbibl>\\n     </div>\\n    </back>\\n   </text>\\n  </tei>\\n </body>\\n</html>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tei_doc = \"../papers-xmls/Learning_to_Optimize_Combinatorial_Functions.tei.xml\"\n",
    "#tei_doc = \"../papers-xmls/Learning_a_Lexicon_and_Translation_Model_from_Phoneme_Lattices.tei.xml\"\n",
    "with open(tei_doc, 'r') as tei:\n",
    "    soup1 = BeautifulSoup(tei, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<head>Introduction</head>,\n",
       " <head>Related Work</head>,\n",
       " <head>Problem Formulation and Approach</head>,\n",
       " <head>Two Stage Learning</head>,\n",
       " <head>Smart Predict then Optimize (SPO)</head>,\n",
       " <head>Combinatorial problems and scaling up</head>,\n",
       " <head>Experimental Evaluation</head>,\n",
       " <head>RQ1: exact versus weaker oracles</head>,\n",
       " <head>Instance</head>,\n",
       " <head>RQ2 benefits of warmstarting</head>,\n",
       " <head>RQ3: SPO versus QPTL</head>,\n",
       " <head>RQ4: Suitability on large, hard optimisation instances</head>,\n",
       " <head>Conclusions and future work</head>,\n",
       " <head>Figure 1 :</head>,\n",
       " <head>Figure 3 :</head>,\n",
       " <head>Figure 5 :</head>,\n",
       " <head>Table 1 :</head>,\n",
       " <head>Table 2 :</head>,\n",
       " <head>Acknowledgments</head>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup1.find_all('head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Introduction',\n",
       " 'Related Work',\n",
       " 'Problem Formulation and Approach',\n",
       " 'Two Stage Learning',\n",
       " 'Smart Predict then Optimize (SPO)',\n",
       " 'Combinatorial problems and scaling up',\n",
       " 'Experimental Evaluation',\n",
       " 'RQ1: exact versus weaker oracles',\n",
       " 'Instance',\n",
       " 'RQ2 benefits of warmstarting',\n",
       " 'RQ3: SPO versus QPTL',\n",
       " 'RQ4: Suitability on large, hard optimisation instances',\n",
       " 'Conclusions and future work',\n",
       " 'Figure 1 :',\n",
       " 'Figure 3 :',\n",
       " 'Figure 5 :',\n",
       " 'Table 1 :',\n",
       " 'Table 2 :']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headings=[]\n",
    "maxx=0\n",
    "for div in soup1.body.find_all(\"head\"):\n",
    "    # div is neither an appendix nor references, just plain text.\n",
    "    try:\n",
    "        if not div.get(\"type\"):\n",
    "            div_text = div.get_text(separator=' ', strip=True)\n",
    "            headings.append((div_text, div['n']))\n",
    "            if(int(div['n'].split('.')[0])>maxx):\n",
    "                maxx=int(div['n'])\n",
    "        #print(divs_text)\n",
    "    except KeyError:\n",
    "        headings.append(div_text)\n",
    "        pass\n",
    "    \n",
    "headings\n",
    "#maxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(divs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': ['Introduction Combinatorial optimization aims to optimize an objective function over a set of feasible solutions defined on a discrete space. Numerous real-life decision-making problems can be formulated as combinatorial optimization problems (Korte et al. 2012; Trevisan 2011) . In the last decade, development of time-efficient algorithms for combinatorial optimization problems paved the way for these algorithms to be widely utilized in industry, including, but not limited to, in resource allocation (Angalakudati et al. 2014 ), efficient energy scheduling (Ngueveu, Artigues, and Lopez 2016), price optimization (Ferreira, Lee, and Simchi-Levi 2015) , sales promotion planning (Cohen et al. 2017) , etc. The last decade has, in parallel, witnessed a tremendous growth in machine learning (ML) methods, which can produce very accurate predictions by leveraging historical and contextual data. In real-world applications, not all parameters of an optimization problem are known at the time of Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. execution and predictive ML models can be used for estimation of those parameters from historical data. For instance, Cohen et al. first predicted future demand of products using an ML model and then use the predicted demand to compute the optimal promotion pricing scheme over the products through non-linear integer programming. When predictive ML is followed by optimization, it is often assumed that improvements in the quality of the predictions (with respect to some suitable evaluation metric) will result in better optimization outcomes. However, ML models make errors and the impact of prediction errors is not uniform throughout the underlying solution space, for example, overestimating the highest-valued prediction might not change a maximization problem outcome, while underestimating it can. Hence, a better prediction model may not ensure a better outcome in the optimization stage. In this regard, Ifrim, OSullivan, and Simonis (2012) experienced that a better predictive model does not always translate to optimized energy-saving schedules. The alternative is to take the effect of the errors on the optimization outcome into account during learning. In the context of linear programming problems, Elmachtoub and Grigas proposed an approach, called \"Smart Predict and Optimize\" (SPO), for training ML models by minimizing a convex surrogate loss function which considers the outcome of the optimization stage. Specifically they consider optimization problems where predictions occur as weights that are linear in the objective. In this work, we build on that approach and consider discrete combinatorial optimization problems. Indeed, the SPO loss is valid for any optimization problem with a linear objective over predictions, where the constraints implicitly define a convex region. Furthermore, any black box optimization method can be used as only its outcome is used to compute the (sub)gradient of the loss. The main challenge is the computational cost of the repeated solving of the optimization problem during training, namely once for every evaluation of the loss function on an instance. For NP-hard problems, this may quickly become infeasible. In order to scale up to large problem instances, we investigate the importance of finding the optimal discrete solution for learning, showing that continuous relax-ations are highly informative for learning. Furthermore, we investigate how to speed up the learning by transfer learning from easier-to-learn models as well as method for speeding up the solving by warmstarting from earlier solutions. Our approach outperforms the state-of-the-art Melding approach (Wilder, Dilkina, and Tambe 2019) in most cases, and for the first time we are able to show the applicability of predict-and-optimize on large scale combinatorial instances, namely from the ICON energy-cost aware scheduling challenge (Simonis et al. ).'],\n",
       " '2': ['Related Work Predict-and-optimize problems arise in many applications. Current practice is to use a two-stage approach where the ML models are trained independent of the optimization problem. As a consequence, the ML models do not account for the optimization tasks (Wang et al. 2006; Mukhopadhyay et al. 2017) . In recent years there is a growing interest in decision-focused learning (Elmachtoub and Grigas 2017; Demirović et al. 2019; , that aims to couple ML and decision making. In the context of portfolio optimization, Bengio (1997) report a deep learning model fails to improve future profit when trained with respect to a standard ML loss function, but a profit-driven loss function turns out to be effective. Kao, Roy, and Yan (2009) consider an unconstrained quadratic optimisation problem, where the predicted values appear linearly with the objective. They train a linear model with respect to a combination of prediction error and optimization loss. They do not mention how this can be applied to optimization problems with constraints. A number of works aim to exploit the fact that the KKT conditions of a quadratic program (QP) define a system of linear equations around the optimal points. For instance, Donti, Amos, and Kolter (2017) propose a framework which computes the gradient of the solution of the QP with respect to the predictions by applying the implicit function theorem to differentiate through the KKT conditions around the optimal point. Wilder, Dilkina, and Tambe (2019) use the same approach, and propose its use for linear programs by adding a small quadratic term to convert it into a concave QP. They also propose a specialisation of it for submodular functions. Our work builds on the SPO approach of Elmachtoub and Grigas (2017) , where the authors provide a framework to train an ML model, which learns with respect to the error in the optimization problem. This is investigated for linear optimization problems with a convex feasible region. We will use the approach for discrete combinatorial problems with a linear objective. They are computationally expensive to solve, e.g. often N P-hard. The decision variables and search space of these problems is discrete, meaning gradients can not be computed in a straightforward manner. However, the SPO approach remains applicable as we will see. Demirović et al. (2019) investigate the predic-tion+optimisation problem for the knapsack problem, and prove that optimizing over predictions are as valid as stochastic optimisation over learned distributions, in case the predictions are used as weights in a linear objective. They further investigate possible learning approaches, and classified them into three groups: indirect approaches, which do not use knowledge of the optimisation problem; semi-direct approaches, which encode knowledge of the optimisation problem, such as the importance of ranking and direct approaches which encode or use the optimisation problem in the learning in some way (Demirović et al. 2019) . Our approach is a direct approach and we examine how to combine the best of such techniques in order to scale to large and hard combinatorial problems.'],\n",
       " '3': [\"Problem Formulation and Approach Optimizing a parameterized problem Traditional optimization algorithms work under the assumption that all the parameters are known precisely. But in a predict-and-optimize setup we assume some parameters of the optimization problem are not known. We formalize a combinatorial optimization problem as follows : v * (θ) ≡ arg min v f (v, θ) s.t. C(v, θ) (1) where θ defines the set of parameters (coefficients) of the optimization problem, v are the decision variables, f (v, θ) is an objective to be minimized, and C(v, θ) is a (set of) constraints that determine(s) the feasible region; hence v * () is an oracle that returns the optimal solution. Consider, the 0-1 knapsack, where a set of items, with their values and wights, are provided. The objective is to select a subset of items respecting a capacity constraint on the sum of weights so that the total value of the subset is maximized. The parameter set θ of the problem consists of the value and weight of each item and the total capacity. The decision variable set consists of 0-1 decision variable for each item, f is a linear sum of the variables and the item values and C describing the capacity constraint. We decompose θ = θ s ∪ θ u where θ s are the set of parameters that are observed (e.g. the weights and capacity of a knapsack) and θ u are the set of unobserved parameters (the value of the knapsack items). To predict the unobserved parameters θ u , some attributes correlated with them are observed. We are equipped with a training dataset D : {(x 1 , θ u1 ), ..., (x n , θ un )} where x i 's are vectors of attributes correlated to θ ui . An ML model m is trained to generate a predictionθ u = m(x; ω). The model m is characterized by a set of learning parameters ω. E.g. in linear regression the ω encompasses the slope and the intercept of the regression line. Once the predictedθ u are obtained, θ = θ s ∪θ u is used for the optimization problem. To ease notation, we will writeθ = (θ s ,θ u ), containing both the observed parameters and predicted parameters. Recall from Equation (1) that v * (θ) is the optimal solution using parametersθ. Then, the objective value of this solution is f (v * (θ), θ). Whereas if the actual θ is known a priori, one could obtain the actual optimal solution v * (θ). The difference in using the predicted instead of the actual values is hence measured by regret(θ,θ) ≡ f (v * (θ), θ) − f (v * (θ), θ) (2) Ideally the aim of the training should be to generate predictions which minimize this regret on unseen data.\"],\n",
       " '4': ['Two Stage Learning First we formalize a two-stage approach, which is widely used in industry and where the prediction and the optimization are performed in a decoupled manner. First the predictive model is trained with the objective of minimizing the expected loss for a suitable choice of loss function L(θ u ,θ u ) For regression, if we use squared-error as the loss function, model parameters ω are estimated by minimizing the Mean Squared Error (MSE) on the training data: L M SE = 1 n n i=1 (θ ui −θ ui ) 2 2 (3) Algorithm 1: Stochastic Batch gradient descent for the two-stage learning for regression tasks (batchsize:N ) and learning rate α repeat Sample N training datapoints for i in 1, ..., N do predictθ ui using current ω ∇L i ← (θ ui − θ ui ) //gradient of L M SE end ∇L = N i=1 ∇Li N ω ← ω − α * ∇L * ∂θu ∂ω until convergence; Training by gradient descent The process of estimating ω to minimize the loss function is executed through stochastic gradient descent algorithms 1 ,where at each epoch, ω are updated after calculating the gradient ∇L of the loss function with respect to the predictions as shown in Algorithm 1. The advantage of the two-stage approach is that training by gradient descent is straightforward. In the prediction stage the objective is to minimize the MSE-loss to generate accurate predictions without considering their impact on the final solution. Clearly this does not require solving the optimization problem during training. Model validation and early stopping with regret It is common practice to perform model validation on a separate validation set while training the model. Early stopping (Bishop 2006) is the practice of choosing the epoch with the smallest loss on the validation set as the final output of the training procedure. The objective is to avoid overfitting on training data. The performance on the validation set is also used to select hyperparameters of the ML models (Bergstra and Bengio 2012) . Considering the final task, in our setup, is minimizing the regret, we modify the two stage learning by measuring regret on the validation set for early stopping and hyperparameter selection. We call this the MSE-r approach. It is more computationally expensive than MSE given that computing regret on the validation data for every epoch requires solving the optimization problems each time.'],\n",
       " '5': ['Smart Predict then Optimize (SPO) The major drawback of the two-stage approach is it does not aim to minimize the regret, but minimizes the error between θ u andθ u directly. As Figure 1 shows, minimizing loss between θ u andθ u does not necessarily result in minimization of the regret. Early stopping with regret can avoid worsening results, but can not improve the learning. The SPO framework proposed by Elmachtoub and Grigas addresses this by integrating prediction and optimization. Note, to minimize regret(θ,θ) directly we have to find the gradient of it with respect toθ which requires differentiating the argmin operator v * (θ) in Eq. 1. This differentiation may not be feasible as v * (θ) can be discontinuous inθ and exponential in size. Consequently we can not train an ML model to minimize the regret through gradient descent. The SPO framework integrates the optimization problem into the loop of gradient descent algorithms in a clever way. In their work, Elmachtoub and Grigas consider an optimization problem with a convex feasible region S and a linear objective : v * (θ) ≡ arg min v∈S θ v (4) where the cost vector θ is not known beforehand. Following Eq. (2), the regret for such a problem when using predicted valuesθ instead of actual θ is: θ (v * (θ) − v * (θ)) , which as discussed is not differentiable. To make it differentiable, they use a convex surrogate upper bound of the regret function, which they name the SPO+ loss function L SP O+ (θ,θ). The gradient of L SP O+ (θ,θ) may not exist as it also involves the argmin operator. However, they have shown that v * (θ) − v * (2θ − θ) is a subgradient of L SP O+ (θ,θ), that is g(θ,θ) = v * (θ) − v * (2θ − θ), g(θ,θ) ∈ ∇L SP O+ (θ,θ) (5) The subgadient formulation is the key to bring the optimization problem into the loop of gradient descent as shown in algorithm 2. The difference between algorithm 1 and algorithm 2 is in their (sub)gradients. In Algorithm 1, the MSE gradient is the signed difference between the actual values Algorithm 2: Stochastic Batch gradient descent for the SPO approach for regression tasks (batchsize:N ) and learning rate α repeat Sample N training datapoints for i in 1, ..., N do predictθ ui using current ω compute v * (2θ − θ) ∇L i ← v * (θ) − v * (2θ − θ) //sub-gradient end ∇L = N i=1 ∇Li N ω ← ω − α * ∇L * ∂θu ∂ω ; until convergence; and predicted ones; in Algorithm 2 the SPO subgradient is the difference of an optimization solution obtained using the actual parameter values and another solution obtained using a convex combination of the predicted values and the true values. For the 0-1 knapsack problem, the solution of a knapsack instance is a 0-1 vector of length equal to the size of the set, where 1 represents the corresponding item is selected. In this case, the subgradient is the element-wise difference between the two solutions and if the solution using the transformed predicted values is the same as the solution using actual values, all entries of the subgradient are zero. In essence, the non-zero entries in the subgradient indicate places where the two solutions contradict. Note, to compute the subgradient for this SPO approach, the optimization problem v * (2θ − θ) needs to be solved for each training instance, while v * (θ) can be precomputed and cached. Moreover, one training instance typically contains multiple predictions. For example, if we consider a 0-1 knapsack problem with 10 items, then one training instance always contains 10 value predictions, one for each item. Furthermore, the dataset may contain thousands of training instances of 10 values each. Hence, one iteration over the training data (one epoch) requires solving hundreds of knapsack problems. As an ML model is trained over several epochs, clearly the training process is computationally expensive.'],\n",
       " '6': ['Combinatorial problems and scaling up We observe that the SPO approach and its corresponding loss function places no restriction on the type of oracle v * () used. Given that our target task is to minimize the regret of the combinatorial problem, an oracle that solves the combinatorial optimisation problem is the most natural choice. We call this approach SPO-full. Weaker oracles Repeatedly solving combinatorial problems is computationally expensive. Hence for large and hard problems, it is necessary to look at ways to reduce the solving time. As there is no restriction on the oracle used, we consider using weaker oracles during training. NP-hard problems that have a polynomial (bounded) approximation algorithm could use the approximation algorithm in the loss as a proxy instead. For example, in case of knapsack, the greedy algorithm (Dantzig 1957) . For mixed integer programming (MIP) formulations, a natural weaker oracle to use is the continuous relaxation of the problem. While disregarding the discrete part, relaxations can often identify what part of the problem is trivial (variable assignments close to 0 or 1) from what part is non-trivial. For example for knapsack, the continuous relaxation leads to very similar solutions compared to the greedy algorithm. Note that we always use the same oracle for v * (θ) and v * (2θ − θ) when computing the loss. We call the approach of using the continuous relaxation as oracle v * () SPO-relax. In case of weak MIP relaxations, one can also use a cutting plane algorithm in the root node and use the resulting tighter relaxation thereof (Ferber et al. 2019) . Other weaker oracles could also be used, for example setting a time-limit on an any-time solver and using the best solution found, or a node-limit on search algorithms. In case of mixed integer programming, we can also set a gap tolerance, which means the solver does not have to prove optimality. We call this SPO-gap. For stability of the learning, it is recommended that the solution returned by the oracle does not vary much when called with (near) identical input. Apart from changing what is being solved, we also investigate ways to warmstart the learning, and to warmstart across solver calls: Warmstarting the learning We consider warmstarting the learning by transfer learning (Pratt and Jennings 1996) , that is, to train the model with an easy to compute loss function, and then continue training it with a more difficult one. In our case, we can pre-train the model using MSE as loss, which means the predictions will already be more informed when we start using an SPO loss afterwards. More elaborate learning schemes are possible, such as curriculum learning (Thrun and Pratt 2012; Pratt and Jennings 1996) where we gradually move from easier to harder to compute loss functions, e.g. by moving from SPO-relax to SPO-gap for decreasing gaps to SPO-full. As we will see later, this is not needed for the cases we studied. Warmstarting the solving When computing the loss, we must solve both v * (θ) using the true values θ, and v * (2θ−θ). Furthermore, we know that an optimal solution to v * (θ) is also a valid (but potentially suboptimal) solution to v * (2θ − θ) as only the coefficients of the objective differ. Furthermore, if this is an optimal solution to the latter than we would achieve 0-regret, hence we can expect the solution of v * (θ) to be of decent quality for v * (2θ − θ) too. We hence want to aide the solving of v * (2θ − θ) by using the optimal solution of v * (θ). One way to do this for CP solvers is solution-guided search (Demirovíc, Chu, and Stuckey 2018) . For MIP/LP we can use warmstart-ing (Yildirim and Wright 2002; Zeilinger, Jones, and Morari 2011) , that is, to use the previous solution as starting point for MIP. In case of linear programming (and hence the relaxation), we can reuse the basis of the solution. An alternative is to use the true solution to compute a bound on the objective function. Indeed, as the solution to S = v * (θ) is valid for v * (2θ − θ) and has an objective value of f (S, (2θ − θ)). Hence, we can use this as a bound on the objective and potentially cut away a large part of the search space. While the true solutions v * (θ) can be cached, we must compute this solution once for each training instance (x, θ) i , which may already take significant time for large problems. We observe that only the objective changes between calls to the oracle v * , and hence any previously computed solution is also a candidate solution for the other calls. We can hence use warmstarting for any solver call after the first, and from any previously computed solution so far.'],\n",
       " '7': ['Experimental Evaluation We consider three types of combinatorial problems: unweighted and weighted knapsack and energy-cost aware scheduling. Below we briefly discuss the problem formulations: Unweighted/weighted knapsack problem The knapsack problem can be formalized as argmax X V X s.t. W X ≤ c. The values V will be predicted from data and weights W and capacity c are given. In the unweighted knapsack, all weights W are 1 and the problem is polynomial time solvable. Weighted knapsacks are NP-hard and it is known that the computational difficulty increases with the correlation between weights and values (Pisinger 2005) . We generated mildly correlated knapsacks as follows: for each of the 48 half-hour slots we assign a weight w i by sampling from the set {3, 5, 7}, then we multiply each profit value v i by its corresponding weight and include some randomness by adding Gaussian noise ξ ∼ N(0, 25) to each v i before multiplying by weight. In the unweighted case, we consider 9 different capacity values c, namely from 5 to 45 increasing by 5. For the weighted knapsack experiment, we consider 7 different capacity values from 30 to 210 increasing by 30. Energy-cost aware scheduling It is a resourceconstrained job scheduling problem where the goal is to minimize (predicted) energy cost, it is described in the CSPLib as problem 059 (Simonis et al. ) . In summary, we are given a number of machines and have to schedule a given number of tasks, where each task has a duration, an earliest start and a latest end, resource requirement and a power usage. Each machine has a resource capacity constraint. We omit startup/shutdown costs. No task is allowed to stretch over midnight between two days and cannot be interrupted once started, nor migrate to another machine. Time is discretized in t timeslots and a schedule has to be made over all timeslots at once. For each timeslot, a (predicted) energy cost is given and the objective is to minimize the total energy cost of running the tasks on the machines. We consider two variants of the problem: easy instances consisting of 30 minute timeslots (e.g. 48 timeslots per day), and hard instances as used in the ICON energy challenge consisting of 5 minute timeslots (288 timeslots per day). The easy instances have 3 machines and respectively 10, 15 and 20 tasks. The hard instances each have 10 machines and 200 tasks. Data Our data is drawn from the Irish Single Electricity Market Operator (SEMO) (Ifrim, OSullivan, and Simonis 2012) . This dataset consists of historical energy price data at 30-minute intervals starting from Midnight 1st November, 2011 to 31st December, 2013. Each instance of the data has calendar attributes; day-ahead estimates of weather characteristics; SEMO day-ahead forecasted energy-load, windenergy production and prices; and actual wind-speed, temperature, CO 2 intensity and price. Of the actual attributes, we keep only the actual price, and use it as a target for prediction. For the hard scheduling instances, each 5-minute timeslots have the same price as the 30-minute timeslot it belongs to, following the ICON challenge. Experimental setup For all our experiments, we use a linear model without any hidden layer as the underlying predictive model. Note, the SPO approach is a model-free approach and it is compatible wih any deep neural network; but earlier work (Ifrim, OSullivan, and Simonis 2012) showed accuracy in predictions is not effective for the downstream optimization task. For the experiments, we divide our data into three sets: training (70%), validation (10%) and test (20%), and evaluate the performance by measuring regret on the test set. Training, validation and test data covers 552, 60 and 177 days of energy data respectively. Our model is trained by batch gradient descent, where each batch corresponds to one day, that is, 48 consecutive training instances namely one for each half hour of that day. This batch together forms one set of parameters of one optimisation instance, e.g. knapsack values or schedule costs. The learning rate and momentum for each model are selected through a grid search based on the regret on the validation dataset. The best combination of parameters is then used in the experiments shown. Solving the knapsack instances takes sub-second time per optimisation instance, solving the easy scheduling problems takes 0.1 to 2 seconds per optimisation instance and for the hard scheduling problems solving just the relaxation already costs 30 to 150 seconds per optimisation instance. For the latter, this means that merely evaluating regret on the test-set of 177 optimisation instances takes about 3 hours. With 552 training instances, one epoch of training requires 9 hours. For all experiments except the hard instances, we repeat it 10 times and report on the mean and standard deviation. We use the Gurobi optimization-solver for solving the combinatorial problems, the nn module in Pytorch to implement the predictive models and the optim module in Pytorch for training the models with corresponding loss functions. Experiments were run on Intel(R) Xeon(R) CPU E3-1225 v5 @ 3.30GHz processors with 32GB memory 2 .'],\n",
       " '8': ['RQ1: exact versus weaker oracles The first research question is what the loss in accuracy of solving the full discrete problems (SPO-full) versus solving only the relaxation (SPO-relax) during training is. Together with this, we look at what the gain is in terms of reductionin-regret over time. We visualise both through the learning curves as evaluated on the test set. For all methods, we compute the exact regret on the test-set, e.g. by fully solving the instances with the predictions. Figure 2 shows the learning curves over epochs, where one epoch is one iteration over all instances of the training data, for three problem instances. We also include the regret of MSE-r as baseline. In all three case we see that MSE-r is worse, and stagnates or slightly decreases in performance over the epochs. This validates the use of MSE-r where the best epoch is chosen retrospectively based on a validation set. It also validates that the SPO-surrogate loss function captures the essence of the intended loss, namely regret. We also see, surprisingly, that SPO-relax achieves very similar performance to SPO-full on all problem instances in our experiments. This means that even though SPO can reason over exact discrete solutions, reasoning over these continuous relaxation solutions is sufficient for the loss function to guide the learning to where it matters. The real advantage of SPO-relax over SPO-full is evident from Figure 3 . Here we present the test regret not against'],\n",
       " '9': ['Instance Baseline MSE-warmstart Warmstart from earlier basis 1 6.5 (1.5) sec 8 (0.5) sec 1.5 (0.2) sec 2 7 (1.5) sec 6 (1.0) sec 1 (0.2) sec 3 10 (0.5) sec 12 (1.0) sec 2.5 (0.1) sec show SPO-relax runs, and hence converges, much quicker in time than SPO. This is thanks to the fact that solving the continuous relaxation can be done in polynomial time while the discrete problem is worst-case exponential. SPO-relax is slower than MSE-r but the difference in quality clearly justifies it. In the subsequent experiments, we will use SPO-relax only.'],\n",
       " '10': ['RQ2 benefits of warmstarting As baseline we use the standard SPO-relax approach. We test warmstarting the learning by first training the network with MSE as loss function for 6 epochs, after which we continue learning with SPO-relax. We indicate this approach by MSE-warmstart. We summarizes the effect of warmstarting in Table 1 , We observe that warmstarting from MSE results in a slightly faster start in the initial seconds, but this has no benefit, nor penalty over the longer run. Warmstarting from an earlier basis, after the MIP pre-solving, did result in runtime improvements overall. We also attempted warmstarting by adding objective cuts, but this slowed down the solving of the relaxation, often doubling it, because more iterations were needed.'],\n",
       " '11': ['RQ3: SPO versus QPTL Next, we compare our approach against the state-of-the-art QP approach (QPTL) of Wilder, Dilkina, and Tambe (2019) which proposes to transform the discrete linear integer program into a continuous QP by taking the continuous relaxation and a squared L2-norm of the decision variables ||x|| 2 2 . This makes the problem quadratic and twice differentiable allowing them to use the differntiable QP solver (Donti, Amos, and Kolter 2017) . Figure 4 shows the average regret on all unweighted and weighted knapsack instances and easy scheduling instances. We can see that for unweighted knapsack, SPO-relax almost always outperforms the other methods, while QPTL performs worse than MSE-r. For weighted knapsacks, SPOrelax is best for all but the lower capacities. For these lower capacities, QPTL is better though its results worsen for higher capacities. The same narrative is reflected in Figure 5 . In Figure 5c (weighted knapsack, capacity:60) QPTL converges to a better solution than SPO. In all other cases SPO-relax produces better quality of solution and in most cases QPTL converges slower than SPO-relax. The poor quality of QPTL at higher capacities may stem from the squared norm which favors sparse solutions, while at high capacities, most items will be included and the best solutions are those that identify which items not to include. On two energy-scheduling instances SPO-relax performs better whereas for the other instance, the regrets of SPOrelax and QPTL are similar. From Figure 5f and 5e, we can see, again, SPO-relax converges faster than QPTL.'],\n",
       " '12': ['RQ4: Suitability on large, hard optimisation instances While SPO-relax performs well across the combinatorial instances used so far, these are still toy-level problems with relatively few decision variables that can be solved in a few seconds. We will use the large-scale optimization instances of the ICON challenge, for which no exact solutions are known. Hence, for this experiment we will report the regret when solving the relaxation of the problem for the test instances, rather than solving the discrete problem during testing as in the previous experiments. We impose a timelimit of 6 hours on the total time budget that SPO-relax can spend on calling the solver. This includes the time to compute and cache the ground-truth solution of Table 2 , for 5 hard scheduling instances. First, we show the test (relaxed) regret after 2, 4, 6 and 8 MSE-r epochs. The results show that the test regret slightly decreases over the epochs; thereafter, we observed, regret tends to increase. With SPO-relax, in 6 hours, it was possible to train only on 300 to 450 different instances, which is only 50 to 80% of the training instances. Table 2 shows even for a limited solving budget of 6 hour and without MSE-warmstarting, it already outperforms the MSE learned models. This shows that even on very large and hard instances that are computationally expensive to solve, training with SPOrelax on a limited time-budget is better than training in a two-stage approach with a non optimisation-directed loss.'],\n",
       " '13': ['Conclusions and future work Smart \"Predict and Optimize\" methods have shown to be able to learn from, and improve task loss. Extending these techniques to be applicable beyond toy problems, more specifically hard combinatorial problems, is essential to the applicability of this promising idea. SPO is able to outperform QPTL and lends itself to a wide applicability as it allows for the use of black-box oracles in its loss computation. We investigated the use of weaker oracles and showed that for the problems studied, learning with SPO loss while solving the relaxation leads to equal performance as solving the discrete combinatorial problem. We have shown how this opens the way to solve larger and more complex combinatorial problems, for which solving the exact solution may not be possible, let alone to do so repeatedly. In case of problems with weaker relaxations, one could consider adding cutting planes prior to solving (Ferber et al. 2019) . Moreover, further improvements could be achieved by exploiting the fact that all previously computed solutions are valid candidates. So far we have only used this for warmstarting the solver. Our work hence encourages more research into the use of weak oracles and relaxation methods, especially those that can benefit from repeated solving. One interesting direction are local search methods and other iterative refinement methods, as they can improve the solutions during the loss computation. With respect to exact methods, knowledge compilation methods such as (relaxed) BDDs could offer both a runtime improvement from faster repeat solving and employing a relaxation.'],\n",
       " '14': ['Acknowledgments We would like to thank the anonymous reviewers for the valuable comments and suggestions. This research is supported by Data-driven logistics (FWO-S007318N).']}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d={}\n",
    "j=0\n",
    "max_t_now=0\n",
    "for i in range(len(divs_text)):\n",
    "    j=j+1\n",
    "    if(i<len(headings) and type(headings[i])==tuple):\n",
    "        if(max_t_now<int(headings[i][1].split('.')[0])):\n",
    "                max_t_now=int(headings[i][1].split('.')[0])\n",
    "                \n",
    "        if(headings[i][1].split('.')[0] not in d.keys()):\n",
    "            x=divs_text[i].split(' ')[len(headings[i][0].split(' ')):]\n",
    "            y=' '.join(x)\n",
    "            d[headings[i][1].split('.')[0]]=[(headings[i][0], y)]\n",
    "        else:\n",
    "            x=divs_text[i].split(' ')[len(headings[i][0].split(' ')):]\n",
    "            y=' '.join(x)\n",
    "            d[headings[i][1].split('.')[0]].append((headings[i][0], y))\n",
    "    \n",
    "    elif (max_t_now<maxx and i<len(headings)):\n",
    "        x=divs_text[i].split(' ')[len(headings[i].split(' ')):]\n",
    "        y=' '.join(x)\n",
    "        d[str(max_t_now)].append((headings[i], y))\n",
    "        \n",
    "    else:\n",
    "        #print(divs_text[i])\n",
    "        d[str(j)]=[divs_text[i]]\n",
    "        \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_files(d, dest):\n",
    "    for key in sorted(d):\n",
    "        #text_file = open(d[key][0][0]+\".txt\", \"w\")\n",
    "        #print(d[key][0])\n",
    "        put=\"\"\n",
    "        if(type(d[key][0])==tuple):\n",
    "            x=d[key][0][0]\n",
    "            y=key+'_'+('_'.join(x.split(' ')))\n",
    "            for i in d[key]:\n",
    "                put=put+\" \"+i[1]    # from subsection take subsection text\n",
    "        else:\n",
    "            x=str(key)\n",
    "            y=x\n",
    "            put=d[key][0]\n",
    "        \n",
    "        print(y)  #filename is y\n",
    "        print(put)\n",
    "        text_file = open(dest+y, \"w\")\n",
    "        n = text_file.write(put)\n",
    "        text_file.close()\n",
    "\n",
    "##for abstract seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_Introduction\n",
      " Sentiment analysis has become a fundamental area of research in Natural Language Processing thanks to the proliferation of user-generated content in the form of online reviews, blogs, internet forums, and social media. A plethora of methods have been proposed in the literature that attempt to distill sentiment information from text, allowing users and service providers to make opinion-driven decisions. The success of neural networks in a variety of applications (Bahdanau et al., 2015; Le and Mikolov, 2014; Socher et al., 2013) and the availability of large amounts of labeled data have led to an increased focus on sentiment classification. Supervised models are typically trained on documents (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b; Tang et al., 2015; Yang et al., 2016) , sentences (Kim, 2014) , or phrases (Socher et al., 2011; [Rating: ] I had a very mixed experience at The Stand. The burger and fries were good. The chocolate shake was divine: rich and creamy. The drive-thru was horrible. It took us at least 30 minutes to order when there were only four cars in front of us. We complained about the wait and got a half-hearted apology. I would go back because the food is good, but my only hesitation is the wait. Summary + The burger and fries were good + The chocolate shake was divine + I would go back because the food is good -The drive-thru was horrible -It took us at least 30 minutes to order Figure 1 : An EDU-based summary of a 2-out-of-5 star review with positive and negative snippets. Socher et al., 2013) annotated with sentiment labels and used to predict sentiment in unseen texts. Coarse-grained document-level annotations are relatively easy to obtain due to the widespread use of opinion grading interfaces (e.g., star ratings accompanying reviews). In contrast, the acquisition of sentence-or phrase-level sentiment labels remains a laborious and expensive endeavor despite its relevance to various opinion mining applications, e.g., detecting or summarizing consumer opinions in online product reviews. The usefulness of finer-grained sentiment analysis is illustrated in the example of Figure 1 , where snippets of opposing polarities are extracted from a 2-star restaurant review. Although, as a whole, the review conveys negative sentiment, aspects of the reviewer's experience were clearly positive. This goes largely unnoticed when focusing solely on the review's overall rating. In this work, we consider the problem of segmentlevel sentiment analysis from the perspective of Multiple Instance Learning (MIL; Keeler, 1991) . Instead of learning from individually labeled segments, our model only requires document-level supervision and learns to introspectively judge the sentiment of constituent segments. Beyond showing how to utilize document collections of rated reviews to train fine-grained sentiment predictors, we also investigate the granularity of the extracted segments. Previous research (Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Nallapati et al., 2017) has predominantly viewed documents as sequences of sentences. Inspired by recent work in summarization (Li et al., 2016) and sentiment classification (Bhatia et al., 2015) , we also represent documents via Rhetorical Structure Theory's (Mann and Thompson, 1988) Elementary Discourse Units (EDUs). Although definitions for EDUs vary in the literature, we follow standard practice and take the elementary units of discourse to be clauses (Carlson et al., 2003) . We employ a state-of-the-art discourse parser (Feng and Hirst, 2012) to identify them. Our contributions in this work are three-fold: a novel multiple instance learning neural model which utilizes document-level sentiment supervision to judge the polarity of its constituent segments; the creation of SPOT, a publicly available dataset which contains Segment-level POlariTy annotations (for sentences and EDUs) and can be used for the evaluation of MIL-style models like ours; and the empirical finding (through automatic and human-based evaluation) that neural multiple instance learning is superior to more conventional neural architectures and other baselines on detecting segment sentiment and extracting informative opinions in reviews. 1\n",
      "2_Background\n",
      " Our work lies at the intersection of multiple research areas, including sentiment classification, opinion mining and multiple instance learning. We review related work in these areas below. Sentiment Classification Sentiment classification is one of the most popular tasks in sentiment analysis. Early work focused on unsupervised methods and the creation of sentiment lexicons (Turney, 2002; Hu and Liu, 2004; Wiebe et al., 2005; Baccianella et al., 2010) based on which the overall po-larity of a text can be computed (e,g., by aggregating the sentiment scores of constituent words). More recently, Taboada et al. (2011) introduced SO-CAL, a state-of-the-art method that combines a rich sentiment lexicon with carefully defined rules over syntax trees to predict sentence sentiment. Supervised learning techniques have subsequently dominated the literature (Pang et al., 2002; Pang and Lee, 2005; Qu et al., 2010; Xia and Zong, 2010; Wang and Manning, 2012; Le and Mikolov, 2014) thanks to user-generated sentiment labels or large-scale crowd-sourcing efforts (Socher et al., 2013) . Neural network models in particular have achieved state-of-the-art performance on various sentiment classification tasks due to their ability to alleviate feature engineering. Kim (2014) introduced a very successful CNN architecture for sentence-level classification, whereas other work (Socher et al., 2011; Socher et al., 2013) uses recursive neural networks to learn sentiment for segments of varying granularity (i.e., words, phrases, and sentences). We describe Kim's (2014) approach in more detail as it is also used as part of our model. Let x i denote a k-dimensional word embedding of the i-th word in text segment s of length n. The segment's input representation is the concatenation of word embeddings x 1 , . . . , x n , resulting in word matrix X. Let X i:i+j refer to the concatenation of embeddings x i , . . . , x i+j . A convolution filter W ∈ R lk , applied to a window of l words, produces a new feature c i = ReLU(W • X i:i+l + b), where ReLU is the Rectified Linear Unit non-linearity, '•' denotes the entrywise product followed by a sum over all elements and b ∈ R is a bias term. Applying the same filter to every possible window of word vectors in the segment, produces a feature map c = [c 1 , c 2 , . . . , c n−l+1 ]. Multiple feature maps for varied window sizes are applied, resulting in a fixed-size segment representation v via max-overtime pooling. We will refer to the application of convolution to an input word matrix X, as CNN(X). A final sentiment prediction is produced using a softmax classifier and the model is trained via backpropagation using sentence-level sentiment labels. The availability of large-scale datasets (Diao et al., 2014; Tang et al., 2015) has also led to the development of document-level sentiment classifiers which exploit hierarchical neural representations. These are obtained by first building representations of sentences and aggregating those into a document feature vector (Tang et al., 2015) . Yang et al. (2016) further acknowledge that words and sentences are deferentially important in different contexts. They present a model which learns to attend (Bahdanau et al., 2015) to individual text parts when constructing document representations. We describe such an architecture in more detail as we use it as a point of comparison with our own model. Given document d comprising segments (s 1 , . . . , s m ), a Hierarchical Network with attention (henceforth HIERNET; based on Yang et al., 2016) produces segment representations (v 1 , . . . , v m ) which are subsequently fed into a bidirectional GRU module (Bahdanau et al., 2015) , whose resulting hidden vectors (h 1 , . . . , h m ) are used to produce attention weights (a 1 , . . . , a m ) (see Section 3.2 for more details on the attention mechanism). A document is represented as the weighted average of the segments' hidden vectors v d = i a i h i . A final sentiment prediction is obtained using a softmax classifier and the model is trained via back-propagation using document-level sentiment labels. The architecture is illustrated in Figure 2 (a). In their proposed model, Yang et al. (2016) use bidirectional GRU modules to represent segments as well as documents, whereas we use a more efficient CNN encoder to compose words into segment vectors 2 (i.e., v i = CNN(X i )). Note that models like HIERNET do not naturally predict sentiment for individual segments; we discuss how they can be used for segment-level opinion extraction in Section 5.2. Our own work draws inspiration from representation learning (Tang et al., 2015; Kim, 2014) , especially the idea that not all parts of a document convey sentiment-worthy clues (Yang et al., 2016) . Our model departs from previous approaches in that it provides a natural way of predicting the polarity of individual text segments without requiring segment-level annotations. Moreover, our attention mechanism directly facilitates opinion detection rather than simply aggregating sentence representations into a single document vector. Opinion Mining A standard setting for opinion mining and summarization (Lerman et al., 2009; Carenini et al., 2006; Ganesan et al., 2010; Di Fabbrizio et al., 2014; Gerani et al., 2014 ) assumes a set of documents that contain opinions about some entity of interest (e.g., camera). The goal of the system is to generate a summary that is representative of the average opinion and speaks to its important aspects (e.g., picture quality, battery life, value). Output summaries can be extractive (Lerman et al., 2009) or abstractive (Gerani et al., 2014; Di Fabbrizio et al., 2014) and the underlying systems exhibit varying degrees of linguistic sophistication from identifying aspects (Lerman et al., 2009 ) to using RSTstyle discourse analysis, and manually defined templates (Gerani et al., 2014; Di Fabbrizio et al., 2014) . Our proposed method departs from previous work in that it focuses on detecting opinions in individual documents. Given a review, we predict the polarity of every segment, allowing for the extraction of sentiment-heavy opinions. We explore the usefulness of EDU segmentation inspired by Li et al. (2016) , who show that EDU-based summaries align with near-extractive summaries constructed by news editors. Importantly, our model is trained in a weakly-supervised fashion on large scale document classification datasets without recourse to finegrained labels or gold-standard opinion summaries. Multiple Instance Learning Our models adopt a Multiple Instance Learning (MIL) framework. MIL deals with problems where labels are associated with groups of instances or bags (documents in our case), while instance labels (segment-level polarities) are unobserved. An aggregation function is used to combine instance predictions and assign labels on the bag level. The goal is either to label bags (Keeler and Rumelhart, 1992; Dietterich et al., 1997; Maron and Ratan, 1998) or to simultaneously infer bag and instance labels (Zhou et al., 2009; Wei et al., 2014; Kotzias et al., 2015) . We view segment-level sentiment analysis as an instantiation of the latter variant. Initial MIL efforts for binary classification made the strong assumption that a bag is negative only if all of its instances are negative, and positive otherwise (Dietterich et al., 1997; Maron and Ratan, 1998; Zhang et al., 2002; Andrews and Hofmann, 2004; Carbonetto et al., 2008) . Subsequent work re-laxed this assumption, allowing for prediction combinations better suited to the tasks at hand. Weidmann et al. (2003) introduced a generalized MIL framework, where a combination of instance types is required to assign a bag label. Zhou et al. (2009) used graph kernels to aggregate predictions, exploiting relations between instances in object and text categorization. Xu and Frank (2004) proposed a multiple-instance logistic regression classifier where instance predictions were simply averaged, assuming equal and independent contribution toward bag classification. More recently, Kotzias et al. (2015) used sentence vectors obtained by a pre-trained hierarchical CNN (Denil et al., 2014) as features under an unweighted average MIL objective. Prediction averaging was further extended by Pappas and Popescu-Belis (2014; , who used a weighted summation of predictions, an idea which we also adopt in our work. Applications of MIL are many and varied. MIL was first explored by Keeler and Rumelhart (1992) for recognizing handwritten post codes, where the position and value of individual digits was unknown. MIL techniques have since been applied to drug activity prediction (Dietterich et al., 1997) , image retrieval (Maron and Ratan, 1998; Zhang et al., 2002) , object detection (Zhang et al., 2006; Carbonetto et al., 2008; Cour et al., 2011) , text classification (Andrews and Hofmann, 2004), image captioning (Wu et al., 2015) , paraphrase detection (Xu et al., 2014) , and information extraction (Hoffmann et al., 2011) . When applied to sentiment analysis, MIL takes advantage of supervision signals on the document level in order to train segment-level sentiment predictors. Although their work is not couched in the framework of MIL, Täckström and McDonald (2011) show how sentence sentiment labels can be learned as latent variables from document-level annotations using hidden conditional random fields. Pappas and Popescu-Belis (2014) use a multiple instance regression model to assign sentiment scores to specific aspects of products. The Group-Instance Cost Function (GICF), proposed by Kotzias et al. (2015) , averages sentence sentiment predictions during trainng, while ensuring that similar sentences receive similar polarity labels. Their work uses a pre-trained hierarchical CNN to obtain sentence embeddings, but is not trainable end-to-end, in contrast with our proposed network. Additionally, none of the aforementioned efforts explicitly evaluate opinion extraction quality.\n",
      "23\n",
      "Acknowledgments The authors gratefully acknowledge the support of the European Research Council (award number 681760). We thank TACL action editor Ani Nenkova and the anonymous reviewers whose feedback helped improve the present paper, as well as Charles Sutton, Timothy Hospedales, and members of EdinburghNLP for helpful discussions and suggestions.\n",
      "3_Methodology\n",
      " In this section we describe how multiple instance learning can be used to address some of the drawbacks seen in previous approaches, namely the need for expert knowledge in lexicon-based sentiment analysis (Taboada et al., 2011) , expensive finegrained annotation on the segment level (Kim, 2014; Socher et al., 2013) or the inability to naturally predict segment sentiment (Yang et al., 2016) . Under multiple instance learning (MIL), a dataset D is a collection of labeled bags, each of which is a group of unlabeled instances. Specifically, each document d is a sequence (bag) of segments (instances). This sequence d = (s 1 , s 2 , . . . , s m ) is obtained from a document segmentation policy (see Section 4 for details). A discrete sentiment label y d ∈ [1, C] is associated with each document, where the labelset is ordered and classes 1 and C correspond to maximally negative and maximally positive sentiment. It is assumed that y d is an unknown function of the unobserved segment-level labels: y d = f (y 1 , y 2 , . . . , y m ) (1) Probabilistic sentiment classifiers will produce document-level predictionsŷ d by selecting the most probable class according to class distribution p d = p (1) d , . . . , p (C) d . In a non-MIL framework a classifier would learn to predict the document's sentiment by directly conditioning on its segments' feature representations or their aggregate: p d =f θ (v 1 , v 2 , . . . , v m ) (2) In contrast, a MIL classifier will produce a class distribution p i for each segment and additionally learn to combine these into a document-level prediction: p i =ĝ θs (v i ) , (3) p d =f θ d (p 1 , p 2 , . . . , p m ) . (4) In this work,ĝ andf are defined using a single neural network, described below. Hierarchical neural models like HIERNET have been used to predict document-level polarity by first encoding sentences and then combining these representations into a document vector. Hierarchical vector composition produces powerful sentiment predictors, but lacks the ability to introspectively judge the polarity of individual segments. Our Multiple Instance Learning Network (henceforth MILNET) is based on the following intuitive assumptions about opinionated text. Each segment conveys a degree of sentiment polarity, ranging from very negative to very positive. Additionally, segments have varying degrees of importance, in relation to the overall opinion of the author. The overarching polarity of a text is an aggregation of segment polarities, weighted by their importance. Thus, our model attempts to predict the polarity of segments and decides which parts of the document are good indicators of its overall sentiment, allowing for the detection of sentiment-heavy opinions. An illustration of MILNET is shown in Figure 2 (b); the model consists of three components: a CNN segment encoder, a softmax segment classifier and an attentionbased prediction weighting module. v i = CNN(X i ) is produced for each segment, using the CNN architecture described in Section 2. Segment Classification Obtaining a separate representation v i for every segment in a document allows us to produce individual segment sentiment predictions p i = p (1) i , . . . , p (C) i . This is achieved using a softmax classifier: p i = softmax(W c v i + b c ) , (5) where W c and b c are the classifier's parameters, shared across all segments. Individual distributions p i are shown in Figure 2 (b) as small bar-charts. In the simplest case, document-level predictions can be produced by taking the average of segment class distributions: p (c) d = 1 / m i p (c) i , c ∈ [1, C] . This is, however, a crude way of combining segment sentiment, as not all parts of a document convey important sentiment clues. We opt for a segment attention mechanism which rewards text units that are more likely to be good sentiment predictors. Our attention mechanism is based on a bidirectional GRU component (Bahdanau et al., 2015) and 21 The starters were quite bland. I didn't enjoy most of them, but the burger was brilliant! inspired by Yang et al. (2016) . However, in contrast to their work, where attention is used to combine sentence representations into a single document vector, we utilize a similar technique to aggregate individual sentiment predictions. We first use separate GRU modules to produce forward and backward hidden vectors, which are then concatenated: − → h i = − −− → GRU(v i ), (6) ← − h i = ← −− − GRU(v i ), (7) h i = [ − → h i , ← − h i ], i ∈ [1, m] . (8) The importance of each segment is measured with the aid of a vector h a , as follows: h i = tanh(W a h i + b a ) , (9) a i = exp(h T i h a ) i exp(h T i h a ) , (10) where Equation (9) defines a one-layer MLP that produces an attention vector for the i-th segment. Attention weights a i are computed as the normalized similarity of each h i with h a . Vector h a , which is randomly initialized and learned during training, can be thought of as a trained key, able to recognize sentiment-heavy segments. The attention mechanism is depicted in the dashed box of Figure 2 , with attention weights shown as shaded circles. Finally, we obtain a document-level distribution over sentiment labels as the weighted sum of segment distributions (see top of Figure 2 (b)): p (c) d = i a i p (c) i , c ∈ [1, C] . (11) Training The model is trained end-to-end on documents with user-generated sentiment labels. We use the negative log likelihood of the document-level prediction as an objective function: L = − d log p (y d ) d (12)\n",
      "4_Polarity-based_Opinion_Extraction\n",
      " After training, our model can produce segment-level sentiment predictions for unseen texts in the form of class probability distributions. A direct application of our method is opinion extraction, where highly positive and negative snippets are selected from the original document, producing extractive sentiment summaries, as described below. Polarity Scoring In order to extract opinion summaries, we need to rank segments according to their sentiment polarity. We introduce a method that takes our model's confidence in the prediction into account, by reducing each segment's class probability distribution p i to a single real-valued polarity score. To achieve this, we first define a real-valued class weight vector w = w (1) , . . . , w (C) | w (c) ∈ [−1, 1] that assigns uniformly-spaced weights to the ordered labelset, such that w (c+1) − w (c) = 2 C−1 . For example, in a 5-class scenario, the class weight vector would be w = −1, −0.5, 0, 0.5, 1 . We compute the polarity score of a segment as the dot-product of the probability distribution p i with vector w: polarity(s i ) = c p (c) i w (c) ∈ [−1, 1] (13) Gated Polarity As a way of increasing the effectiveness of our method, we introduce a gated extension that uses the attention mechanism of our model to further differentiate between segments that carry 22 significant sentiment cues and those that do not: gated-polarity(s i ) = a i · polarity(s i ) , (14) where a i is the attention weight assigned to the i-th segment. This forces the polarity scores of segments the model does not attend to closer to 0. An illustration of our polarity scoring function is provided in Figure 3 , where the class predictions (top) of three restaurant review segments are mapped to their corresponding polarity scores (bottom). We observe that our method produces the desired result; segments 1 and 2 convey negative sentiment and receive negative scores, whereas the third segment is mapped to a positive score. Although the same discrete class label is assigned to the first two, the second segment's score is closer to 0 (neutral) as its class probability mass is more evenly distributed. As mentioned earlier, one of the hypotheses investigated in this work regards the use of subsentential units as the basis of extraction. Specifically, our model was applied to sentences and Elementary Discourse Units (EDUs), obtained from a Rhetorical Structure Theory (RST) parser (Feng and Hirst, 2012). According to RST, documents are first segmented into EDUs corresponding roughly to independent clauses which are then recursively combined into larger discourse spans. This results in a tree representation of the document, where connected nodes are characterized by discourse relations. We only utilize RST's segmentation, and leave the potential use of the tree structure to future work. The example in Figure 3 illustrates why EDUbased segmentation might be beneficial for opinion extraction. The second and third EDUs correspond to the sentence: I didn't enjoy most of them, but the burger was brilliant. Taken as a whole, the sentence conveys mixed sentiment, whereas the EDUs clearly convey opposing sentiment.\n",
      "5_Experimental_Setup\n",
      " In this section we describe the data used to assess the performance of our model. We also give details on model training and comparison systems. Our models were trained on two large-scale sentiment classification collections. The Yelp'13 corpus was introduced in Tang et al. (2015) and contains customer reviews of local businesses, each associated with human ratings on a scale from 1 (negative) to 5 (positive). The IMDB corpus of movie reviews was obtained from Diao et al. (2014) ; each review is associated with user ratings ranging from 1 to 10. Both datasets are split into training (80%), validation (10%) and test (10%) sets. A summary of statistics for each collection is provided in Table 1 . In order to evaluate model performance on the segment level, we constructed a new dataset named SPOT (as a shorthand for Segment POlariTy) by annotating documents from the Yelp'13 and IMDB collections. Specifically, we sampled reviews from each collection such that all document-level classes are represented uniformly, and the document lengths are representative of the respective corpus. Documents were segmented into sentences and EDUs, resulting in two segment-level datasets per collection. Statistics are summarized in Table 2 . Each review was presented to three Amazon Mechanical Turk (AMT) annotators who were asked to judge the sentiment conveyed by each segment (i.e., sentence or EDU) as negative, neutral, or pos- itive. We assigned labels using a majority vote or a fourth annotator in the rare cases of no agreement (< 5%). Figure 4 shows the distribution of segment labels for each document-level class. As expected, documents with positive labels contain a larger number of positive segments compared to documents with negative labels and vice versa. Neutral segments are distributed in an approximately uniform manner across document classes. Interestingly, the proportion of neutral EDUs is significantly higher compared to neutral sentences. The observation reinforces our argument in favor of EDU segmentation, as it suggests that a sentence with positive or negative overall polarity may still contain neutral EDUs. Discarding neutral EDUs, could therefore lead to more concise opinion extraction compared to relying on entire sentences. We further experimented on two collections introduced by Kotzias et al. (2015) which also originate from the YELP'13 and IMDB datasets. Each collection consists of 1,000 randomly sampled sentences annotated with binary sentiment labels. On the task of segment classification we compared MILNET, our multiple instance learning network, against the following methods: Majority: Majority class applied to all instances. State-of-the-art lexicon-based system that classifies segments into positive, neutral, and negative classes (Taboada et al., 2011) . Fully-supervised CNN segment classifier trained on SPOT's labels (Kim, 2014) . The Group-Instance Cost Function model introduced in Kotzias et al. (2015) . This is an unweighted average prediction aggregation MIL method that uses sentence features from a pretrained convolutional neural model. HIERNET: HIERNET does not explicitly generate individual segment predictions. Segment polarity scores are obtained by assigning the documentlevel prediction to every segment. We can then produce finer-grained polarity distinctions via gating, using the model's attention weights. We further illustrate the differences between HI-ERNET and MILNET in Figure 5 , which includes short descriptions and simplified equations for each model. MILNET naturally produces distinct segment polarities, while HIERNET assigns a single polarity score to every segment. In both cases, gating is a further means of identifying neutral segments. Finally, we differentiate between variants of HI-ERNET and MILNET according to: Polarity source: Controls whether we assign polarities via segment-specific or document-wide predictions. HIERNET only allows for documentwide predictions. MILNET can use both. Attention: We use models without gating (no subscript), with gating (gt subscript) as well as models trained with the attention mechanism disabled, falling back to simple averaging (avg subscript). We trained MILNET and HIERNET using Adadelta (Zeiler, 2012) for 25 epochs. Mini-batches of 200 documents were organized based on the reviews' segment and document lengths so the amount of padding was minimized. We used 300-dimensional pre-trained word2vec embeddings. We tuned hyperparameters on the validation sets of the document classification collections, resulting in the following configuration (unless otherwise noted). For the CNN segment encoder, we used window sizes of 3, 4 and 5 words with 100 feature maps per window size, resulting in 300-dimensional segment vectors. The GRU hidden vector dimensions for each direction were set to 50 and the attention vector dimensionality to 100. We used L2-normalization and dropout to regularize the softmax classifiers and additional dropout on the internal GRU connections. Real-valued polarity scores produced by the two models are mapped to discrete labels using two appropriate thresholds t 1 , t 2 ∈ [−1, 1], so that a segment s is classified as negative if polarity(s) < t 1 , positive if polarity(s) > t 2 or neutral otherwise. 3 To evaluate performance, we use macro-averaged F1 which is unaffected by class imbalance. We select optimal thresholds using 10-fold cross-validation and report mean scores across folds. The fully-supervised convolutional segment classifier (Seg-CNN) uses the same window size and feature map configuration as our segment encoder. Seg-CNN was trained on SPOT using segment labels directly and 10-fold cross-validation (identical folds as in our main models). Seg-CNN is not directly comparable to MILNET (or HIERNET) due to differences in supervision type (segment vs. document labels) and training size (1K-2K segment labels vs. ∼250K document labels). However, the comparison is indicative of the utility of fine-grained sentiment predictors that do not rely on expensive segment-level annotations.\n",
      "6_Results\n",
      " We evaluated models in two ways. We first assessed their ability to classify segment polarity in reviews using the newly created SPOT dataset and, additionally, the sentence corpora of Kotzias et al. (2015) . Our second suite of experiments focused on opinion extraction: we conducted a judgment elicitation study to determine whether extracts produced by MILNET are useful and of higher quality compared to HIERNET and other baselines. We were also interested to find out whether EDUs provide a better basis for opinion extraction than sentences. Table 3 summarizes our results. The first block in the table reports the performance of the majority class baseline. The second block considers models that do not utilize segment-level predictions, namely HIERNET which assigns polarity scores to segments using its document-level predictions, as well as the variant of MILNET which similarly uses document-level predictions only (Equation (11) ). In the third block, MILNET's segment-level predictions are used. Each block further differentiates between three levels of attention integration, as previ- Table 3 : Segment classification results (in macroaveraged F1). † indicates that the system in question is significantly different from MILNET gt (approximate randomization test (Noreen, 1989) , p < 0.05). ously described. The final block shows the performance of SO-CAL and the Seg-CNN classifier. When considering models that use documentlevel supervision, MILNET with gated, segmentspecific polarities obtains the best classification performance across all four datasets. Interestingly, it performs comparably to Seg-CNN, the fullysupervised segment classifier, which provides additional evidence that MILNET can effectively identify segment polarity without the need for segmentlevel annotations. Our model also outperforms the strong SO-CAL baseline in all but one datasets which is remarkable given the expert knowledge and linguistic information used to develop the latter. Document-level polarity predictions result in lower classification performance across the board. Differences between the standard hierarchical and multiple instance networks are less pronounced in this case, as MILNET loses the advantage of producing segment-specific sentiment predictions. Models without attention perform worse in most cases. The use of gated polarities benefits all model configurations, indicating the method's ability to selectively focus on segments with significant sentiment cues. We further analyzed the polarities assigned by MILNET and HIERNET to positive, negative, and Table 5 : Accuracy scores on the sentence classification datasets introduced in Kotzias et al. (2015) . neutral segments. Figure 6 illustrates the distribution of polarity scores produced by the two models on the Yelp'13 dataset (sentence segmentation). In the case of negative and positive sentences, both models demonstrate appropriately skewed distributions. However, the neutral class appears to be particularly problematic for HIERNET, where polarity scores are scattered across a wide range of values. In contrast, MILNET is more successful at identifying neutral sentences, as its corresponding distribution has a single mode near zero. Attention gating addresses this issue by moving the polarity scores of sentiment-neutral segments towards zero. This is illustrated in Table 4 where we observe that gated variants of both models do a better job at identifying neutral segments. The effect is very significant for HIERNET, while MILNET benefits slightly and remains more effective overall. Similar trends were observed in all four SPOT datasets. In order to examine the effect of training size, we trained multiple models using subsets of the original document collections. We trained on five random subsets for each training size, ranging from 100 documents to the full training set, and tested segment classification performance on SPOT. The results, averaged across trials, are presented in Figure 7 . With the exception of the IMDB EDU-segmented dataset, MILNET only requires a few thousand training documents to outperform the supervised Seg-CNN. HI-ERNET follows a similar curve, but is inferior to MILNET. A reason for MILNET's inferior performance on the IMDB corpus (EDU-split) can be lowquality EDUs, due to the noisy and informal style of language used in IMDB reviews. Finally, we compared MILNET against the GICF model (Kotzias et al., 2015) on their Yelp and IMDB sentence sentiment datasets. 4 Their model requires sentence embeddings from a pre-trained neural model. We used the hierarchical CNN from their work (Denil et al., 2014) and, additionally, pre-trained HIERNET and MILNET sentence embeddings. The results in Table 5 show that MILNET outperforms all variants of GIFC. Our models also seem to learn better sentence embeddings, as they improve GICF's performance on both collections. 4 GICF only handles binary labels, which makes it unsuitable for the full-scale comparisons in Table 3 . Here, we binarize our training datasets and use same-sized sentence embeddings for all four models (R 150 for Yelp, R 72 for IMDB). Informativeness Table 6 : Human evaluation results (in percentages). † indicates that the system in question is significantly different from MILNET (sign-test, p < 0.01). Polarity In our opinion extraction experiments, AMT workers (all native English speakers) were shown an original review and a set of extractive, bullet-style summaries, produced by competing systems using a 30% compression rate. Participants were asked to decide which summary was best according to three criteria: Informativeness (Which summary best captures the salient points of the review?), Polarity (Which summary best highlights positive and negative comments?) and Coherence (Which summary is more coherent and easier to read?). Subjects were allowed to answer \"Unsure\" in cases where they could not discriminate between summaries. We used all reviews from our SPOT dataset and collected three responses per document. We ran four judgment elicitation studies: one comparing HIERNET and MILNET when summarizing reviews segmented as sentences, a second one comparing the two models with EDU segmentation, a third which compares EDU-and sentence-based summaries produced by MILNET, and a fourth where EDU-based summaries from MILNET were compared to a LEAD (the first N words from each document) and a RAN-DOM (random EDUs) baseline. ] As with any family-run hole in the wall, service can be slow. What the staff lacked in speed, they made up for in charm. The food was good, but nothing wowed me. I had the Pierogis while my friend had swedish meatballs. Both dishes were tasty, as were the sides. One thing that was disappointing was that the food was a a little cold (lukewarm). The restaurant itself is bright and clean. I will go back again when i feel like eating outside the box.  ence for MILNET across criteria. The second block shows significant preference for MILNET against HIERNET on informativeness and polarity, whereas HIERNET was more often preferred in terms of coherence, although the difference is not statistically significant. The third block compares sentence and EDU summaries produced by MILNET. EDU summaries were perceived as significantly better in terms of informativeness and polarity, but not coherence. This is somewhat expected as EDUs tend to produce more terse and telegraphic text and may seem unnatural due to segmentation errors. In the fourth block we observe that participants find MIL-NET more informative and better at distilling polarity compared to the LEAD and RANDOM (EDUs) baselines. We should point out that the LEAD system is not a strawman; it has proved hard to outperform by more sophisticated methods (Nenkova, 2005) , particularly on the newswire domain. Example EDU-and sentence-based summaries produced by gated variants of HIERNET and MIL-NET are shown in Figure 8 , with attention weights and polarity scores of the extracted segments shown in round and square brackets respectively. For both granularities, HIERNET's positive document-level prediction results in a single polarity score assigned to every segment, and further adjusted using the corresponding attention weights. The extracted segments are informative, but fail to capture the negative sentiment of some segments. In contrast, MIL-NET is able to detect positive and negative snippets via individual segment polarities. Here, EDU segmentation produced a more concise summary with a clearer grouping of positive and negative snippets.\n",
      "7_Conclusions\n",
      " In this work, we presented a neural network model for fine-grained sentiment analysis within the framework of multiple instance learning. Our model can be trained on large scale sentiment classification datasets, without the need for segment-level labels. As a departure from the commonly used vector-based composition, our model first predicts sentiment at the sentence-or EDU-level and subsequently combines predictions up the document hierarchy. An attention-weighted polarity scoring technique provides a natural way to extract sentimentheavy opinions. Experimental results demonstrate the superior performance of our model against more conventional neural architectures. Human evaluation studies also show that MILNET opinion extracts are preferred by participants and are effective at capturing informativeness and polarity, especially when using EDU segments. In the future, we would like to focus on multi-document, aspect-based extraction (Cao et al., 2017) and ways of improving the coherence of our summaries by taking into account more fine-grained discourse information (Daumé III and Marcu, 2002) . 28\n"
     ]
    }
   ],
   "source": [
    "make_files(d,\"OUT/Multiple_Instance_Learning_Networks_for_Fine-Grained_Sentiment_Analysis/\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"sample.txt\", \"w\")\n",
    "n = text_file.write(soup.abstract.getText(separator=' ', strip=True))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. * Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. † Work performed while at Google Brain.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.abstract.getText(separator=' ', strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this-is-a-sentence'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = ['this','is','a','sentence']\n",
    "'-'.join(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Introduction']\n",
      "Introduction\n",
      "['Background']\n",
      "Background\n",
      "['Model', 'Architecture']\n",
      "Model Architecture\n",
      "['Encoder', 'and', 'Decoder', 'Stacks']\n",
      "Encoder and Decoder Stacks\n",
      "['Attention']\n",
      "Attention\n",
      "['Scaled', 'Dot-Product', 'Attention']\n",
      "Scaled Dot-Product Attention\n",
      "['Multi-Head', 'Attention']\n",
      "Multi-Head Attention\n",
      "['Applications', 'of', 'Attention', 'in', 'our', 'Model']\n",
      "Applications of Attention in our Model\n",
      "['Position-wise', 'Feed-Forward', 'Networks']\n",
      "Position-wise Feed-Forward Networks\n",
      "['Embeddings', 'and', 'Softmax']\n",
      "Embeddings and Softmax\n",
      "['Positional', 'Encoding']\n",
      "Positional Encoding\n",
      "['Why', 'Self-Attention']\n",
      "Why Self-Attention\n",
      "['Training']\n",
      "Training\n",
      "['Training', 'Data', 'and', 'Batching']\n",
      "Training Data and Batching\n",
      "['Hardware', 'and', 'Schedule']\n",
      "Hardware and Schedule\n",
      "['Optimizer']\n",
      "Optimizer\n",
      "['Regularization']\n",
      "Regularization\n",
      "['Results']\n",
      "Results\n",
      "['Machine', 'Translation']\n",
      "Machine Translation\n",
      "['Model', 'Variations']\n",
      "Model Variations\n",
      "['Conclusion']\n",
      "Conclusion\n",
      "['Acknowledgements']\n",
      "F\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(divs_text)):\n",
    "    x=divs_text[i]\n",
    "    y=headings[i][0]\n",
    "    \n",
    "    print(x.split(' ')[0:len(y.split(' '))])\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-28-fa22775edf15>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-fa22775edf15>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    for i in divs_text:\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "final=[]\n",
    "for i in divs_text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': ['Introduction Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5] . Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13] . Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h t , as a function of the previous hidden state h t−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [18] and conditional computation [26] , while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16] . In all but a few cases [22] , however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.'],\n",
       " '2': ['Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20] , ByteNet [15] and ConvS2S [8] , all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [11] . In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19] . End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28] . To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8] .'],\n",
       " '3': ['Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29] . Here, the encoder maps an input sequence of symbol representations (x 1 , ..., x n ) to a sequence of continuous representations z = (z 1 , ..., z n ). Given z, the decoder then generates an output sequence (y 1 , ..., y m ) of symbols one element at a time. At each step the model is auto-regressive [9] , consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1 , respectively.',\n",
       "  'Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [10] around each of the two sub-layers, followed by layer normalization [1] . That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d model = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.',\n",
       "  'Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. query with all keys, divide each by √ d k , and apply a softmax function to obtain the weights on the values.',\n",
       "  'Scaled Dot-Product Attention In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as: Attention(Q, K, V ) = softmax( QK T √ d k )V (1) The two most commonly used attention functions are additive attention [2] , and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1 √ d k . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of d k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d k [3] . We suspect that for large values of d k , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4 . To counteract this effect, we scale the dot products by 1 √ d k .',\n",
       "  'Multi-Head Attention Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k , d k and d v dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2 . Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. MultiHead(Q, K, V ) = Concat(head 1 , ..., head h )W O where head i = Attention(QW Q i , KW K i , V W V i ) Where the projections are parameter matrices W Q i ∈ R dmodel×d k , W K i ∈ R dmodel×d k , W V i ∈ R dmodel×dv and W O ∈ R hdv×dmodel . In this work we employ h = 8 parallel attention layers, or heads. For each of these we use d k = d v = d model /h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.',\n",
       "  'Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8 ].',\n",
       "  'Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d model = 512, and the inner-layer has dimensionality d f f = 2048.',\n",
       "  'Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model . We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [24] . In the embedding layers, we multiply those weights by √ d model .',\n",
       "  'Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the Table 1 : Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n 2 · d) O(1) O(1) Recurrent O(n · d 2 ) O(n) O(n) Convolutional O(k · n · d 2 ) O(1) O(log k (n)) Self-Attention (restricted) O(r · n · d) O(1) O(n/r) bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [8] . In this work, we use sine and cosine functions of different frequencies: where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P E pos+k can be represented as a linear function of P E pos . We also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.'],\n",
       " '4': ['Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1 , ..., x n ) to another sequence of equal length (z 1 , ..., z n ), with x i , z i ∈ R d , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [11] . Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1 , a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(log k (n)) in the case of dilated convolutions [15] , increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6] , however, decrease the complexity considerably, to O(k · n · d + n · d 2 ). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.'],\n",
       " '5': ['Training This section describes the training regime for our models.',\n",
       "  'Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3] , which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [31] . Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.',\n",
       "  'Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, (described on the  bottom line of table 3) , step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).',\n",
       "  'Optimizer We used the Adam optimizer [17] with β 1 = 0.9, β 2 = 0.98 and = 10 −9 . We varied the learning rate over the course of training, according to the formula: lrate = d −0.5 model · min(step_num −0.5 , step_num · warmup_steps −1.5 ) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.',\n",
       "  'Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0.1. Label Smoothing During training, we employed label smoothing of value ls = 0.1 [30] . This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.'],\n",
       " '6': ['Results',\n",
       "  'Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2 ) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3 . Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P drop = 0.1, instead of 0.3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [31] . These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [31] .',\n",
       "  'Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3 . In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size d k hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [8] , and observe nearly identical results to the base model.'],\n",
       " '7': ['Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.'],\n",
       " '22': ['Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.'],\n",
       " 2: [1]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#d={}\n",
    "if(2 not in d.keys()):\n",
    "    d[2]=[1]\n",
    "else:\n",
    "    d[2].append(2)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#for div in soup1.body.find_all(\"head\"):\n",
    "#    print(type(div))\n",
    "#    print(div.name)\n",
    "#    print(div['n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
