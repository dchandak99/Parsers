{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 1 soup:\n",
    "tei_doc = 'sample.tei.xml'\n",
    "with open(tei_doc, 'r') as tei:\n",
    "    soup = BeautifulSoup(tei, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.abstract.getText(separator=' ', strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alan Turing authored many influential publications in computer science.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helpers:\n",
    "def read_tei(tei_file):\n",
    "    with open(tei_file, 'r') as tei:\n",
    "        soup = BeautifulSoup(tei, 'lxml')\n",
    "        return soup\n",
    "    raise RuntimeError('Cannot generate a soup from the input')\n",
    "\n",
    "def elem_to_text(elem, default=''):\n",
    "    if elem:\n",
    "        return elem.getText()\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Person:\n",
    "    firstname: str\n",
    "    middlename: str\n",
    "    surname: str\n",
    "\n",
    "turing_author = Person(firstname='Alan', middlename='M', surname='Turing')\n",
    "\n",
    "f\"{turing_author.firstname} {turing_author.surname} authored many influential publications in computer science.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEIFile(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.soup = read_tei(filename)\n",
    "        self._text = None\n",
    "        self._title = ''\n",
    "        self._abstract = ''\n",
    "\n",
    "    @property\n",
    "    def doi(self):\n",
    "        idno_elem = self.soup.find('idno', type='DOI')\n",
    "        if not idno_elem:\n",
    "            return ''\n",
    "        else:\n",
    "            return idno_elem.getText()\n",
    "\n",
    "    @property\n",
    "    def title(self):\n",
    "        if not self._title:\n",
    "            self._title = self.soup.title.getText()\n",
    "        return self._title\n",
    "\n",
    "    @property\n",
    "    def abstract(self):\n",
    "        if not self._abstract:\n",
    "            abstract = self.soup.abstract.getText(separator=' ', strip=True)\n",
    "            self._abstract = abstract\n",
    "        return self._abstract\n",
    "\n",
    "    @property\n",
    "    def authors(self):\n",
    "        authors_in_header = self.soup.analytic.find_all('author')\n",
    "\n",
    "        result = []\n",
    "        for author in authors_in_header:\n",
    "            persname = author.persname\n",
    "            if not persname:\n",
    "                continue\n",
    "            firstname = elem_to_text(persname.find(\"forename\", type=\"first\"))\n",
    "            middlename = elem_to_text(persname.find(\"forename\", type=\"middle\"))\n",
    "            surname = elem_to_text(persname.surname)\n",
    "            person = Person(firstname, middlename, surname)\n",
    "            result.append(person)\n",
    "        return result\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        divs_text = []\n",
    "        if not self._text:\n",
    "            \n",
    "            for div in self.soup.body.find_all(\"div\")[1:]:\n",
    "                # div is neither an appendix nor references, just plain text.\n",
    "                if not div.get(\"type\"):\n",
    "                    div_text = div.get_text(separator=' ', strip=True)\n",
    "                    divs_text.append(div_text)\n",
    "\n",
    "            plain_text = \" \".join(divs_text)\n",
    "            self._text = divs_text\n",
    "        return self._text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tei = TEIFile('sample2.tei.xml')\n",
    "f\"The authors of the paper entitled '{tei.title}' are {tei.authors}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_divs(soup):\n",
    "    divs_text=[]\n",
    "    for div in soup.body.find_all(\"div\")[1:]:\n",
    "        # div is neither an appendix nor references, just plain text.\n",
    "        if not div.get(\"type\"):\n",
    "            div_text = div.get_text(separator=' ', strip=True)\n",
    "            divs_text.append(div_text)\n",
    "        #rint(divs_text)\n",
    "    return divs_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tei_doc = 'sample2.tei.xml'\n",
    "with open(tei_doc, 'r') as tei:\n",
    "    soup1 = BeautifulSoup(tei, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_head(soup1, maxx):    \n",
    "    headings=[]\n",
    "    for div in soup1.body.find_all(\"head\"):\n",
    "        # div is neither an appendix nor references, just plain text.\n",
    "        try:\n",
    "            if not div.get(\"type\"):\n",
    "                div_text = div.get_text(separator=' ', strip=True)\n",
    "                headings.append((div_text, div['n']))\n",
    "                if(int(div['n'].split('.')[0])>maxx):\n",
    "                    maxx=int(div['n'].split('.')[0])\n",
    "            #print(divs_text)\n",
    "        except KeyError:\n",
    "            headings.append(div_text)\n",
    "            pass\n",
    "    \n",
    "    return headings, maxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_d(divs_text, headings, maxx):\n",
    "    d={}\n",
    "    j=0\n",
    "    max_t_now=0\n",
    "    for i in range(len(divs_text)):\n",
    "        j=j+1\n",
    "        if(i<len(headings) and type(headings[i])==tuple):\n",
    "            if(max_t_now<int(headings[i][1].split('.')[0])):\n",
    "                max_t_now=int(headings[i][1].split('.')[0])\n",
    "                \n",
    "            if(headings[i][1].split('.')[0] not in d.keys()):\n",
    "                x=divs_text[i].split(' ')[len(headings[i][0].split(' ')):]\n",
    "                y=' '.join(x)\n",
    "                d[headings[i][1].split('.')[0]]=[(headings[i][0], y)]\n",
    "            else:\n",
    "                x=divs_text[i].split(' ')[len(headings[i][0].split(' ')):]\n",
    "                y=' '.join(x)\n",
    "                d[headings[i][1].split('.')[0]].append((headings[i][0], y))\n",
    "    \n",
    "        elif (max_t_now<maxx and i<len(headings)):\n",
    "            x=divs_text[i].split(' ')[len(headings[i].split(' ')):]\n",
    "            y=' '.join(x)\n",
    "            d[str(max_t_now)].append((headings[i], y))\n",
    "        \n",
    "        else:\n",
    "            #print(divs_text[i])\n",
    "            d[str(j)]=[divs_text[i]]\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCFNet__Deep_Neural_Network_with_Decomposed_Convolutional_Filters\n",
      "Modeling_Semantic_Expectation__Using_Script_Knowledge_for_Referent_Prediction\n",
      "EQUATION_PARSING___Mapping_Sentences_to_Grounded_Equations\n",
      "Multi-task_Learning_with_Labeled_and_Unlabeled_Tasks\n",
      "CoSimRank__A_Flexible___Efficient_Graph-Theoretic_Similarity_Measure\n",
      "Miscommunication_Recovery_in_Physically_Situated_Dialogue\n",
      "An_Empirical_Study_of_Self-Disclosure_in_Spoken_Dialogue_Systems\n",
      "Targeted_Syntactic_Evaluation_of_Language_Models\n",
      "Supervised_Learning_of_Automatic_Pyramid_for_Optimization-Based_Multi-Document_Summarization\n",
      "TextFlow__A_Text_Similarity_Measure_based_on_Continuous_Sequences\n",
      "Colorless_green_recurrent_networks_dream_hierarchically\n",
      "Stochastic_Gradient_Monomial_Gamma_Sampler\n",
      "Comparing_Dynamics__Deep_Neural_Networks_versus_Glassy_Systems\n",
      "RESIDE__Improving_Distantly-Supervised_Neural_Relation_Extraction_using_Side_Information\n",
      "Trainable_Greedy_Decoding_for_Neural_Machine_Translation\n",
      "Bayesian_Uncertainty_Estimation_for_Batch_Normalized_Deep_Networks\n",
      "Count-Based_Exploration_with_Neural_Density_Models\n",
      "The_Argument_Reasoning_Comprehension_Task__Identification_and_Reconstruction_of_Implicit_Warrants\n",
      "Real-Time_Adaptive_Image_Compression\n",
      "Polyglot_Semantic_Role_Labeling\n",
      "Cross-lingual_Abstract_Meaning_Representation_Parsing\n",
      "Learning_to_Aggregate_Ordinal_Labels_by_Maximizing_Separating_Width\n",
      "Sequential_Dialogue_Context_Modeling_for_Spoken_Language_Understanding\n",
      "Delayed_Impact_of_Fair_Machine_Learning\n",
      "Learning_Latent_Semantic_Annotations_for_Grounding_Natural_Language_to_Structured_Data\n",
      "Variational_Inference_and_Model_Selection__with_Generalized_Evidence_Bounds\n",
      "Stein_Variational_Message_Passing_for_Continuous_Graphical_Models\n",
      "Dance_Dance_Convolution\n",
      "Inference_Suboptimality_in_Variational_Autoencoders\n",
      "Anchoring_and_Agreement_in_Syntactic_Annotations\n",
      "Beyond_Error_Propagation_in_Neural_Machine_Translation__Characteristics_of_Language_Also_Matter\n",
      "A_Simulated_Annealing_Based_Inexact_Oracle_for_Wasserstein_Loss_Minimization\n",
      "Unfolding_and_Shrinking_Neural_Machine_Translation_Ensembles\n",
      "Tensor_Fusion_Network_for_Multimodal_Sentiment_Analysis\n",
      "Satirical_News_Detection_and_Analysis_using_Attention_Mechanism_and_Linguistic_Features\n",
      "Ordinal_Graphical_Models__A_Tale_of_Two_Approaches\n",
      "The_Best_of_Both_Worlds__Combining_Recent_Advances_in_Neural_Machine_Translation\n",
      "Massively_Parallel_Algorithms_and_Hardness_for_Single-Linkage_Clustering_under_`p_Distances\n",
      "Tensor-Train_Recurrent_Neural_Networks_for_Video_Classification\n",
      "Graph_Convolutional_Encoders_for_Syntax-aware_Neural_Machine_Translation\n",
      "AMR-to-text_Generation_with_Synchronous_Node_Replacement_Grammar\n",
      "Learning_to_Disentangle_Interleaved_Conversational_Threads_with_a_Siamese_Hierarchical_Network_and_Similarity_Ranking\n",
      "Why_Neural_Translations_are_the_Right_Length\n",
      "Knowledgeable_Reader__Enhancing_Cloze-Style_Reading_Comprehension_with_External_Commonsense_Knowledge\n",
      "Collecting_Diverse_Natural_Language_Inference_Problems_for_Sentence_Representation_Evaluation\n",
      "CARER__Contextualized_Affect_Representations_for_Emotion_Recognition\n",
      "Cross-Sentence_N_-ary_Relation_Extraction_with_Graph_LSTMs\n",
      "Detecting_Egregious_Conversations_between_Customers_and_Virtual_Agents\n",
      "Information-Theory_Interpretation_of_the_Skip-Gram_Negative-Sampling_Objective_Function\n",
      "diaNED__Time-Aware_Named_Entity_Disambiguation_for_Diachronic_Corpora\n",
      "Failures_of_Gradient-Based_Deep_Learning\n",
      "Semi-Supervised_QA_with_Generative_Domain-Adaptive_Nets\n",
      "An_Alternative_Softmax_Operator_for_Reinforcement_Learning\n",
      "On_the_Sampling_Problem_for_Kernel_Quadrature\n",
      "A_Framework_for_Representing_Language_Acquisition_in_a_Population_Setting\n",
      "SQL-Rank__A_Listwise_Approach_to_Collaborative_Ranking\n",
      "Rationalizing_Neural_Predictions\n",
      "Differentially_Private_Clustering_in_High-Dimensional_Euclidean_Spaces\n",
      "Probabilistic_Path_Hamiltonian_Monte_Carlo\n",
      "Greedy_Search_with_Probabilistic_N-gram_Matching_for_Neural_Machine_Translation\n",
      "Learning_a_Neural_Semantic_Parser_from_User_Feedback\n",
      "Learning_Neural_Templates_for_Text_Generation\n",
      "Axiomatic_Attribution_for_Deep_Networks\n",
      "Subspace_Embedding_and_Linear_Regression_with_Orlicz_Norm\n",
      "Predicting_News_Headline_Popularity_with_Syntactic_and_Semantic_Knowledge_Using_Multi-Task_Learning\n",
      "Analytical_Guarantees_on_Numerical_Precision_of_Deep_Neural_Networks\n",
      "Matching_Citation_Text_and_Cited_Spans_in_Biomedical_Literature__a_Search-Oriented_Approach\n",
      "Conditional_Noise-Contrastive_Estimation_of_Unnormalised_Models\n",
      "A_Distributional_and_Orthographic_Aggregation_Model_for_English_Derivational_Morphology\n",
      "Deep-speare__A_joint_neural_model_of_poetic_language,_meter_and_rhyme\n",
      "Joint_Named_Entity_Recognition_and_Disambiguation\n",
      "Deriving_Boolean_structures_from_distributional_vectors\n",
      "Averaged-DQN__Variance_Reduction_and_Stabilization_for_Deep_Reinforcement_Learning\n",
      "Robust_Guarantees_of_Stochastic_Greedy_Algorithms\n",
      "Implicitly-Defined_Neural_Networks_for_Sequence_Labeling\n",
      "Finding_Syntax_in_Human_Encephalography_with_Beam_Search\n",
      "Network_Global_Testing_by_Counting_Graphlets\n",
      "Variational_Policy_for_Guiding_Point_Processes\n",
      "Representation_Learning_on_Graphs_with_Jumping_Knowledge_Networks_\n",
      "Deep_Exhaustive_Model_for_Nested_Named_Entity_Recognition\n",
      "Global_Neural_CCG_Parsing_with_Optimality_Guarantees\n",
      "Using_millions_of_emoji_occurrences_to_learn_any-domain_representations_for_detecting_sentiment,_emotion_and_sarcasm\n",
      "Any-language_frame-semantic_parsing\n",
      "Probabilistic_Typology__Deep_Generative_Models_of_Vowel_Inventories\n",
      "Natural_Language_Comprehension_with_the_EpiReader\n",
      "Multi-modal_Summarization_for_Asynchronous_Collection_of_Text,_Image,_Audio_and_Video\n",
      "Improving_Statistical_Machine_Translation_with_a_Multilingual_Paraphrase_Database\n",
      "RecipeQA__A_Challenge_Dataset_for_Multimodal_Comprehension_of_Cooking_Recipes\n",
      "Multi-Timescale_Long_Short-Term_Memory_Neural_Network_for_Modelling_Sentences_and_Documents\n",
      "Bridging_Languages_through_Images_with_Deep_Partial_Canonical_Correlation_Analysis\n",
      "Reasoning_about_Actions_and_State_Changes_by_Injecting_Commonsense_Knowledge\n",
      "Just_Sort_It!_A_Simple_and_Effective_Approach_to_Active_Preference_Learning\n",
      "Learning_to_Speed_Up_Structured_Output_Prediction\n",
      "Deep_Dirichlet_Multinomial_Regression\n",
      "Input_Convex_Neural_Networks\n",
      "Affect-LM__A_Neural_Language_Model_for_Customizable_Affective_Text_Generation\n",
      "A_Neural_Network_for_Coordination_Boundary_Prediction\n",
      "Noisy_Natural_Gradient_as_Variational_Inference\n",
      "Adaptive_Knowledge_Sharing_in_Multi-Task_Learning__Improving_Low-Resource_Neural_Machine_Translation\n",
      "Linguistically-Informed_Self-Attention_for_Semantic_Role_Labeling\n",
      "Lexicosyntactic_Inference_in_Neural_Models\n",
      "Sliced_Wasserstein_Kernel_for_Persistence_Diagrams\n",
      "Projection-free_Distributed_Online_Learning_in_Networks\n",
      "Robust_Structured_Estimation_with_Single-Index_Models\n",
      "Improved_Relation_Extraction_with_Feature-Rich_Compositional_Embedding_Models\n",
      "Pain-Free_Random_Differential_Privacy_with_Sensitivity_Sampling\n",
      "Joint_Reasoning_for_Temporal_and_Causal_Relations\n",
      "Multilabel_Classification_with_Group_Testing_and_Codes\n",
      "Head-Lexicalized_Bidirectional_Tree_LSTMs\n",
      "Variational_Dropout_Sparsifies_Deep_Neural_Networks\n",
      "Zipporah__a_Fast_and_Scalable_Data_Cleaning_System_for_Noisy_Web-Crawled_Parallel_Corpora\n",
      "Rule_Extraction_for_Tree-to-Tree_Transducers_by_Cost_Minimization\n",
      "Spectrally_Approximating_Large_Graphs_with_Smaller_Graphs\n",
      "Lazifying_Conditional_Gradient_Algorithms\n",
      "Joint_Dimensionality_Reduction_and_Metric_Learning__A_Geometric_Take\n",
      "Recurrent_Predictive_State_Policy_Networks\n",
      "Ultra-Fine_Entity_Typing\n",
      "Deep_Learning_in_Semantic_Kernel_Spaces\n",
      "Entity_Linking_for_Queries_by_Searching_Wikipedia_Sentences\n",
      "Neural_Text_Generation_in_Stories_Using_Entity_Representations_as_Context\n",
      "Learning_unknown_ODE_models_with_Gaussian_processes\n",
      "Predicting_Semantic_Relations_using_Global_Graph_Properties\n",
      "Does_Distributionally_Robust_Supervised_Learning_Give_Robust_Classifiers_\n",
      "Near_Optimal_Frequent_Directions_for_Sketching_Dense_and_Sparse_Matrices\n",
      "Meta-Learning_for_Low-Resource_Neural_Machine_Translation\n",
      "Deep_Linear_Networks_with_Arbitrary_Loss__All_Local_Minima_Are_Global\n",
      "Policy_Gradient_as_a_Proxy_for_Dynamic_Oracles_in_Constituency_Parsing\n",
      "Maximum_Selection_and_Ranking_under_Noisy_Comparisons\n",
      "Bag_of_Experts_Architectures_for_Model_Reuse_in_Conversational_Language_Understanding\n",
      "NeuralREG__An_end-to-end_approach_to_referring_expression_generation\n",
      "Kernelized_Support_Tensor_Machines\n",
      "Segmentation_for_Efficient_Supervised_Language_Annotation_with_an_Explicit_Cost-Utility_Tradeoff\n",
      "Multi-Passage_Machine_Reading_Comprehension_with_Cross-Passage_Answer_Verification\n",
      "A_Principled_Framework_for_Evaluating_Summarizers__Comparing_Models_of_Summary_Quality_against_Human_Judgments\n",
      "Approximate_Newton_Methods_and_Their_Local_Convergence\n",
      "Joint_Concept_Learning_and_Semantic_Parsing_from_Natural_Language_Explanations\n",
      "A_Nested_Attention_Neural_Hybrid_Model_for_Grammatical_Error_Correction\n",
      "Incorporating_Dialectal_Variability_for_Socially_Equitable_Language_Identification\n",
      "Soft_Actor-Critic__Off-Policy_Maximum_Entropy_Deep_Reinforcement_Learning_with_a_Stochastic_Actor\n",
      "Automatically_Generating_Rhythmic_Verse_with_Neural_Networks\n",
      "A_Richer_Theory_of_Convex_Constrained_Optimization_with_Reduced_Projections_and_Improved_Rates\n",
      "Differentiable_Dynamic_Programming_for_Structured_Prediction_and_Attention\n",
      "Submodular_Hypergraphs__p-Laplacians,_Cheeger_Inequalities__and_Spectral_Clustering\n",
      "A_Sense-Based_Translation_Model_for_Statistical_Machine_Translation\n",
      "Author_Commitment_and_Social_Power__Automatic_Belief_Tagging_to_Infer_the_Social_Context_of_Interactions\n",
      "Learning_How_to_Actively_Learn__A_Deep_Imitation_Learning_Approach\n",
      "Learning_Sentiment-Specific_Word_Embedding_for_Twitter_Sentiment_Classification\n",
      "Imitation_Learning_for_Neural_Morphological_String_Transduction\n",
      "Retrieval_of_the_Best_Counterargument_without_Prior_Topic_Knowledge\n",
      "Deep_One-Class_Classification\n",
      "Generalized_Earley_Parser__Bridging_Symbolic_Grammars_and_Sequence_Data_for_Future_Prediction\n",
      "Paraphrase_to_Explicate__Revealing_Implicit_Noun-Compound_Relations\n",
      "Exact_MAP_Inference_by_Avoiding_Fractional_Vertices\n",
      "A_Co-Matching_Model_for_Multi-choice_Reading_Comprehension\n",
      "Neural_Machine_Translation_with_Source-Side_Latent_Graph_Parsing\n",
      "LEAPSANDBOUNDS__A_Method_for_Approximately_Optimal_Algorithm_Configuration\n",
      "Representing_Text_for_Joint_Embedding_of_Text_and_Knowledge_Bases\n",
      "Modeling_Semantic_Plausibility_by_Injecting_World_Knowledge\n",
      "Concept_Transfer_Learning_for_Adaptive_Language_Understanding\n",
      "Coming_to_Your_Senses__on_Controls_and_Evaluation_Sets_in_Polysemy_Research\n",
      "ZipML__Training_Linear_Models_with_End-to-End_Low_Precision,_and_a_Little_Bit_of_Deep_Learning\n",
      "Bayesian_Optimization_of_Text_Representations\n",
      "Deep_Neural_Machine_Translation_with_Linear_Associative_Unit\n",
      "Toward_Efficient_and_Accurate_Covariance_Matrix__Estimation_on_Compressed_Data\n",
      "Sentence_Compression_for_Arbitrary_Languages_via_Multilingual_Pivoting\n",
      "Fluency_Boost_Learning_and_Inference_for_Neural_Grammatical_Error_Correction\n",
      "Exploring_the_Role_of_Prior_Beliefs_for_Argument_Persuasion\n",
      "Adapting_Kernel_Representations_Online_Using_Submodular_Maximization\n",
      "Not_All_Samples_Are_Created_Equal___Deep_Learning_with_Importance_Sampling\n",
      "Adversarial_Connective-exploiting_Networks_for_Implicit_Discourse_Relation_Classification\n",
      "Co-Training_for_Topic_Classification_of_Scholarly_Data\n",
      "Pretraining_Sentiment_Classifiers_with_Unlabeled_Dialog_Data\n",
      "KBGAN__Adversarial_Learning_for_Knowledge_Graph_Embeddings\n",
      "Detecting_Perspectives_in_Political_Debates\n",
      "Scalable_Multi-Class_Gaussian_Process_Classification__using_Expectation_Propagation\n",
      "Sequential_Matching_Network__A_New_Architecture_for_Multi-turn_Response_Selection_in_Retrieval-Based_Chatbots\n",
      "Cut_to_the_Chase__A_Context_Zoom-in_Network_for_Reading_Comprehension\n",
      "Program_Induction_by_Rationale_Generation__Learning_to_Solve_and_Explain_Algebraic_Word_Problems\n",
      "What_do_Neural_Machine_Translation_Models_Learn_about_Morphology_\n",
      "Fast_and_Scalable_Expansion_of_Natural_Language_Understanding_Functionality_for_Intelligent_Agents\n",
      "Broad-coverage_CCG_Semantic_Parsing_with_AMR\n",
      "Capacity_Releasing_Diffusion_for_Speed_and_Locality\n",
      "Linear-Time_Constituency_Parsing_with_RNNs_and_Dynamic_Programming\n",
      "Learning_Memory_Access_Patterns\n",
      "Efficient_softmax_approximation_for_GPUs\n",
      "Adversarial_Contrastive_Estimation\n",
      "Phrase-Indexed_Question_Answering__A_New_Challenge_for_Scalable_Document_Comprehension\n",
      "Automatic_Discovery_of_the_Statistical_Types_of_Variables_in_a_Dataset\n",
      "A_Theoretical_Explanation_for_Perplexing_Behaviors_of_Backpropagation-based_Visualizations\n",
      "High-Quality_Prediction_Intervals_for_Deep_Learning___A_Distribution-Free,_Ensembled_Approach\n",
      "Joint_Extraction_of_Entities_and_Relations_Based_on_a_Novel_Tagging_Scheme\n",
      "Zero-Shot_Dialog_Generation_with_Cross-Domain_Latent_Actions\n",
      "Forest-type_Regression_with_General_Losses__and_Robust_Forest\n",
      "Natasha__Faster_Non-Convex_Stochastic_Optimization__via_Strongly_Non-Convex_Parameter\n",
      "Sample-efficient_Actor-Critic_Reinforcement_Learning_with_Supervised_Data_for_Dialogue_Management\n",
      "Constrained_Interacting_Submodular_Groupings\n",
      "Semi-supervised_Chinese_Word_Segmentation_based_on_Bilingual_Information\n",
      "Convex_Phase_Retrieval_without_Lifting_via_PhaseMax\n",
      "Neural_Utterance_Ranking_Model_for_Conversational_Dialogue_Systems\n",
      "Parameterized_Algorithms_for_the_Matrix_Completion_Problem\n",
      "An_Analysis_of_Frequency-_and_Memory-Based_Processing_Costs\n",
      "Neural_Models_for_Documents_with_Metadata\n",
      "Stochastic_Variance-Reduced_Hamilton_Monte_Carlo_Methods\n",
      "Extending_a_Parser_to_Distant_Domains_Using_a_Few_Dozen_Partially_Annotated_Examples\n",
      "Improving_Sign_Random_Projections_With_Additional_Information\n",
      "Understanding_and_Detecting_Supporting_Arguments_of_Diverse_Types\n",
      "Label-aware_Double_Transfer_Learning_for_Cross-Specialty_Medical_Named_Entity_Recognition\n",
      "Session-level_Language_Modeling_for_Conversational_Speech\n",
      "Optimization,_Fast_and_Slow__Optimally_Switching_between_Local_and_Bayesian_Optimization\n",
      "Topically_Driven_Neural_Language_Model\n",
      "Numeracy_for_Language_Models__Evaluating_and_Improving_their_Ability_to_Predict_Numbers\n",
      "Deep_Generative_Model_for_Joint_Alignment_and_Word_Representation\n",
      "Unpaired_Sentiment-to-Sentiment_Translation__A_Cycled_Reinforcement_Learning_Approach\n",
      "Classifying_Relations_via_Long_Short_Term_Memory_Networks_along_Shortest_Dependency_Paths\n",
      "Frames__A_Corpus_for_Adding_Memory_to_Goal-Oriented_Dialogue_Systems\n",
      "Image-to-Markup_Generation_with_Coarse-to-Fine_Attention\n",
      "Deep_Reinforcement_Learning_for_Mention-Ranking_Coreference_Models\n",
      "Nonconvex_Optimization_for_Regression_with_Fairness_Constraints\n",
      "A__CCG_Parsing_with_a_Supertag_and_Dependency_Factored_Model\n",
      "HUME__Human_UCCA-Based_Evaluation_of_Machine_Translation\n",
      "A_Step-wise_Usage-based_Method_for_Inducing_Polysemy-aware_Verb_Classes\n",
      "Exploiting_Sentence_Similarities_for_Better_Alignments\n",
      "Neural_Related_Work_Summarization_with_a_Joint_Context-driven_Attention_Mechanism\n",
      "Bounds_on_the_Approximation_Power_of_Feedforward_Neural_Networks\n",
      "oi-VAE__Output_Interpretable_VAEs_for_Nonlinear_Group_Factor_Analysis\n",
      "Fast,_Small_and_Exact__Infinite-order_Language_Modelling_with_Compressed_Suffix_Trees\n",
      "Quickshift++__Provably_Good_Initializations_for_Sample-Based_Mean_Shift\n",
      "Dependency_Graph-to-String_Translation\n",
      "Stochastic_Convex_Optimization__Faster_Local_Growth_Implies_Faster_Global_Convergence\n",
      "Marginal_Likelihood_Training_of_BiLSTM-CRF_for_Biomedical_Named_Entity_Recognition_from_Disjoint_Label_Sets\n",
      "Mitigating_Bias_in_Adaptive_Data_Gathering_via_Differential_Privacy\n",
      "Hierarchical_Recurrent_Neural_Network_for_Document_Modeling\n",
      "Augment_and_Reduce__Stochastic_Inference_for_Large_Categorical_Distributions\n",
      "Is_this_a_wampimuk__Cross-modal_mapping_between_distributional_semantics_and_the_visual_world\n",
      "Regularising_Non-linear_Models_Using_Feature_Side-information\n",
      "On_orthogonality_and_learning_recurrent_networks_with_long_term_dependencies\n",
      "Do_latent_tree_learning_models_identify_meaningful_structure_in_sentences_\n",
      "Neural-Davidsonian_Semantic_Proto-role_Labeling\n",
      "Stochastic_Training_of_Graph_Convolutional_Networks_with_Variance_Reduction\n",
      "MEC__Memory-efficient_Convolution_for_Deep_Neural_Network_\n",
      "Tense_Manages_to_Predict_Implicative_Behavior_in_Verbs\n",
      "A_Birth-Death_Process_for_Feature_Allocation\n",
      "Knowledge_Base_Inference_using_Bridging_Entities\n",
      "Closed-form_Marginal_Likelihood_in_Gamma-Poisson_Matrix_Factorization\n",
      "Improving_the_Gaussian_Mechanism_for_Differential_Privacy__Analytical_Calibration_and_Optimal_Denoising\n",
      "Parsing_Speech__A_Neural_Approach_to_Integrating_Lexical_and_Acoustic-Prosodic_Information\n",
      "Heuristically_Informed_Unsupervised_Idiom_Usage_Recognition\n",
      "On_The_Projection_Operator_to_A_Three-view_Cardinality_Constrained_Set(1)\n",
      "Fully_Character-Level_Neural_Machine_Translation_without_Explicit_Segmentation\n",
      "Developing_Bug-Free_Machine_Learning_Systems_With_Formal_Mathematics\n",
      "Probabilistic_Boolean_Tensor_Decomposition\n",
      "Generalizing_Word_Embeddings_using_Bag_of_Subwords\n",
      "Training_Classifiers_with_Natural_Language_Explanations\n",
      "Improved_Neural_Relation_Detection_for_Knowledge_Base_Question_Answering\n",
      "Discourse_Marker_Augmented_Network_with_Reinforcement_Learning_for_Natural_Language_Inference\n",
      "Knowledge-Rich_Morphological_Priors_for_Bayesian_Language_Models\n",
      "Measuring_the_Evolution_of_a_Scientific_Field_through_Citation_Frames\n",
      "Neural_Cross-Lingual_Coreference_Resolution_And_Its_Application_To_Entity_Linking\n",
      "Batch_IS_NOT_Heavy__Learning_Word_Representations_From_All_Samples\n",
      "The_Role_of_Conversation_Context_for_Sarcasm_Detection_in_Online_Interactions\n",
      "A_Transition-Based_Directed_Acyclic_Graph_Parser_for_UCCA\n",
      "Breaking_NLI_Systems_with_Sentences_that_Require_Simple_Lexical_Inferences\n",
      "A_Binarized_Neural_Network_Joint_Model_for_Machine_Translation\n",
      "Joint_Mention_Extraction_and_Classification_with_Mention_Hypergraphs\n",
      "Learning_Simplifications_for_Specific_Target_Audiences\n",
      "When_Are_Tree_Structures_Necessary_for_Deep_Learning_of_Representations_\n",
      "Depth-Width_Tradeoffs_in_Approximating_Natural_Functions_with_Neural_Networks\n",
      "Exploiting_Rich_Syntactic_Information_for_Semantic_Parsing_with_Graph-to-Sequence_Model\n",
      "Intra-Sentential_Subject_Zero_Anaphora_Resolution_using_Multi-Column_Convolutional_Neural_Network\n",
      "Learning_attention_for_historical_text_normalization_by_learning_to_pronounce\n",
      "The_Weighted_Kendall_and_High-order_Kernels_for_Permutations\n",
      "Fast_Information-theoretic_Bayesian_Optimisation\n",
      "Towards_Binary-Valued_Gates_for_Robust_LSTM_Training\n",
      "Modelling_Protagonist_Goals_and_Desires_in_First-Person_Narrative\n",
      "Visualizing_and_Understanding_Neural_Machine_Translation\n",
      "Detecting_and_Characterizing_Events\n",
      "Selective_Inference_for_Sparse_High-Order_Interaction_Models\n",
      "Controlling_Personality-Based_Stylistic_Variation_with_Neural_Natural_Language_Generators\n",
      "DOC__Deep_Open_Classification_of_Text_Documents\n",
      "Recursive_Neural_Structural_Correspondence_Network_for_Cross-domain_Aspect_and_Opinion_Co-Extraction\n",
      "Friendships,_Rivalries,_and_Trysts__Characterizing_Relations_between_Ideas_in_Texts\n",
      "Learning_Stable_Stochastic_Nonlinear_Dynamical_Systems\n",
      "A_Spatial_Model_for_Extracting_and_Visualizing_Latent_Discourse_Structure_in_Text\n",
      "Cogent__A_Generic_Dialogue_System_Shell_Based_on_a_Collaborative_Problem_Solving_Model\n",
      "Learning_Important_Features_Through_Propagating_Activation_Differences\n",
      "Argument_Mining__Extracting_Arguments_from_Online_Dialogue\n",
      "XNLI__Evaluating_Cross-lingual_Sentence_Representations\n",
      "Unimodal_Probability_Distributions_for_Deep_Ordinal_Classification\n",
      "On_the_Spectrum_of_Random_Features_Maps_of_High_Dimensional_Data\n",
      "Experiments_with_crowdsourced_re-annotation_of_a_POS_tagging_data_set\n",
      "Fast_and_Scalable_Bayesian_Deep_Learning_by_Weight-Perturbation_in_Adam\n",
      "Local_Private_Hypothesis_Testing__Chi-Square_Tests\n",
      "Comment-to-Article_Linking_in_the_Online_News_Domain\n",
      "A_strong_baseline_for_question_relevancy_ranking\n",
      "Extracting_Condition-Opinion_Relations_Toward_Fine-grained_Opinion_Mining\n",
      "Microblog_Conversation_Recommendation_via_Joint_Modeling_of_Topics_and_Discourse\n",
      "Finding_Influential_Training_Samples_for_Gradient_Boosted_Decision_Trees\n",
      "Randomized_Greedy_Inference_for_Joint_Segmentation,_POS_Tagging_and_Dependency_Parsing\n",
      "A_Unified_Maximum_Likelihood_Approach_for_Estimating_Symmetric_Properties_of_Discrete_Distributions\n",
      "Neural_Architectures_for_Multilingual_Semantic_Parsing\n",
      "Differentiable_Programs_with_Neural_Libraries\n",
      "Multicalibration__Calibration_for_the_(Computationally-Identifiable)_Masses\n",
      "Combining_Language_and_Vision_with_a_Multimodal_Skip-gram_Model\n",
      "Learning_Semantic_Representations_for_Nonterminals_in_Hierarchical_Phrase-Based_Translation\n",
      "Zero-Shot_Transfer_Learning_for_Event_Extraction\n",
      "Deep_Multi-Task_Learning_with_Shared_Memory_for_Text_Classification\n",
      "Understanding_Black-box_Predictions_via_Influence_Functions\n",
      "CALCS__Continuously_Approximating_Longest_Common_Subsequence_for_Sequence_Level_Optimization\n",
      "Provable_Variable_Selection_for_Streaming_Features\n",
      "Solving_Geometry_Problems__Combining_Text_and_Diagram_Interpretation\n",
      "A_Structured_Learning_Approach_to_Temporal_Relation_Extraction\n",
      "Cross-topic_Argument_Mining_from_Heterogeneous_Sources\n",
      "Continuous_Representation_of_Location_for_Geolocation_and_Lexical_Dialectology_using_Mixture_Density_Networks\n",
      "Neural_Discourse_Structure_for_Text_Categorization\n",
      "Accelerated_Spectral_Ranking\n",
      "Follow_the_Moving_Leader_in_Deep_Learning\n",
      "Surprisingly_Easy_Hard-Attention_for_Sequence_to_Sequence_Learning\n",
      "Supersense_Tagging_for_Arabic__the_MT-in-the-Middle_Attack\n",
      "Video_Prediction_with_Appearance_and_Motion_Conditions\n",
      "Detect_Rumors_in_Microblog_Posts_Using_Propagation_Structure_via_Kernel_Learning\n",
      "Improving_Information_Extraction_by_Acquiring_External_Evidence_with_Reinforcement_Learning\n",
      "Spatio-temporal_Bayesian_On-line_Changepoint_Detection_with_Model_Selection_\n",
      "SQuAD__100,000+_Questions_for_Machine_Comprehension_of_Text\n",
      "CORE__Context-Aware_Open_Relation_Extraction_with_Factorization_Machines\n",
      "Efficient_and_Consistent_Adversarial_Bipartite_Matching\n",
      "Adaptive_Exploration-Exploitation_Tradeoff_for_Opportunistic_Bandits\n",
      "Faster_Greedy_MAP_Inference_for_Determinantal_Point_Processes\n",
      "An_Interpretable_Knowledge_Transfer_Model_for_Knowledge_Base_Completion\n",
      "Learning_the_Structure_of_Generative_Models_without_Labeled_Data\n",
      "Understanding_Synthetic_Gradients_and_Decoupled_Neural_Interfaces\n",
      "Content_Selection_in_Deep_Learning_Models_of_Summarization\n",
      "Mapping_to_Declarative_Knowledge_for_Word_Problem_Solving\n",
      "Sequence-Level_Knowledge_Distillation\n",
      "Scene_Graph_Parsing_as_Dependency_Parsing\n",
      "A_Hierarchical_Latent_Structure_for_Variational_Conversation_Modeling\n",
      "Selecting_Machine-Translated_Data_for_Quick_Bootstrapping_of_a_Natural_Language_Understanding_System\n",
      "Maximum_Margin_Reward_Networks_for_Learning_from_Explicit_and_Implicit_Supervision\n",
      "Parallel_and_Distributed_Thompson_Sampling_for_Large-scale_Accelerated_Exploration_of_Chemical_Space\n",
      "Generating_and_Exploiting_Large-scale_Pseudo_Training_Data_for_Zero_Pronoun_Resolution\n",
      "A_Unified_Syntax-aware_Framework_for_Semantic_Role_Labeling\n",
      "Learning_Policy_Representations_in_Multiagent_Systems\n",
      "Learning_to_Map_Context-Dependent_Sentences_to_Executable_Formal_Queries\n",
      "Multimodal_Emoji_Prediction\n",
      "Truth_of_Varying_Shades__Analyzing_Language_in_Fake_News_and_Political_Fact-Checking\n",
      "GSOS__Gauss-Seidel_Operator_Splitting_Algorithm_for__Multi-Term_Nonsmooth_Convex_Composite_Optimization\n",
      "An_Estimation_and_Analysis_Framework_for_the_Rasch_Model\n",
      "Deep_Pyramid_Convolutional_Neural_Networks_for_Text_Categorization\n",
      "Essentially_No_Barriers_in_Neural_Network_Energy_Landscape\n",
      "Improved_nearest_neighbor_search_using_auxiliary_information_and_priority_functions\n",
      "A_Multi-Axis_Annotation_Scheme_for_Event_Temporal_Relations\n",
      "Classical_Structured_Prediction_Losses_for_Sequence_to_Sequence_Learning\n",
      "Span-Based_Constituency_Parsing_with_a_Structure-Label_System_and_Provably_Optimal_Dynamic_Oracles\n",
      "Deeper_Attention_to_Abusive_User_Content_Moderation\n",
      "Unsupervised_Acquisition_of_Comprehensive_Multiword_Lexicons_using_Competition_in_an_n-gram_Lattice\n",
      "Identifying_Semantic_Divergences_in_Parallel_Text_without_Annotations\n",
      "Learning_Neural_Representation_for_CLIR_with_Adversarial_Framework\n",
      "Neural_Message_Passing_for_Quantum_Chemistry\n",
      "Dual_Iterative_Hard_Thresholding__From_Non-convex_Sparse_Minimization_to_Non-smooth_Concave_Maximization\n",
      "Cross-Lingual_Semantic_Similarity_of_Words_as_the_Similarity_of_Their_Semantic_Word_Responses\n",
      "Reducing_Gender_Bias_in_Abusive_Language_Detection\n",
      "Stochastic_Modified_Equations__and_Adaptive_Stochastic_Gradient_Algorithms\n",
      "iSurvive__An_Interpretable,_Event-time_Prediction_Model_for_mHealth\n",
      "The_Alexa_Meaning_Representation_Language\n",
      "Using_Out-of-Domain_Data_for_Lexical_Addressee_Detection_in_Human-Human-Computer_Dialog\n",
      "Did_the_Model_Understand_the_Question_\n",
      "Rumor_Detection_on_Twitter_with_Tree-structured_Recursive_Neural_Networks\n",
      "Learning_from_Clinical_Judgments__Semi-Markov-Modulated_Marked__Hawkes_Processes_for_Risk_Prognosis\n",
      "Bayesian_Model_Selection_for_Change_Point_Detection_and_Clustering\n",
      "A_Stylometric_Inquiry_into_Hyperpartisan_and_Fake_News\n",
      "The_Dynamics_of_Learning__A_Random_Matrix_Approach\n",
      "Learning_Lexico-Functional_Patterns_for_First-Person_Affect\n",
      "Multichannel_End-to-end_Speech_Recognition\n",
      "Deep_Relevance_Ranking_Using_Enhanced_Document-Query_Interactions\n",
      "Joint_Learning_for_Emotion_Classification_and_Emotion_Cause_Detection\n",
      "Know-Evolve__Deep_Temporal_Reasoning_for_Dynamic_Knowledge_Graphs\n",
      "Evaluation_Metrics_for_Machine_Reading_Comprehension__Prerequisite_Skills_and_Readability\n",
      "Object_Ordering_with_Bidirectional_Matchings_for_Visual_Reasoning\n",
      "SwitchOut__an_Efficient_Data_Augmentation_Algorithm_for_Neural_Machine_Translation\n",
      "Human-in-the-Loop_Parsing\n",
      "Reasoning_with_Sarcasm_by_Reading_In-between\n",
      "Learning_Structured_Natural_Language_Representations_for_Semantic_Parsing\n",
      "Adversarial_Training_for_Multi-task_and_Multi-lingual_Joint_Modeling_of_Utterance_Intent_Classification\n",
      "Practical_Contextual_Bandits_with_Regression_Oracles\n",
      "Tensor_Belief_Propagation\n",
      "Deletion-Robust_Submodular_Maximization__Data_Summarization_with_``the_Right_to_be_Forgotten_\n",
      "Generating_Contrastive_Referring_Expressions\n",
      "A_Stochastic_Decoder_for_Neural_Machine_Translation\n",
      "Solving_General_Arithmetic_Word_Problems\n",
      "Convergence_Analysis_of_Proximal_Gradient_with_Momentum_for_Nonconvex_Optimization\n",
      "Stochastic_Top-k_ListNet\n",
      "Automatic_Recognition_of_Conversational_Strategies_in_the_Service_of_a_Socially-Aware_Dialog_System\n",
      "Neural_Transductive_Learning_and_Beyond__Morphological_Generation_in_the_Minimal-Resource_Setting\n",
      "A_Novel_Approach_to_Part_Name_Discovery_in_Noisy_Text\n",
      "Deep_Attentive_Sentence_Ordering_Network\n",
      "SQL-to-Text_Generation_with_Graph-to-Sequence_Model\n",
      "Joint_Semantic_Synthesis_and_Morphological_Analysis_of_the_Derived_Word\n",
      "Online_Convolutional_Sparse_Coding_with_Sample-Dependent_Dictionary\n",
      "On_the_Power_of_Over-parametrization_in_Neural_Networks_with_Quadratic_Activation\n",
      "Personalized_Review_Generation_by_Expanding_Phrases_and_Attending_on_Aspect-Aware_Representations\n",
      "Parallel_Multiscale_Autoregressive_Density_Estimation\n",
      "Density-Driven_Cross-Lingual_Transfer_of_Dependency_Parsers\n",
      "Bootstrapping_Generators_from_Noisy_Data\n",
      "Language_Generation_via_DAG_Transduction\n",
      "Learning_to_Ask_Questions_in_Open-domain_Conversational_Systems_with_Typed_Decoders\n",
      "Signal_and_Noise_Statistics_Oblivious_Orthogonal_Matching_Pursuit_\n",
      "Distributed_and_Provably_Good_Seedings_for_k-Means_in_Constant_Rounds\n",
      "Cross-Target_Stance_Classification_with_Self-Attention_Networks\n",
      "PACRR__A_Position-Aware_Neural_IR_Model_for_Relevance_Matching\n",
      "Globally_Coherent_Text_Generation_with_Neural_Checklist_Models\n",
      "Neural_Taylor_Approximations___Convergence_and_Exploration_in_Rectifier_Networks\n",
      "Pragmatically_Informative_Image_Captioning_with_Character-Level_Inference\n",
      "Sparse_and_Constrained_Attention_for_Neural_Machine_Translation\n",
      "Large-scale_Analysis_of_Counseling_Conversations__An_Application_of_Natural_Language_Processing_to_Mental_Health\n",
      "Target-Sensitive_Memory_Networks_for_Aspect_Sentiment_Classification\n",
      "Compact,_Efficient_and_Unlimited_Capacity__Language_Modeling_with_Compressed_Suffix_Trees\n",
      "Fast_and_Sample_Efficient_Inductive_Matrix_Completion_via_Multi-Phase_Procrustes_Flow\n",
      "Unifying_Task_Specification_in_Reinforcement_Learning\n",
      "A_Question_Answering_Approach_to_Emotion_Cause_Extraction\n",
      "Prediction_under_Uncertainty_in_Sparse_Spectrum_Gaussian_Processes__with_Applications_to_Filtering_and_Control\n",
      "Ranking_Distributions_based_on_Noisy_Sorting\n",
      "An_Unsupervised_Probability_Model_for_Speech-to-Translation_Alignment_of_Low-Resource_Languages\n",
      "ProtoNN__Compressed_and_Accurate_kNN_for_Resource-scarce_Devices\n",
      "Contextual_Decision_Processes_with_low_Bellman_rank_are_PAC-Learnable\n",
      "Feasible_Arm_Identification\n",
      "Schema_Networks__Zero-shot_Transfer_with_a_Generative_Causal_Model_of_Intuitive_Physics\n",
      "Representation_Learning_Using_Multi-Task_Deep_Neural_Networks_for_Semantic_Classification_and_Information_Retrieval\n",
      "Multi-lingual_neural_title_generation_for_e-Commerce_browse_pages\n",
      "Loss_Decomposition_for_Fast_Learning_in_Large_Output_Spaces\n",
      "Joint_prediction_in_MST-style_discourse_parsing_for_argumentation_mining\n",
      "Working_Memory_Networks__Augmenting_Memory_Networks_with_a_Relational_Reasoning_Module\n",
      "Uniform_Convergence_Rates_for_Kernel_Density_Estimation\n",
      "Recognizing_Implicit_Discourse_Relations_via_Repeated_Reading__Neural_Networks_with_Multi-Level_Attention\n",
      "Probabilistic_Recurrent_State-Space_Models\n",
      "Learning_multiview_embeddings_for_assessing_dementia\n",
      "Who_did_What__A_Large-Scale_Person-Centered_Cloze_Dataset\n",
      "Yes,_but_Did_It_Work__Evaluating_Variational_Inference\n",
      "Residual_Unfairness_in_Fair_Machine_Learning_from_Prejudiced_Data\n",
      "Knowledge_Graph_Embedding_with_Hierarchical_Relation_Structure\n",
      "Understanding_Task_Design_Trade-offs_in_Crowdsourced_Paraphrase_Collection\n",
      "Learning_Discrete_Representations_via_Information_Maximizing_Self-Augmented_Training\n",
      "Comparing_Computational_Cognitive_Models_of_Generalization_in_a_Language_Acquisition_Task\n",
      "Device_Placement_Optimization_with_Reinforcement_Learning\n",
      "Neural_End-to-End_Learning_for_Computational_Argumentation_Mining\n",
      "Understanding_Generalization_and_Optimization_Performance_of_Deep_CNNs\n",
      "Learning_Word_Representations_with_Cross-Sentence_Dependency_for_End-to-End_Co-reference_Resolution\n",
      "Few-Shot_and_Zero-Shot_Multi-Label_Learning_for_Structured_Label_Spaces\n",
      "Stochastic_PCA_with_`2_and_`1_Regularization\n",
      "CRAFTML,_an_Efficient_Clustering-based_Random__Forest_for_Extreme_Multi-label_Learning_\n",
      "Gradient_Coding__Avoiding_Stragglers_in_Distributed_Learning\n",
      "High_Dimensional_Bayesian_Optimization_with_Elastic_Gaussian_Process\n",
      "Out-of-sample_extension_of_graph_adjacency_spectral_embedding\n",
      "Human-Machine_Dialogue_as_a_Stochastic_Game\n",
      "Identifying_Domain_Independent_Update_Intents_in_Task_Based_Dialogs\n",
      "Near_Human-Level_Performance_in_Grammatical_Error_Correction_with_Hybrid_Machine_Translation\n",
      "Unsupervised_Bilingual_Lexicon_Induction_via_Latent_Variable_Models\n",
      "Reasoning_about_Pragmatics_with_Neural_Listeners_and_Speakers\n",
      "MentorNet__Learning_Data-Driven_Curriculum_for_Very_Deep_Neural_Networks_on_Corrupted_Labels\n",
      "Proportional_Allocation___Simple,_Distributed,_and_Diverse_Matching_with_High_Entropy_\n",
      "Privacy-preserving_Neural_Representations_of_Text\n",
      "A_Neural_Layered_Model_for_Nested_Named_Entity_Recognition\n",
      "Controlling_Output_Length_in_Neural_Encoder-Decoders\n",
      "Translating_Neuralese\n",
      "Identifying_Political_Sentiment_between_Nation_States_with_Social_Media\n",
      "Probably_Approximately_Metric-Fair_Learning\n",
      "Before_Name-calling__Dynamics_and_Triggers_of_Ad_Hominem_Fallacies_in_Web_Argumentation\n",
      "An_Infinite_Hidden_Markov_Model_With_Similarity-Biased_Transitions\n",
      "Tandem_Anchoring__a_Multiword_Anchor_Approach_for_Interactive_Topic_Modeling\n",
      "Document_Modeling_with_Gated_Recurrent_Neural_Network_for_Sentiment_Classification\n",
      "Deterministic_Non-Autoregressive_Neural_Sequence_Modeling_by_Iterative_Refinement\n",
      "Rates_of_Convergence_of_Spectral_Methods_for_Graphon_Estimation\n",
      "Counterfactual_Data-Fusion_for_Online_Reinforcement_Learners\n",
      "Learning_Maximum-A-Posteriori_Perturbation_Models_for_Structured_Prediction_in_Polynomial_Time\n",
      "Dependency-Based_Word_Embeddings\n",
      "Exploiting_Deep_Representations_for_Neural_Machine_Translation\n",
      "A_Multi-sentiment-resource_Enhanced_Attention_Network_for_Sentiment_Classification\n",
      "Deep_Tensor_Convolution_on_Multicores\n",
      "Unsupervised_Cross-lingual_Transfer_of_Word_Embedding_Spaces\n",
      "Structured_Output_Learning_with_Abstention__Application_to_Accurate_Opinion_Prediction\n",
      "Supervised_Attentions_for_Neural_Machine_Translation\n",
      "Addressee_and_Response_Selection_for_Multi-Party_Conversation\n",
      "Aspect-augmented_Adversarial_Networks_for_Domain_Adaptation\n",
      "Joint_Event_Trigger_Identification_and_Event_Coreference_Resolution_with_Structured_Perceptron\n",
      "Beyond_Filters__Compact_Feature_Map_for_Portable_Deep_Model\n",
      "Large_Margin_Neural_Language_Model\n",
      "Weakly_Consistent_Optimal_Pricing_Algorithms_in_Repeated_Posted-Price_Auctions_with_Strategic_Buyer\n",
      "Not_All_Dialogues_are_Created_Equal__Instance_Weighting_for_Neural_Conversational_Models\n",
      "Dictionary_Learning_Based_on_Sparse_Distribution_Tomography\n",
      "An_Adaptive_Test_of_Independence_with_Analytic_Kernel_Embeddings\n",
      "Exploring_Neural_Text_Simplification_Models\n",
      "Context-dependent_Semantic_Parsing_for_Time_Expressions\n",
      "Learning_to_Discover_Sparse_Graphical_Models\n",
      "Resource-efficient_Machine_Learning_in_2_KB_RAM_for_the_Internet_of_Things\n",
      "Joint_Learning_for_Event_Coreference_Resolution\n",
      "Long_Short-Term_Memory_as_a_Dynamically_Computed_Element-wise_Weighted_Sum\n",
      "Multimodal_Affective_Analysis_Using_Hierarchical_Attention_Strategy_with_Word-Level_Alignment\n",
      "Accurate_Supervised_and_Semi-Supervised_Machine_Reading_for_Long_Documents\n",
      "Parseval_Networks__Improving_Robustness_to_Adversarial_Examples\n",
      "On_the_Practical_Computational_Power_of_Finite_Precision_RNNs_for_Language_Recognition\n",
      "Stochastic_Proximal_Algorithms_for_AUC_Maximization\n",
      "Backpropagating_through_Structured_Argmax_using_a_SPIGOT\n",
      "Self-Paced_Co-training\n",
      "Pieces_of_Eight__8-bit_Neural_Machine_Translation\n",
      "Twitter_Universal_Dependency_Parsing_for_African-American_and_Mainstream_American_English\n",
      "Adversarially_Regularized_Autoencoders\n",
      "Improving_Topic_Models_with_Latent_Feature_Word_Representations\n",
      "Hearst_Patterns_Revisited__Automatic_Hypernym_Detection_from_Large_Text_Corpora\n",
      "A_Dataset_for_Telling_the_Stories_of_Social_Media_Videos\n",
      "Learning_Localized_Spatio-Temporal_Models_From_Streaming_Data\n",
      "Measuring_Sample_Quality_with_Kernels\n",
      "Strong_Baselines_for_Simple_Question_Answering_over_Knowledge_Graphs_with_and_without_Neural_Networks\n",
      "Unsupervised_Statistical_Machine_Translation\n",
      "Revealing_Common_Statistical_Behaviors_in_Heterogeneous_Populations\n",
      "Sequence_Tutor__Conservative_Fine-Tuning_of_Sequence_Generation_Models_with_KL-control\n",
      "The_Galactic_Dependencies_Treebanks__Getting_More_Data_by_Synthesizing_New_Languages\n",
      "Evaluating_Visual_Representations_for_Topic_Understanding_and_Their_Effects_on_Manually_Generated_Topic_Labels\n",
      "SafeCity__Understanding_Diverse_Forms_of_Sexual_Harassment_Personal_Stories\n",
      "What_Action_Causes_This__Towards_Naive_Physical_Action-Effect_Prediction\n",
      "Handling_Cold-Start_Problem_in_Review_Spam_Detection_by_Jointly_Embedding_Texts_and_Behaviors\n",
      "The_glass_ceiling_in_NLP\n",
      "Generating_Fine-Grained_Open_Vocabulary_Entity_Type_Descriptions\n",
      "On_the_Challenges_of_Translating_NLP_Research_into_Commercial_Products\n",
      "SimpleQuestions_Nearly_Solved__A_New_Upperbound_and_Baseline_Approach\n",
      "Neural_Episodic_Control\n",
      "Bucket_Renormalization_for_Approximate_Inference\n",
      "Neural_Network_based_Extreme_Classification_and_Similarity_Models_for_Product_Matching\n",
      "Risk_Bounds_for_Transferring_Representations_With_and_Without_Fine-Tuning\n",
      "Optimal_Rates_of_Sketched-regularized_Algorithms_for_Least-Squares_Regression_over_Hilbert_Spaces\n",
      "Coarse-to-Fine_Decoding_for_Neural_Semantic_Parsing\n",
      "Representations_of_language_in_a_model_of_visually_grounded_speech_signal\n",
      "EmoNet__Fine-Grained_Emotion_Detection_with_Gated_Recurrent_Neural_Networks\n",
      "Neural_Word_Segmentation_with_Rich_Pretraining\n",
      "Deep_Temporal-Recurrent-Replicated-Softmax_for_Topical_Trends_over_Time\n",
      "Universal_Neural_Machine_Translation_for_Extremely_Low_Resource_Languages\n",
      "Semi-Supervised_Classification_Based_on_Classification_from_Positive_and_Unlabeled_Data\n",
      "Convexified_Convolutional_Neural_Networks\n",
      "FewRel__A_Large-Scale_Supervised_Few-Shot_Relation_Classification_Dataset_with_State-of-the-Art_Evaluation\n",
      "Distributed_Mean_Estimation_with_Limited_Communication\n",
      "From_Language_to_Programs__Bridging_Reinforcement_Learning_and_Maximum_Marginal_Likelihood\n",
      "Re-revisiting_Learning_on_Hypergraphs___Confidence_Interval_and_Subgradient_Method\n",
      "TWOWINGOS__A_Two-Wing_Optimization_Strategy_for_Evidential_Claim_Verification\n",
      "Efficient_Methods_for_Incorporating_Knowledge_into_Topic_Models\n",
      "Understanding_Satirical_Articles_Using_Common-Sense\n",
      "Bandits_with_Delayed,_Aggregated_Anonymous_Feedback\n",
      "Fine-grained_Opinion_Mining_with_Recurrent_Neural_Networks_and_Word_Embeddings\n",
      "Exploiting_Domain_Knowledge_via_Grouped_Weight_Sharing_with_Application_to_Text_Categorization\n",
      "Compact_Personalized_Models_for_Neural_Machine_Translation\n",
      "Inter_and_Intra_Topic_Structure_Learning_with_Word_Embeddings\n",
      "How_to_Memorize_a_Random_60-Bit_String\n",
      "Learning_Sequence_Encoders_for_Temporal_Knowledge_Graph_Completion\n",
      "Fast(er)_Exact_Decoding_and_Global_Training_for_Transition-Based_Dependency_Parsing_via_a_Minimal_Feature_Set\n",
      "Learning_word-like_units_from_joint_audio-visual_analysis\n",
      "The_Loss_Surface_of_Deep_and_Wide_Neural_Networks\n",
      "Sparse_Non-negative_Matrix_Language_Modeling\n",
      "Simple_and_Effective_Multi-Paragraph_Reading_Comprehension\n",
      "Learning_to_Explain__An_Information-Theoretic_Perspective__on_Model_Interpretation\n",
      "Parsing_with_Traces__An_O(n)_Algorithm_and_a_Structural_Representation\n",
      "Measuring_abstract_reasoning_in_neural_networks\n",
      "Noising_and_Denoising_Natural_Language__Diverse_Backtranslation_for_Grammar_Correction\n",
      "TDNN__A_Two-stage_Deep_Neural_Network_for_Prompt-independent_Automated_Essay_Scoring\n",
      "Automatic_Estimation_of_Simultaneous_Interpreter_Performance\n",
      "Attention-based_Deep_Multiple_Instance_Learning\n",
      "Hierarchical_Structured_Model_for_Fine-to-coarse_Manifesto_Text_Analysis\n",
      "FOIL_it!_Find_One_mismatch_between_Image_and_Language_caption\n",
      "Creating_Training_Corpora_for_NLG_Micro-Planning\n",
      "Sequence-to-Dependency_Neural_Machine_Translation\n",
      "Dimensionality-Driven_Learning_with_Noisy_Labels\n",
      "Scalable_Bayesian_Rule_Lists\n",
      "Collect_at_Once,_Use_Effectively__Making_Non-interactive_Locally_Private_Learning_Possible\n",
      "Understanding_Language_Preference_for_Expression_of_Opinion_and_Sentiment__What_do_Hindi-English_Speakers_do_on_Twitter_\n",
      "End-to-end_Active_Object_Tracking_via_Reinforcement_Learning\n",
      "Extreme_Learning_to_Rank_via_Low_Rank_Assumption\n",
      "Augmented_CycleGAN__Learning_Many-to-Many_Mappings__from_Unpaired_Data\n",
      "Unifying_Text,_Metadata,_and_User_Network_Representations_with_a_Neural_Network_for_Geolocation_Prediction\n",
      "An_Analytical_Formula_of_Population_Gradient_for_two-layered_ReLU_network_and_its_Applications_in_Convergence_and_Critical_Point_Analysis\n",
      "Jointly_Predicting_Predicates_and_Arguments_in_Neural_Semantic_Role_Labeling\n",
      "Bayesian_Modeling_of_Lexical_Resources_for_Low-Resource_Settings\n",
      "Gradually_Updated_Neural_Networks_for_Large-Scale_Image_Recognition\n",
      "What_Makes_Reading_Comprehension_Questions_Easier_\n",
      "Joint_Learning_for_Targeted_Sentiment_Analysis\n",
      "Deep_Multi-Task_Learning_for_Aspect_Term_Extraction_with_Memory_Interaction\n",
      "Identifying_Best_Interventions_through_Online_Importance_Sampling\n",
      "Attention-over-Attention_Neural_Networks_for_Reading_Comprehension\n",
      "Modeling_Naive_Psychology_of_Characters_in_Simple_Commonsense_Stories\n",
      "Decomposition_of_Uncertainty_in_Bayesian_Deep_Learning_for_Efficient_and_Risk-sensitive_Learning\n",
      "Deep_Voice__Real-time_Neural_Text-to-Speech\n",
      "Language-Guided_Adaptive_Perception_for_Efficient_Grounded_Communication_with_Robotic_Manipulators_in_Cluttered_Environments\n",
      "Position-aware_Attention_and_Supervised_Data_Improve_Slot_Filling\n",
      "Variable_Typing__Assigning_Meaning_to_Variables_in_Mathematical_Text\n",
      "Two_Methods_for_Domain_Adaptation_of_Bilingual_Tasks__Delightfully_Simple_and_Broadly_Applicable\n",
      "Stochastic_Adaptive_Quasi-Newton_Methods_for_Minimizing_Expected_Values\n",
      "The_Price_of_Differential_Privacy_for_Online_Learning\n",
      "Differentially_Private_Database_Release_via_Kernel_Mean_Embeddings\n",
      "Autoencoder_as_Assistant_Supervisor__Improving_Text_Representation_for_Chinese_Social_Media_Text_Summarization\n",
      "Defoiling_Foiled_Image_Captions\n",
      "Generating_Topical_Poetry\n",
      "Stabilising_Experience_Replay_for_Deep_Multi-Agent_Reinforcement_Learning\n",
      "Dynamic_Word_Embeddings\n",
      "Neural_Program_Synthesis_from_Diverse_Demonstration_Videos\n",
      "Adversarial_Example_Generation_with_Syntactically_Controlled_Paraphrase_Networks\n",
      "A_Structured_Syntax-Semantics_Interface_for_English-AMR_Alignment\n",
      "Fast_Approximate_Spectral_Clustering_for_Dynamic_Networks\n",
      "Generalized_Robust_Bayesian_Committee_Machine_for_Large-scale_Gaussian_Process_Regression\n",
      "Distant_Supervision_from_Disparate_Sources_for_Low-Resource_Part-of-Speech_Tagging\n",
      "Leveraging_Well-Conditioned_Bases__Streaming_and_Distributed_Summaries_in_Minkowski_p-Norms\n",
      "Generating_Sentences_by_Editing_Prototypes\n",
      "Cross-Lingual_Transfer_Learning_for_POS_Tagging_without_Cross-Lingual_Resources\n",
      "Faster_Principal_Component_Regression__and_Stable_Matrix_Chebyshev_Approximation\n",
      "Exemplar_Encoder-Decoder_for_Neural_Conversation_Generation\n",
      "Improving_Regression_Performance_with_Distributional_Losses\n",
      "Effective_Approaches_to_Attention-based_Neural_Machine_Translation\n",
      "Strong_Baselines_for_Neural_Semi-Supervised_Learning_under_Domain_Shift\n",
      "Thompson_Sampling_for_Combinatorial_Semi-Bandits\n",
      "Deep_Value_Networks_Learn_to_Evaluate_and_Iteratively_Refine_Structured_Outputs\n",
      "Towards_Debate_Automation__a_Recurrent_Model_for_Predicting_Debate_Winners\n",
      "Asynchronous_Stochastic_Gradient_Descent_with_Delay_Compensation\n",
      "Learning_to_Generate_Long-term_Future_via_Hierarchical_Prediction\n",
      "Joint_Modeling_of_Topics,_Citations,_and_Topical_Authority_in_Academic_Corpora\n",
      "Predicate_Argument_Alignment_using_a_Global_Coherence_Model\n",
      "Antecedent_Selection_for_Sluicing__Structure_and_Content\n",
      "Multimodal_Language_Analysis_in_the_Wild__CMU-MOSEI_Dataset_and_Interpretable_Dynamic_Fusion_Graph\n",
      "Continuous-Time_Flows_for_Efficient_Inference_and_Density_Estimation\n",
      "Neural_Segmental_Hypergraphs_for_Overlapping_Mention_Recognition\n",
      "How_Much_Information_Does_a_Human_Translator_Add_to_the_Original_\n",
      "Semantic_Role_Labeling_for_Learner_Chinese__the_Importance_of_Syntactic_Parsing_and_L2-L1_Parallel_Data\n",
      "Improving_Abstraction_in_Text_Summarization\n",
      "Whodunnit__Crime_Drama_as_a_Case_for_Natural_Language_Understanding\n",
      "Algebraic_Variety_Models_for_High-Rank_Matrix_Completion\n",
      "Topological_Mixture_Estimation\n",
      "meProp__Sparsified_Back_Propagation_for_Accelerated_Deep_Learning_with_Reduced_Overfitting\n",
      "Estimating_the_unseen_from_multiple_populations\n",
      "Nonparanormal_Information_Estimation\n",
      "Key-Value_Memory_Networks_for_Directly_Reading_Documents\n",
      "Citation_Resolution__A_method_for_evaluating_context-based_citation_recommendation_systems\n",
      "Continual_Learning_Through_Synaptic_Intelligence\n",
      "Unsupervised_Neural_Machine_Translation_with_Weight_Sharing\n",
      "Learning_to_Coordinate_with_Coordination_Graphs_in_Repeated_Single-Stage_Multi-Agent_Decision_Problems\n",
      "Fast_Bayesian_Intensity_Estimation_for_the_Permanental_Process\n",
      "To_Understand_Deep_Learning_We_Need_to_Understand_Kernel_Learning\n",
      "Uniform_Deviation_Bounds_for_k-Means_Clustering\n",
      "Structured_Multi-Label_Biomedical_Text_Tagging_via_Attentive_Neural_Tree_Decoding\n",
      "Learning_Sentence_Embeddings_with_Auxiliary_Tasks_for_Cross-Domain_Sentiment_Classification\n",
      "Style_Transfer_Through_Back-Translation\n",
      "Deep_Reinforcement_Learning_for_Dialogue_Generation\n",
      "Unified_Pragmatic_Models_for_Generating_and_Following_Instructions\n",
      "Scalable_Generative_Models_for_Multi-label_Learning_with_Missing_Labels\n",
      "Gradient_descent_with_identity_initialization__efficiently_learns_positive_definite_linear_transformations__by_deep_residual_networks_\n",
      "Sequence_to_Better_Sequence__Continuous_Revision_of_Combinatorial_Structures\n",
      "Which_Melbourne__Augmenting_Geocoding_with_Maps\n",
      "A_Neural_Attention_Model_for_Sentence_Summarization\n",
      "A_Two-Stage_Parsing_Method_for_Text-Level_Discourse_Analysis\n",
      "Multi-task_Learning_of_Pairwise_Sequence_Classification_Tasks_Over_Disparate_Label_Spaces\n",
      "A_Neural_Architecture_for_Automated_ICD_Coding\n",
      "Lost_Relatives_of_the_Gumbel_Trick\n",
      "A_Closer_Look_at_Memorization_in_Deep_Networks\n",
      "Fast_Maximization_of_Non-Submodular,_Monotonic_Functions_on_the_Integer_Lattice\n",
      "A_Study_of_Reinforcement_Learning_for_Neural_Machine_Translation\n",
      "Learning_Joint_Semantic_Parsers_from_Disjoint_Data\n",
      "Tensor_Balancing_on_Statistical_Manifold\n",
      "Learning_to_Parse_and_Translate_Improves_Neural_Machine_Translation\n",
      "Improving_Multi-Modal_Representations_Using_Image_Dispersion__Why_Less_is_Sometimes_More\n",
      "Adversarial_Feature_Matching_for_Text_Generation\n",
      "Neural_Response_Generation_via_GAN_with_an_Approximate_Embedding_Layer\n",
      "Fairness_Without_Demographics_in_Repeated_Loss_Minimization\n",
      "Structured_Variational_Learning_of_Bayesian_Neural_Networks_with_Horseshoe_Priors\n",
      "Graph-Based_Seed_Set_Expansion_for_Relation_Extraction_Using_Random_Walk_Hitting_Times\n",
      "Accurate_Uncertainties_for_Deep_Learning_Using_Calibrated_Regression\n",
      "Do_we_need_bigram_alignment_models__On_the_effect_of_alignment_quality_on_transduction_accuracy_in_G2P\n",
      "Online_and_Linear-Time_Attention_by_Enforcing_Monotonic_Alignments\n",
      "A_Meaning-based_Statistical_English_Math_Word_Problem_Solver\n",
      "Covariate_Adjusted_Precision_Matrix_Estimation_via_Nonconvex_Optimization\n",
      "Optimization_Landscape_and_Expressivity_of_Deep_CNNs\n",
      "On_Approximation_Guarantees_for_Greedy_Low_Rank_Optimization\n",
      "Opinion_Recommendation_Using_A_Neural_Model\n",
      "Magnetic_Hamiltonian_Monte_Carlo\n",
      "An_AMR_Aligner_Tuned_by_Transition-based_Parser\n",
      "A_Visual_Attention_Grounding_Neural_Model_for_Multimodal_Machine_Translation\n",
      "Mimicking_Word_Embeddings_using_Subword_RNNs\n",
      "Insertion_Position_Selection_Model_for_Flexible_Non-Terminals_in_Dependency_Tree-to-Tree_Machine_Translation\n",
      "An_Analysis_of_Action_Recognition_Datasets_for_Language_and_Vision_Tasks\n",
      "Latent_LSTM_Allocation__Joint_Clustering_and_Non-Linear_Dynamic_Modeling_of_Sequential_Data\n",
      "Embedding_Multimodal_Relational_Data_for_Knowledge_Base_Completion\n",
      "Modelling_and_Optimizing_on_Syntactic_N-Grams_for_Statistical_Machine_Translation\n",
      "Scalable_Deletion-Robust_Submodular_Maximization__Data_Summarization_with_Privacy_and_Fairness_Constraints\n",
      "Overcoming_Language_Variation_in_Sentiment_Analysis_with_Social_Attention\n",
      "Adaptive_Sampling_Probabilities_for_Non-Smooth_Optimization\n",
      "Reference_Resolution_in_Situated_Dialogue_with_Learned_Semantics\n",
      "Topic_Memory_Networks_for_Short_Text_Classification\n",
      "Word_Embedding_and_WordNet_Based_Metaphor_Identification_and_Interpretation\n",
      "The_Impact_of_Modeling_Overall_Argumentation_with_Tree_Kernels\n",
      "Straight_to_the_Tree__Constituency_Parsing_with_Neural_Syntactic_Distance\n",
      "Document_Embedding_Enhanced_Event_Detection_with_Hierarchical_and_Supervised_Attention\n",
      "Local-to-Global_Bayesian_Network_Structure_Learning\n",
      "Sequence-to-sequence_Models_for_Cache_Transition_Systems\n",
      "Deciding_How_to_Decide___Dynamic_Routing_in_Artificial_Neural_Networks\n",
      "Learning_Determinantal_Point_Processes_with_Moments_and_Cycles\n",
      "Looking_for_structure_in_lexical_and_acoustic-prosodic_entrainment_behaviors\n",
      "Language_Modeling_with_Gated_Convolutional_Networks\n",
      "A_Simple_Regularization-based_Algorithm_for_Learning_Cross-Domain_Word_Embeddings\n",
      "Data-Efficient_Policy_Evaluation_Through_Behavior_Policy_Search\n",
      "Implicit_Regularization_in_Nonconvex_Statistical_Estimation__Gradient_Descent_Converges_Linearly_for_Phase_Retrieval_and_Matrix_Completion\n",
      "Optimal_and_Adaptive_Off-policy_Evaluation_in_Contextual_Bandits\n",
      "Domain_Attention_with_an_Ensemble_of_Experts\n",
      "Co-clustering_through_Optimal_Transport\n",
      "Differentially_Private_Identity_and_Equivalence_Testing_of_Discrete_Distributions\n",
      "DVAE++__Discrete_Variational_Autoencoders_with_Overlapping_Transformations\n",
      "Compiling_Combinatorial_Prediction_Games\n",
      "Autoregressive_Quantile_Networks_for_Generative_Modeling\n",
      "Multi-Relational_Question_Answering_from_Narratives__Machine_Reading_and_Reasoning_in_Simulated_Worlds\n",
      "Doubly_Greedy_Primal-Dual_Coordinate_Descent_for_Sparse_Empirical_Risk_Minimization\n",
      "Scientific_Article_Summarization_Using_Citation-Context_and_Article_s_Discourse_Structure\n",
      "ConStance__Modeling_Annotation_Contexts_to_Improve_Stance_Classification\n",
      "A_Scalable_Neural_Shortlisting-Reranking_Approach_for_Large-Scale_Domain_Classification_in_Natural_Language_Understanding\n",
      "DialSQL__Dialogue_Based_Structured_Query_Generation\n",
      "Training_Structured_Prediction_Energy_Networks_with_Indirect_Supervision\n",
      "Harvesting_Paragraph-Level_Question-Answer_Pairs_from_Wikipedia\n",
      "Revisiting_the_Importance_of_Encoding_Logic_Rules_in_Sentiment_Classification\n",
      "Large-scale_Semantic_Parsing_without_Question-Answer_Pairs\n",
      "Semi-Implicit_Variational_Inference\n",
      "Tight_Regret_Bounds_for_Bayesian_Optimization_in_One_Dimension\n",
      "Neural_Networks_Should_Be_Wide_Enough_to_Learn_Disconnected_Decision_Regions\n",
      "Ranking_Paragraphs_for_Improving_Answer_Recall_in_Open-Domain_Question_Answering\n",
      "Using_pseudo-senses_for_improving_the_extraction_of_synonyms_from_word_embeddings\n",
      "Towards_End-to-End_Learning_for_Dialog_State_Tracking_and_Management_using_Deep_Reinforcement_Learning\n",
      "The_Lazy_Encoder__A_Fine-Grained_Analysis_of_the_Role_of_Morphology_in_Neural_Machine_Translation\n",
      "Generating_Syntactic_Paraphrases\n",
      "A_Skeleton-Based_Model_for_Promoting_Coherence_Among_Sentences_in_Narrative_Story_Generation\n",
      "Relations_such_as_Hypernymy__Identifying_and_Exploiting_Hearst_Patterns_in_Distributional_Vectors_for_Lexical_Entailment\n",
      "A_Spectral_Approach_to_Gradient_Estimation_for_Implicit_Distributions\n",
      "Improved_Variational_Autoencoders_for_Text_Modeling_using_Dilated_Convolutions\n",
      "Equivalence_of_Multicategory_SVM_and_Simplex_Cone_SVM___Fast_Computations_and_Statistical_Theory\n",
      "Multimodal_Frame_Identification_with_Multilingual_Evaluation\n",
      "Cognitive_Psychology_for_Deep_Neural_Networks___A_Shape_Bias_Case_Study_\n",
      "Hierarchical_Deep_Generative_Models_for_Multi-Rate_Multivariate_Time_Series\n",
      "Unsupervised_Learning_of_Morphological_Forests\n",
      "Variational_Sequential_Labelers_for_Semi-Supervised_Learning\n",
      "Large-Scale_QA-SRL_Parsing\n",
      "Bi-directional_Attention_with_Agreement_for_Dependency_Parsing\n",
      "Optimal_Data_Set_Selection__An_Application_to_Grapheme-to-Phoneme_Conversion\n",
      "Tropical_Geometry_of_Deep_Neural_Networks\n",
      "A_Corpus_of_Natural_Language_for_Visual_Reasoning\n",
      "Approximate_Steepest_Coordinate_Descent\n",
      "Learning_Latent_Space_Models_with_Angular_Constraints\n",
      "A_Tree-based_Decoder_for_Neural_Machine_Translation\n",
      "Learning_Unsupervised_Word_Translations_Without_Adversaries\n",
      "No_Metrics_Are_Perfect__Adversarial_Reward_Learning_for_Visual_Storytelling\n",
      "Toward_Zero-shot_Entity_Recognition_in_Task-oriented_Conversational_Agents\n",
      "Multimodal_Hierarchical_Reinforcement_Learning_Policy_for_Task-Oriented_Visual_Dialog\n",
      "Mixed_batches_and_symmetric_discriminators_for_GAN_training\n",
      "FeUdal_Networks_for_Hierarchical_Reinforcement_Learning\n",
      "A_Semismooth_Newton_Method_for_Fast,_Generic_Convex_Programming\n",
      "Learning_Infinite_Layer_Networks_Without_the_Kernel_Trick\n",
      "Active_Learning_with_Logged_Data\n",
      "Evaluating_Spoken_Dialogue_Processing_for_Time-Offset_Interaction\n",
      "Evaluating_Bayesian_Models_with_Posterior_Dispersion_Indices\n",
      "Heterogeneous_Supervision_for_Relation_Extraction__A_Representation_Learning_Approach\n",
      "Supervised_Domain_Enablement_Attention_for_Personalized_Domain_Classification\n",
      "The_SENSEI_Annotated_Corpus__Human_Summaries_of_Reader_Comment_Conversations_in_On-line_News\n",
      "Context-Dependent_Sentiment_Analysis_in_User-Generated_Videos\n",
      "Dynamical_Isometry_and_a_Mean_Field_Theory_of_RNNs__Gating_Enables_Signal_Propagation_in_Recurrent_Neural_Networks\n",
      "Vancouver_Welcomes_You!_Minimalist_Location_Metonymy_Resolution\n",
      "Grasping_the_Finer_Point__A_Supervised_Similarity_Network_for_Metaphor_Detection\n",
      "AllSummarizer_system_at_MultiLing_2015__Multilingual_single_and_multi-document_summarization\n",
      "Neural_AMR__Sequence-to-Sequence_Models_for_Parsing_and_Generation\n",
      "Multi-Class_Optimal_Margin_Distribution_Machine\n",
      "Judicious_Selection_of_Training_Data_in_Assisting_Language_for_Multilingual_Neural_NER\n",
      "Efficient_end-to-end_learning_for_quantizable_representations\n",
      "Answering_Elementary_Science_Questions_by_Constructing_Coherent_Scenes_using_Background_Knowledge\n",
      "Learning_to_Optimize_Combinatorial_Functions\n",
      "Towards_String-to-Tree_Neural_Machine_Translation\n",
      "Sparse_Coding_of_Neural_Word_Embeddings_for_Multilingual_Sequence_Labeling\n",
      "Automated_Curriculum_Learning_for_Neural_Networks\n",
      "Agent-Aware_Dropout_DQN_for_Safe_and_Efficient_On-line_Dialogue_Policy_Learning\n",
      "A_Study_of_Style_in_Machine_Translation__Controlling_the_Formality_of_Machine_Translation_Output\n",
      "Analogs_of_Linguistic_Structure_in_Deep_Representations\n",
      "Neural_Joint_Model_for_Transition-based_Chinese_Syntactic_Analysis\n",
      "On_the_Limitations_of_First-Order_Approximation_in_GAN_Dynamics\n",
      "A_Tabular_Method_for_Dynamic_Oracles_in_Transition-Based_Parsing\n",
      "NEWSROOM__A_Dataset_of_1_3_Million_Summaries_with_Diverse_Extractive_Strategies\n",
      "From_Patches_to_Images__A_Nonparametric_Generative_Model\n",
      "Nonparametric_Regression_with_Comparisons__Escaping_the_Curse_of_Dimensionality_with_Ordinal_Information\n",
      "Representation_Tradeoffs_for_Hyperbolic_Embeddings\n",
      "A_large_annotated_corpus_for_learning_natural_language_inference\n",
      "Affinity-Preserving_Random_Walk_for_Multi-Document_Summarization\n",
      "Encoding_Sentences_with_Graph_Convolutional_Networks_for_Semantic_Role_Labeling\n",
      "Learning_to_Generate_Compositional_Color_Descriptions\n",
      "SCDV___Sparse_Composite_Document_Vectors_using_soft_clustering_over_distributional_representations\n",
      "Learning_in_POMDPs_with_Monte_Carlo_Tree_Search\n",
      "Long_Short-Term_Memory_Neural_Networks_for_Chinese_Word_Segmentation\n",
      "Large-Scale_Multi-Domain_Belief_Tracking_with_Knowledge_Sharing\n",
      "StingyCD__Safely_Avoiding_Wasteful_Updates_in_Coordinate_Descent\n",
      "Deep_Density_Destructors\n",
      "A_Hybrid_Convolutional_Variational_Autoencoder_for_Text_Generation\n",
      "Uncovering_Causality_from_Multivariate_Hawkes_Integrated_Cumulants\n",
      "Sequence_Effects_in_Crowdsourced_Annotations\n",
      "On_the_Generalization_of_Equivariance_and_Convolution_in_Neural_Networks__to_the_Action_of_Compact_Groups\n",
      "Gaussian_Mixture_Latent_Vector_Grammars\n",
      "Can_Neural_Machine_Translation_be_Improved_with_User_Feedback_\n",
      "Learning_principled_bilingual_mappings_of_word_embeddings_while_preserving_monolingual_invariance\n",
      "Towards_End-to-End_Prosody_Transfer__for_Expressive_Speech_Synthesis_with_Tacotron\n",
      "Analyzing_the_Robustness_of_Nearest_Neighbors_to_Adversarial_Examples\n",
      "Spinning_Straw_into_Gold__Using_Free_Text_to_Train_Monolingual_Alignment_Models_for_Non-factoid_Question_Answering\n",
      "Distilling_Knowledge_for_Search-based_Structured_Prediction\n",
      "Multimodal_Language_Analysis_with_Recurrent_Multistage_Fusion\n",
      "Selective_Encoding_for_Abstractive_Sentence_Summarization\n",
      "Importance_sampling_for_unbiased_on-demand_evaluation_of_knowledge_base_population\n",
      "Memory-Based_Acquisition_of_Argument_Structures_and_its_Application_to_Implicit_Role_Detection\n",
      "Variational_Inference_for_Sparse_and_Undirected_Models\n",
      "Discrete-Continuous_Mixtures_in_Probabilistic_Programming__Generalized_Semantics_and_Inference_Algorithms\n",
      "Coordinated_Multi-Agent_Imitation_Learning\n",
      "Neural_Text_Generation_from_Structured_Data_with_Application_to_the_Biography_Domain\n",
      "Multilingual_Summarization_with_Polytope_Model\n",
      "Learning_Context-Aware_Convolutional_Filters_for_Text_Processing\n",
      "Document_Context_Neural_Machine_Translation_with_Memory_Networks\n",
      "Unsupervised_Discrete_Sentence_Representation_Learning_for_Interpretable_Neural_Dialog_Generation\n",
      "Geodesic_Convolutional_Shape_Optimization\n",
      "A_Full_Non-Monotonic_Transition_System_for_Unrestricted_Non-Projective_Parsing\n",
      "Robust_Cross-lingual_Hypernymy_Detection_using_Dependency_Context\n",
      "Playing_20_Question_Game_with_Policy-Based_Reinforcement_Learning\n",
      "Convolutional_Neural_Network_Language_Models\n",
      "Learning_Structural_Kernels_for_Natural_Language_Processing\n",
      "Meritocratic_Fairness_for_Cross-Population_Selection\n",
      "Automatically_Solving_Number_Word_Problems_by_Semantic_Parsing_and_Reasoning\n",
      "SciDTB__Discourse_Dependency_TreeBank_for_Scientific_Abstracts\n",
      "Transformation_Networks_for_Target-Oriented_Sentiment_Classification\n",
      "simNet__Stepwise_Image-Topic_Merging_Network_for_Generating_Detailed_and_Comprehensive_Image_Captions\n",
      "PreCo__A_Large-scale_Dataset_in_Preschool_Vocabulary_for_Coreference_Resolution\n",
      "Entity_Commonsense_Representation_for_Neural_Abstractive_Summarization\n",
      "A_Corpus_of_Sentence-level_Revisions_in_Academic_Writing__A_Step_towards_Understanding_Statement_Strength_in_Communication\n",
      "Cross-lingual_Opinion_Analysis_via_Negative_Transfer_Detection\n",
      "Binary_Partitions_with_Approximate_Minimum_Impurity\n",
      "Zeroshot_Multimodal_Named_Entity_Disambiguation_for_Noisy_Social_Media_Posts\n",
      "Alternative_Objective_Functions_for_Training_MT_Evaluation_Metrics\n",
      "Enhancing_Drug-Drug_Interaction_Extraction_from_Texts_by_Molecular_Structure_Information\n",
      "Neural_Optimizer_Search_with_Reinforcement_Learning\n",
      "Personalized_Machine_Translation__Predicting_Translational_Preferences\n",
      "Solving_Partial_Assignment_Problems_using_Random_Clique_Complexes\n",
      "Improving_Semantic_Parsing_with_Enriched_Synchronous_Context-Free_Grammar\n",
      "On_Learning_Sparsely_Used_Dictionaries_from_Incomplete_Samples\n",
      "Naturalizing_a_Programming_Language_via_Interactive_Learning\n",
      "How_Much_Reading_Does_Reading_Comprehension_Require__A_Critical_Investigation_of_Popular_Benchmarks\n",
      "Disfluency_Detection_Using_Multi-step_Stacked_Learning\n",
      "Towards_a_Discourse_Relation-aware_Approach_for_Chinese-English_Machine_Translation\n",
      "Attention-based_LSTM_Network_for_Cross-Lingual_Sentiment_Classification\n",
      "Name_List_Only__Target_Entity_Disambiguation_in_Short_Texts\n",
      "Spherical_Structured_Feature_Maps_for_Kernel_Approximation\n",
      "Batched_High-dimensional_Bayesian_Optimization_via_Structural_Kernel_Learning\n",
      "Approaching_Neural_Grammatical_Error_Correction_as_a_Low-Resource_Machine_Translation_Task\n",
      "Multi-Reference_Training_with_Pseudo-References_for_Neural_Translation_and_Text_Generation\n",
      "Dual_Supervised_Learning\n",
      "Personalizing_Dialogue_Agents__I_have_a_dog,_do_you_have_pets_too_\n",
      "Globally_Induced_Forest__A_Prepruning_Compression_Scheme\n",
      "Adversarial_Examples_for_Evaluating_Reading_Comprehension_Systems\n",
      "Kernel_Recursive_ABC__Point_Estimation_with_Intractable_Likelihood\n",
      "High-dimensional_Non-Gaussian_Single_Index_Models_via_Thresholded_Score_Function_Estimation\n",
      "Efficient_Contextualized_Representation__Language_Model_Pruning_for_Sequence_Labeling\n",
      "HOTPOTQA__A_Dataset_for_Diverse,_Explainable_Multi-hop_Question_Answering\n",
      "Abstractive_Document_Summarization_with_a_Graph-Based_Attentional_Neural_Model\n",
      "Confident_Multiple_Choice_Learning\n",
      "Transition-Based_Dependency_Parsing_with_Heuristic_Backtracking\n",
      "Semi-Supervised_Sequence_Modeling_with_Cross-View_Training\n",
      "Towards_Black-box_Iterative_Machine_Teaching\n",
      "Automatic_Measures_to_Characterise_Verbal_Alignment_in_Human-Agent_Interaction\n",
      "Colors_in_Context__A_Pragmatic_Neural_Model_for_Grounded_Language_Understanding\n",
      "Learning_Deep_Architectures_via_Generalized_Whitened_Neural_Networks\n",
      "Stochastic_Variance_Reduction_Methods_for_Policy_Evaluation\n",
      "Fine-grained_Coordinated_Cross-lingual_Text_Stream_Alignment_for_Endless_Language_Knowledge_Acquisition\n",
      "Towards_End-to-End_Reinforcement_Learning_of_Dialogue_Agents_for_Information_Access\n",
      "Dropout_Inference_in_Bayesian_Neural_Networks_with_Alpha-divergences\n",
      "On_the_Limitations_of_Unsupervised_Bilingual_Dictionary_Induction\n",
      "A_Nil-Aware_Answer_Extraction_Framework_for_Question_Answering\n",
      "Asynchronous_Stochastic_Quasi-Newton_MCMC_for_Non-Convex_Optimization\n",
      "Conversations_Gone_Awry__Detecting_Early_Signs_of_Conversational_Failure\n",
      "Bayesian_Models_of_Data_Streams_with_Hierarchical_Power_Priors\n",
      "Robust_Submodular_Maximization___A_Non-Uniform_Partitioning_Approach\n",
      "Token-level_and_sequence-level_loss_smoothing_for_RNN_language_models\n",
      "Mining_Inference_Formulas_by_Goal-Directed_Random_Walks\n",
      "End-to-End_Learning_for_the_Deep_Multivariate_Probit_Model_\n",
      "Modeling_Empathy_and_Distress_in_Reaction_to_News_Stories\n",
      "Syntax-based_Rewriting_for_Simultaneous_Machine_Translation\n",
      "Semantics_as_a_Foreign_Language\n",
      "Batch_Bayesian_Optimization_via_Multi-objective_Acquisition_Ensemble_for_Automated_Analog_Circuit_Design\n",
      "Semi-Supervised_Learning_on_Data_Streams_via_Temporal_Label_Propagation\n",
      "Verbal_and_Nonverbal_Clues_for_Real-life_Deception_Detection\n",
      "Variational_Bayesian_dropout__pitfalls_and_fixes\n",
      "Semantic_Annotation_for_Microblog_Topics_Using_Wikipedia_Temporal_Information\n",
      "Competitive_Caching_with_Machine_Learned_Advice\n",
      "Error-repair_Dependency_Parsing_for_Ungrammatical_Texts\n",
      "Noise2Noise__Learning_Image_Restoration_without_Clean_Data\n",
      "To_Attend_or_not_to_Attend__A_Case_Study_on_Syntactic_Structures_for_Semantic_Relatedness\n",
      "Consistent_k-Clustering\n",
      "How_NOT_To_Evaluate_Your_Dialogue_System__An_Empirical_Study_of_Unsupervised_Evaluation_Metrics_for_Dialogue_Response_Generation\n",
      "A_Primal-Dual_Analysis_of_Global_Optimality_in_Nonconvex_Low-Rank__Matrix_Recovery\n",
      "Parsing_as_Language_Modeling\n",
      "Going_out_on_a_limb__Joint_Extraction_of_Entity_Mentions_and_Relations_without_Dependency_Trees\n",
      "The_University_of_Alicante_at_MultiLing_2015__approach,_results_and_further_insights\n",
      "Identifying_Semantic_Edit_Intentions_from_Revisions_in_Wikipedia\n",
      "Investigating_Capsule_Networks_with_Dynamic_Routing_for_Text_Classification\n",
      "Gradient_Descent_for_Sparse_Rank-One_Matrix_Completion_for_Crowd-Sourced_Aggregation_of_Sparsely_Interacting_Workers\n",
      "Convolutional_Sequence_to_Sequence_Learning\n",
      "Using_Personal_Traits_For_Brand_Preference_Prediction\n",
      "Neural_Symbolic_Machines__Learning_Semantic_Parsers_on_Freebase_with_Weak_Supervision\n",
      "Neural_Open_Information_Extraction\n",
      "Fitting_New_Speakers_Based_on_a_Short_Untranscribed_Sample\n",
      "A_robust_self-learning_method_for_fully_unsupervised_cross-lingual_mappings_of_word_embeddings\n",
      "Globally_Normalized_Reader\n",
      "Comprehensive_Supersense_Disambiguation_of_English_Prepositions_and_Possessives\n",
      "Efficient_and_Expressive_Knowledge_Base_Completion_Using_Subgraph_Feature_Extraction\n",
      "One-Shot_Segmentation_in_Clutter\n",
      "MISSION__Ultra_Large-Scale_Feature_Selection_using_Count-Sketches\n",
      "Semantic_Parsing_with_Semi-Supervised_Sequential_Autoencoders\n",
      "Predicting_Native_Language_from_Gaze\n",
      "Neural_Argument_Generation_Augmented_with_Externally_Retrieved_Evidence\n",
      "Learning_Word_Embeddings_for_Low-resource_Languages_by_PU_Learning\n",
      "Bayesian_Boolean_Matrix_Factorisation\n",
      "Attentive_listening_system_with_backchanneling,_response_generation_and_flexible_turn-taking\n",
      "TAPAS__Tricks_to_Accelerate_(encrypted)_Prediction_As_a_Service\n",
      "Fine-Grained_Discourse_Structures_in_Continuation_Semantics\n",
      "End-to-End_Differentiable_Adversarial_Imitation_Learning\n",
      "Neural_Machine_Translation_via_Binary_Code_Prediction\n",
      "Improving_Slot_Filling_in_Spoken_Language_Understanding_with_Joint_Pointer_and_Attention\n",
      "Regret_Minimization_for_Partially_Observable_Deep_Reinforcement_Learning\n",
      "Attention_Strategies_for_Multi-Source_Sequence-to-Sequence_Learning\n",
      "Automatic_Metric_Validation_for_Grammatical_Error_Correction\n",
      "On_The_Projection_Operator_to_A_Three-view_Cardinality_Constrained_Set\n",
      "Theoretical_Analysis_of_Sparse_Subspace_Clustering_with_Missing_Entries\n",
      "Semantically_Conditioned_LSTM-based_Natural_Language_Generation_for_Spoken_Dialogue_Systems\n",
      "Training_Millions_of_Personalized_Dialogue_Agents\n",
      "Statistical_Inference_for_Incomplete_Ranking_Data__The_Case_of_Rank-Dependent_Coarsening\n",
      "DARLA__Improving_Zero-Shot_Transfer_in_Reinforcement_Learning\n",
      "Neural_Relation_Extraction_with_Multi-lingual_Attention\n",
      "Probabilistic_Submodular_Maximization_in_Sub-Linear_Time\n",
      "An_Iterative,_Sketching-based_Framework_for_Ridge_Regression\n",
      "Get_To_The_Point__Summarization_with_Pointer-Generator_Networks\n",
      "Pushing_the_Limits_of_Translation_Quality_Estimation\n",
      "Interpretable_and_Compositional_Relation_Learning_by_Joint_Training_with_an_Autoencoder\n",
      "Tackling_the_Story_Ending_Biases_in_The_Story_Cloze_Test\n",
      "Bilingual_Structured_Language_Models_for_Statistical_Machine_Translation\n",
      "WIKIQA__A_Challenge_Dataset_for_Open-Domain_Question_Answering\n",
      "NetGAN__Generating_Graphs_via_Random_Walks\n",
      "Winning_on_the_Merits__The_Joint_Effects_of_Content_and_Style_on_Debate_Outcomes\n",
      "Knowledge_transfer_between_speakers_for_personalised_dialogue_management\n",
      "NEXUS_Network__Connecting_the_Preceding_and_the_Following_in_Dialogue_Generation\n",
      "What_makes_a_convincing_argument__Empirical_analysis_and_detecting_attributes_of_convincingness_in_Web_argumentation\n",
      "Sharp_Models_on_Dull_Hardware__Fast_and_Accurate_Neural_Machine_Translation_Decoding_on_the_CPU\n",
      "Adversarial_Transfer_Learning_for_Chinese_Named_Entity_Recognition_with_Self-Attention_Mechanism\n",
      "Interpretable_Emoji_Prediction_via_Label-Wise_Attention_LSTMs\n",
      "Exploiting_the_Scope_of_Negations_and_Heterogeneous_Features_for_Relation_Extraction__A_Case_Study_for_Drug-Drug_Interaction_Extraction\n",
      "Efficient_Gradient-Free_Variational_Inference_using_Policy_Search\n",
      "Gromov-Wasserstein_Alignment_of_Word_Embedding_Spaces\n",
      "One_Vector_is_Not_Enough__Entity-Augmented_Distributed_Semantics_for_Discourse_Relations\n",
      "Active_Learning_for_Top-K_Rank_Aggregation_from_Noisy_Comparisons\n",
      "The_Importance_of_Being_Recurrent_for_Modeling_Hierarchical_Structure\n",
      "Gradient_Boosted_Decision_Trees_for_High_Dimensional_Sparse_Output\n",
      "Linguistic_Cues_to_Deception_and_Perceived_Deception_in_Interview_Dialogues\n",
      "Stochastic_Video_Generation_with_a_Learned_Prior\n",
      "Differentially_Private_Submodular_Maximization__Data_Summarization_in_Disguise\n",
      "Candidates_vs__Noises_Estimation_for_Large_Multi-Class_Classification_Problem\n",
      "Joint_Multitask_Learning_for_Community_Question_Answering_Using_Task-Specific_Embeddings\n",
      "Distributional_vectors_encode_referential_attributes\n",
      "Prox-PDA__The_Proximal_Primal-Dual_Algorithm_for_Fast_Distributed_Nonconvex_Optimization_and_Learning_Over_Networks\n",
      "Dynamical_Isometry_and_a_Mean_Field_Theory_of_CNNs__How_to_Train_10,000-Layer_Vanilla_Convolutional_Neural_Networks\n",
      "On_the_Implicit_Bias_of_Dropout\n",
      "Enumerating_Distinct_Decision_Trees\n",
      "Sentence_Simplification_with_Deep_Reinforcement_Learning\n",
      "Automatic_Extraction_of_Implicit_Interpretations_from_Modal_Constructions\n",
      "Adaptive_HTER_Estimation_for_Document-Specific_MT_Post-Editing\n",
      "Consistency_Analysis_for_Binary_Classification_Revisited\n",
      "Nonparametric_Bayesian_Semi-supervised_Word_Segmentation\n",
      "Improved_Dependency_Parsing_using_Implicit_Word_Connections_Learned_from_Unlabeled_Data\n",
      "Prediction_Rule_Reshaping\n",
      "Non-Projective_Dependency_Parsing_with_Non-Local_Transitions\n",
      "Exploiting_Strong_Convexity_from_Data_with_Primal-Dual_First-Order_Algorithms\n",
      "Conversational_Image_Editing__Incremental_Intent_Identification_in_a_New_Dialogue_Task\n",
      "Open_Category_Detection_with_PAC_Guarantees\n",
      "Multi-Task_Identification_of_Entities,_Relations,_and_Coreference_for_Scientific_Knowledge_Graph_Construction\n",
      "Deeply_AggreVaTeD__Differentiable_Imitation_Learning_for_Sequential_Prediction\n",
      "Oracle_Complexity_of_Second-Order_Methods_for_Finite-Sum_Problems\n",
      "Deep_contextualized_word_representations\n",
      "Local_Density_Estimation_in_High_Dimensions\n",
      "Problems_in_Current_Text_Simplification_Research__New_Data_Can_Help\n",
      "Safe_Element_Screening_for_Submodular_Function_Minimization\n",
      "A_Deep_Neural_Network_Sentence_Level_Classification_Method_with_Context_Information\n",
      "Higher-order_logical_inference_with_compositional_semantics\n",
      "DR-BiLSTM__Dependent_Reading_Bidirectional_LSTM_for_Natural_Language_Inference\n",
      "Stochastic_Bouncy_Particle_Sampler\n",
      "Has_Machine_Translation_Achieved_Human_Parity__A_Case_for_Document-level_Evaluation\n",
      "Towards_Evaluating_Narrative_Quality_In_Student_Writing\n",
      "Differentially_Private_Learning_of_Undirected_Graphical_Models_Using_Collective_Graphical_Models\n",
      "SimVerb-3500__A_Large-Scale_Evaluation_Set_of_Verb_Similarity\n",
      "Discourse_Complements_Lexical_Semantics_for_Non-factoid_Answer_Reranking\n",
      "Pathologies_of_Neural_Models_Make_Interpretations_Difficult\n",
      "Do_Multi-Sense_Embeddings_Improve_Natural_Language_Understanding_\n",
      "Recurrent_Polynomial_Network_for_Dialogue_State_Tracking_with_Mismatched_Semantic_Parsers\n",
      "A_Deep_Generative_Model_of_Vowel_Formant_Typology\n",
      "Modeling_Linguistic_and_Personality_Adaptation_for_Natural_Language_Generation\n",
      "K-means_clustering_using_random_matrix_sparsification\n",
      "An_Empirical_Comparison_Between_N-gram_and_Syntactic_Language_Models_for_Word_Ordering\n",
      "Differentially_Private_Chi-squared_Test_by_Unit_Circle_Mechanism\n",
      "A_Convolutional_Encoder_Model_for_Neural_Machine_Translation\n",
      "Semi-supervised_User_Geolocation_via_Graph_Convolutional_Networks\n",
      "Partial_Optimality_and_Fast_Lower_Bounds_for_Weighted_Correlation_Clustering\n",
      "Integrating_Transformer_and_Paraphrase_Rules_for_Sentence_Simplification\n",
      "Subword_Regularization__Improving_Neural_Network_Translation_Models_with_Multiple_Subword_Candidates\n",
      "Simpler_but_More_Accurate_Semantic_Dependency_Parsing\n",
      "Social_Media_Text_Classification_under_Negative_Covariate_Shift\n",
      "Identifying_Transferable_Information_Across_Domains_for_Cross-domain_Sentiment_Classification\n",
      "Are_BLEU_and_Meaning_Representation_in_Opposition_\n",
      "Reordering_Grammar_Induction\n",
      "Fake_News_Mitigation_via_Point_Process_Based_Intervention\n",
      "Sentence_Alignment_Methods_for_Improving_Text_Simplification_Systems\n",
      "Summarizing_Opinions__Aspect_Extraction_Meets_Sentiment_Prediction_and_They_Are_Both_Weakly_Supervised\n",
      "Analogical_Inference_for_Multi-relational_Embeddings\n",
      "Evaluation_methods_for_unsupervised_word_embeddings\n",
      "Large-scale_Cloze_Test_Dataset_Created_by_Teachers\n",
      "Specializing_Word_Embeddings_for_Similarity_or_Relatedness\n",
      "Valency-Augmented_Dependency_Parsing\n",
      "AMR_Dependency_Parsing_with_a_Typed_Semantic_Algebra\n",
      "Domain_Adaptation_with_Adversarial_Training_and_Graph_Embeddings\n",
      "The_strange_geometry_of_skip-gram_with_negative_sampling\n",
      "The_Influence_of_Context_on_Sentence_Acceptability_Judgements\n",
      "The_Generalization_Error_of_Dictionary_Learning_with_Moreau_Envelopes\n",
      "Binary_Classification_with_Karmic,_Threshold-Quasi-Concave_Metrics\n",
      "Learning_Cross-lingual_Distributed_Logical_Representations_for_Semantic_Parsing\n",
      "Sketched_Ridge_Regression__Optimization_Perspective,__Statistical_Perspective,_and_Model_Averaging\n",
      "Lexical_Event_Ordering_with_an_Edge-Factored_Model\n",
      "Improving_Neural_Parsing_by_Disentangling_Model_Combination_and_Reranking_Effects\n",
      "Learning_Hawkes_Processes_from_Short_Doubly-Censored_Event_Sequences\n",
      "Temporally_Grounding_Natural_Sentence_in_Video\n",
      "A_Joint_Graph_Model_for_Pinyin-to-Chinese_Conversion_with_Typo_Correction\n",
      "Toward_incremental_dialogue_act_segmentation_in_fast-paced_interactive_dialogue_systems\n",
      "Firing_Bandits__Optimizing_Crowdfunding\n",
      "Using_Left-corner_Parsing_to_Encode_Universal_Structural_Constraints_in_Grammar_Induction\n",
      "Multi-fidelity_Bayesian_Optimisation_with_Continuous_Approximations\n",
      "Demographic_Dialectal_Variation_in_Social_Media__A_Case_Study_of_African-American_English\n",
      "Improving_Gibbs_Sampler_Scan_Quality_with_DoGS\n",
      "Jointly_Multiple_Events_Extraction_via_Attention-based_Graph_Information_Aggregation\n",
      "Generalizing_and_Hybridizing_Count-based_and_Neural_Language_Models\n",
      "Regret_Minimization_in_Behaviorally-Constrained_Zero-Sum_Games\n",
      "Triangular_Architecture_for_Rare_Language_Translation\n",
      "Compressed_Sensing_using_Generative_Models\n",
      "Lifelong_Learning_CRF_for_Supervised_Aspect_Extraction\n",
      "Image_Pivoting_for_Learning_Multilingual_Multimodal_Representations\n",
      "Simplifying_Neural_Machine_Translation_with_Addition-Subtraction_Twin-Gated_Recurrent_Networks\n",
      "Syntactic_Scaffolds_for_Semantic_Structures\n",
      "Speeding_Up_Neural_Machine_Translation_Decoding_by_Cube_Pruning\n",
      "Learning_Dynamics_of_Linear_Denoising_Autoencoders\n",
      "Shift-Reduce_Constituency_Parsing_with_Dynamic_Programming_and_POS_Tag_Lattice\n",
      "Globally_Optimal_Gradient_Descent_for_a_ConvNet_with_Gaussian_Inputs\n",
      "Noisin__Unbiased_Regularization_for_Recurrent_Neural_Networks\n",
      "To_Link_or_Not_to_Link__A_Study_on_End-to-End_Tweet_Entity_Linking\n",
      "Bootstrapping_into_Filler-Gap__An_Acquisition_Story\n",
      "Towards_Less_Generic_Responses_in_Neural_Conversation_Models__A_Statistical_Re-weighting_Method\n",
      "Minimizing_Trust_Leaks_for_Robust_Sybil_Detection\n",
      "Towards_Full_Text_Shallow_Discourse_Relation_Annotation__Experiments_with_Cross-Paragraph_Implicit_Relations_in_the_PDTB\n",
      "A_Mention-Ranking_Model_for_Abstract_Anaphora_Resolution\n",
      "Improving_Lexical_Choice_in_Neural_Machine_Translation\n",
      "Personalized_neural_language_models_for_real-world_query_auto_completion\n",
      "Evaluation_of_Word_Vector_Representations_by_Subspace_Alignment\n",
      "Neural_Latent_Relational_Analysis_to_Capture_Lexical_Semantic_Relations_in_a_Vector_Space\n",
      "Estimating_individual_treatment_effect__generalization_bounds_and_algorithms\n",
      "Distributed_Clustering_via_LSH_Based_Data_Partitioning\n",
      "Curriculum_Learning_by_Transfer_Learning__Theory_and_Experiments_with_Deep_Networks\n",
      "Parallel_and_Streaming_Algorithms_for_K-Core_Decomposition\n",
      "Unsupervised_Timeline_Generation_for_Wikipedia_History_Articles\n",
      "How_to_Escape_Saddle_Points_Efficiently\n",
      "Using_Linguistic_Features_to_Improve_the_Generalization_Capability_of_Neural_Coreference_Resolvers\n",
      "Bayesian_inference_on_random_simple_graphs_with_power_law_degree_distributions\n",
      "Building_a_shared_world__mapping_distributional_to_model-theoretic_semantic_spaces\n",
      "Subgoal_Discovery_for_Hierarchical_Dialogue_Policy_Learning\n",
      "Examining_Temporality_in_Document_Classification\n",
      "Neural_Net_Models_of_Open-domain_Discourse_Coherence\n",
      "Explicit_Inductive_Bias_for_Transfer_Learning_with_Convolutional_Networks\n",
      "Enriching_Word_Vectors_with_Subword_Information\n",
      "Personalized_Language_Model_for_Query_Auto-Completion\n",
      "What_we_need_to_learn_if_we_want_to_do_and_not_just_talk\n",
      "Using_Reinforcement_Learning_to_Model_Incrementality_in_a_Fast-Paced_Dialogue_Game\n",
      "VERB_PHYSICS__Relative_Physical_Knowledge_of_Actions_and_Objects\n",
      "Semantic_Parsing_for_Task_Oriented_Dialog_using_Hierarchical_Representations\n",
      "What_Your_Username_Says_About_You\n",
      "Learning_Generic_Sentence_Representations_Using_Convolutional_Neural_Networks\n",
      "Capturing_Regional_Variation_with_Distributed_Place_Representations_and_Geographic_Retrofitting\n",
      "Differentially_Private_Matrix_Completion_Revisited\n",
      "Structured_Variationally_Auto-encoded_Optimization\n",
      "Alignment-Based_Compositional_Semantics_for_Instruction_Following\n",
      "Fast_k-Nearest_Neighbour_Search_via_Prioritized_DCI\n",
      "Robust_Budget_Allocation_via_Continuous_Submodular_Functions\n",
      "Obtaining_Reliable_Human_Ratings_of_Valence,_Arousal,_and_Dominance_for_20,000_English_Words\n",
      "All_Fingers_are_not_Equal__Intensity_of_References_in_Scientific_Articles\n",
      "Identification_and_Model_Testing_in_Linear_Structural_Equation_Models_using_Auxiliary_Variables\n",
      "A_Simple_Multi-Class_Boosting_Framework__with_Theoretical_Guarantees_and_Empirical_Proficiency\n",
      "Leveraging_Union_of_Subspace_Structure_to_Improve_Constrained_Clustering\n",
      "Sub-sampled_Cubic_Regularization_for_Non-convex_Optimization\n",
      "Deriving_Machine_Attention_from_Human_Rationales\n",
      "Classifying_Temporal_Relations_by_Bidirectional_LSTM_over_Dependency_Paths\n",
      "Filling_the_Blanks_(hint__plural_noun)_for_Mad_Libs_Humor\n",
      "Unsupervised_Identification_of_Translationese\n",
      "Socially-Informed_Timeline_Generation_for_Complex_Events\n",
      "Composite_Marginal_Likelihood_Methods_for_Random_Utility_Models\n",
      "Creating_and_Characterizing_a_Diverse_Corpus_of_Sarcasm_in_Dialogue\n",
      "Lessons_from_the_Bible_on_Modern_Topics__Low-Resource_Multilingual_Topic_Model_Evaluation\n",
      "Source-Target_Similarity_Modelings_for_Multi-Source_Transfer_Gaussian_Process_Regression\n",
      "Apples_to_Apples__Learning_Semantics_of_Common_Entities_Through_a_Novel_Comprehension_Task\n",
      "Neural_Fine-Grained_Entity_Type_Classification_with_Hierarchy-Aware_Loss\n",
      "Human_Needs_Categorization_of_Affective_Events_Using_Labeled_and_Unlabeled_Data\n",
      "Decoding_Anagrammed_Texts_Written_in_an_Unknown_Language_and_Script\n",
      "Context-Aware_Neural_Machine_Translation_Learns_Anaphora_Resolution\n",
      "Streaming_Principal_Component_Analysis_in_Noisy_Settings\n",
      "A_Cross-language_Study_on_Automatic_Speech_Disfluency_Detection\n",
      "Neural_Machine_Translation_with_Source_Dependency_Representation\n",
      "Call_Centre_Conversation_Summarization__A_Pilot_Task_at_Multiling_2015\n",
      "NASH__Toward_End-to-End_Neural_Architecture_for_Generative_Semantic_Hashing\n",
      "Scalable_Approximate_Bayesian_Inference_for_Particle_Tracking_Data\n",
      "A_Multi-lingual_Multi-task_Architecture_for_Low-resource_Sequence_Labeling\n",
      "Geometry_of_Neural_Network_Loss_Surfaces_via_Random_Matrix_Theory\n",
      "Measuring_Thematic_Fit_with_Distributional_Feature_Overlap\n",
      "Creating_a_Large_Benchmark_for_Open_Information_Extraction\n",
      "Tunable_Efficient_Unitary_Neural_Networks_(EUNN)_and_their_application_to_RNNs\n",
      "Syntax_for_Semantic_Role_Labeling,_To_Be,_Or_Not_To_Be\n",
      "Neural_Networks_and_Rational_Functions\n",
      "Deep_IV__A_Flexible_Approach_for_Counterfactual_Prediction\n",
      "Polish_evaluation_dataset_for_compositional_distributional_semantics_models\n",
      "Neural_Audio_Synthesis_of_Musical_Notes__with_WaveNet_Autoencoders\n",
      "Gram-CTC__Automatic_Unit_Selection_and_Target_Decomposition_for_Sequence_Labelling\n",
      "Key-Value_Retrieval_Networks_for_Task-Oriented_Dialogue\n",
      "Story_Comprehension_for_Predicting_What_Happens_Next\n",
      "Neural_Hidden_Markov_Model_for_Machine_Translation\n",
      "Auto-Sizing_Neural_Networks__With_Applications_to_n-gram_Language_Models\n",
      "Biography-Dependent_Collaborative_Entity_Archiving_for_Slot_Filling\n",
      "Lifted_Rule_Injection_for_Relation_Embeddings\n",
      "Crowdsourcing_with_Arbitrary_Adversaries\n",
      "A_Neural_Model_of_Adaptation_in_Reading\n",
      "Multi-Metric_Optimization_Using_Ensemble_Tuning\n",
      "The_Role_of_Prosody_and_Speech_Register_in_Word_Segmentation__A_Computational_Modelling_Perspective\n",
      "Composable_Planning_with_Attributes\n",
      "Learning_bilingual_word_embeddings_with_(almost)_no_bilingual_data\n",
      "Style_Tokens__Unsupervised_Style_Modeling,_Control_and_Transfer_in_End-to-End_Speech_Synthesis\n",
      "Disambiguating_False-Alarm_Hashtag_Usages_in_Tweets_for_Irony_Detection\n",
      "Building_compositional_semantics_and_higher-order_inference_system_for_a_wide-coverage_Japanese_CCG_parser\n",
      "Diversity_driven_attention_model_for_query-based_abstractive_summarization\n",
      "Quasi-Second-Order_Parsing_for_1-Endpoint-Crossing,_Pagenumber-2_Graphs\n",
      "Towards_Universal_Dialogue_State_Tracking\n",
      "Semantic_Word_Clusters_Using_Signed_Spectral_Clustering\n",
      "Identifying_civilians_killed_by_police_with_distantly_supervised_entity-event_extraction\n",
      "A_Just-In-Time_Keyword_Extraction_from_Meeting_Transcripts\n",
      "Automatic_Extraction_of_Causal_Relations_from_Text_using_Linguistically_Informed_Deep_Neural_Networks\n",
      "Behavior_Analysis_of_NLI_Models__Uncovering_the_Influence_of_Three_Factors_on_Robustness\n",
      "Knowledge_Completion_for_Generics_using_Guided_Tensor_Factorization\n",
      "Projection-Free_Online_Optimization_with_Stochastic_Gradient__From_Convexity_to_Submodularity\n",
      "Strongly-Typed_Agents_are_Guaranteed_to_Interact_Safely\n",
      "Large-Scale_Sparse_Inverse_Covariance_Estimation_via_Thresholding_and_Max-Det_Matrix_Completion\n",
      "Specialising_Word_Vectors_for_Lexical_Entailment\n",
      "Exact_Inference_for_Integer_Latent-Variable_Models\n",
      "Robust_Gaussian_Graphical_Model_Estimation_with_Arbitrary_Corruption\n",
      "Text_Alignment_for_Real-Time_Crowd_Captioning\n",
      "An_Incremental_Turn-Taking_Model_with_Active_System_Barge-in_for_Spoken_Dialog_Systems\n",
      "Error_Estimation_for_Randomized_Least-Squares_Algorithms_via_the_Bootstrap\n",
      "Deal_or_No_Deal__End-to-End_Learning_for_Negotiation_Dialogues\n",
      "Spectral_Analysis_of_Information_Density_in_Dialogue_Predicts_Collaborative_Task_Performance\n",
      "Neural_Machine_Translation_with_Word_Predictions\n",
      "Collective_Tweet_Wikification_based_on_Semi-supervised_Graph_Regularization\n",
      "Trainable_Calibration_Measures_For_Neural_Networks_From_Kernel_Mean_Embeddings\n",
      "Word_Ordering_Without_Syntax\n",
      "Unsupervised_Grammar_Induction_with_Depth-bounded_PCFG\n",
      "Improving_Unsupervised_Word-by-Word_Translation_with_Language_Model_and_Denoising_Autoencoder\n",
      "Junction_Tree_Variational_Autoencoder_for_Molecular_Graph_Generation\n",
      "Inducing_Temporal_Relations_from_Time_Anchor_Annotation\n",
      "A_Probabilistic_Theory_of_Supervised_Similarity_Learning_for_Pointwise_ROC_Curve_Optimization\n",
      "Multi-Fidelity_Black-Box_Optimization_with_Hierarchical_Partitions\n",
      "Constraining_the_Dynamics_of_Deep_Probabilistic_Models\n",
      "Phrase-level_Self-Attention_Networks_for_Universal_Sentence_Encoding\n",
      "Understanding_the_Loss_Surface_of_Neural_Networks_for_Binary_Classification\n",
      "ELDEN__Improved_Entity_Linking_Using_Densified_Knowledge_Graphs\n",
      "Analysis_and_Optimization_of_Graph_Decompositions_by_Lifted_Multicuts\n",
      "emrQA__A_Large_Corpus_for_Question_Answering_on_Electronic_Medical_Records\n",
      "Improving_Neural_Abstractive_Document_Summarization_with_Explicit_Information_Selection_Modeling\n",
      "A_Spline_Theory_of_Deep_Networks\n",
      "Local_Bayesian_Optimization_of_Motor_Skills\n",
      "Approximation-Aware_Dependency_Parsing_by_Belief_Propagation\n",
      "Cross-narrative_temporal_ordering_of_medical_events\n",
      "Hierarchical_Clustering_with_Structural_Constraints\n",
      "SARAH__A_Novel_Method_for_Machine_Learning_Problems__Using_Stochastic_Recursive_Gradient\n",
      "Convolutional_Imputation_of_Matrix_Networks\n",
      "Max-value_Entropy_Search_for_Efficient_Bayesian_Optimization\n",
      "A_Polynomial-Time_Dynamic_Programming_Algorithm_for_Phrase-Based_Decoding_with_a_Fixed_Distortion_Limit\n",
      "Deep_Variational_Reinforcement_Learning_for_POMDPs\n",
      "A_Decomposable_Attention_Model_for_Natural_Language_Inference\n",
      "Aggregating_and_Predicting_Sequence_Labels_from_Crowd_Annotations\n",
      "Dependency-based_empty_category_detection_via_phrase_structure_trees\n",
      "Open_Domain_Question_Answering_Using_Early_Fusion_of_Knowledge_Bases_and_Text\n",
      "Semi-Supervised_Discriminative_Language_Modeling_with_Out-of-Domain_Text_Data\n",
      "An_Unsupervised_Neural_Attention_Model_for_Aspect_Extraction\n",
      "Adaptive_Scaling_for_Sparse_Detection_in_Information_Extraction\n",
      "Language_Understanding_for_Text-based_Games_using_Deep_Reinforcement_Learning\n",
      "The_Well-Tempered_Lasso\n",
      "Task-Oriented_Query_Reformulation_with_Reinforcement_Learning\n",
      "Supervised_Clustering_of_Questions_into_Intents_for_Dialog_System_Applications\n",
      "Zonotope_Hit-and-run_for_Efficient_Sampling_from_Projection_DPPs\n",
      "WikiConv__A_Corpus_of_the_Complete_Conversational_History_of_a_Large_Online_Collaborative_Community\n",
      "Markov_Modulated_Gaussian_Cox_Processes_forSemi-Stationary_Intensity_Modeling_of_Events_Data\n",
      "Learning_Scalar_Adjective_Intensity_from_Paraphrases\n",
      "High-Dimensional_Structured_Quantile_Regression\n",
      "Sparse_Coding_of_Neural_Word_Embeddings_for_Multilingual_Sequence_Labeling(1)\n",
      "Nonparametric_Bayesian_Models_for_Spoken_Language_Understanding\n",
      "Recursive_Partitioning_for_Personalization_using_Observational_Data\n",
      "Characterizing_Interactions_and_Relationships_between_People\n",
      "Distant_Supervision_for_Relation_Extraction_via_Piecewise_Convolutional_Neural_Networks\n",
      "Discourse_Coherence__Concurrent_Explicit_and_Implicit_Relations\n",
      "Why_is_unsupervised_alignment_of_English_embeddings_from_different_algorithms_so_hard_\n",
      "Meta-Learning_by_Adjusting_Priors_Based_on_Extended_PAC-Bayes_Theory\n",
      "Predicting_accuracy_on_large_datasets_from_smaller_pilot_data\n",
      "Evaluating_Theory_of_Mind_in_Question_Answering(1)\n",
      "Neural_Networks_for_Open_Domain_Targeted_Sentiment\n",
      "Movie_Script_Summarization_as_Graph-based_Scene_Extraction\n",
      "Adaptive_Neural_Networks_for_Efficient_Inference\n",
      "Zero-shot_User_Intent_Detection_via_Capsule_Neural_Networks\n",
      "Bounding_and_Counting_Linear_Regions_of_Deep_Neural_Networks\n",
      "Improving_a_Neural_Semantic_Parser_by_Counterfactual_Learning_from_Human_Bandit_Feedback\n",
      "Alignment_at_Work__Using_Language_to_Distinguish_the_Internalization_and_Self-Regulation_Components_of_Cultural_Fit_in_Organizations\n",
      "Tied_Multitask_Learning_for_Neural_Speech_Translation\n",
      "Consistent_On-Line_Off-Policy_Evaluation\n",
      "Higher-order_Coreference_Resolution_with_Coarse-to-fine_Inference\n",
      "An_Encoder-Decoder_Approach_to_the_Paradigm_Cell_Filling_Problem\n",
      "Algorithmic_Stability_and_Hypothesis_Complexity\n",
      "Synthetic_Data_Made_to_Order__The_Case_of_Parsing\n",
      "Train-O-Matic__Large-Scale_Supervised_Word_Sense_Disambiguation_in_Multiple_Languages_without_Manual_Training_Data\n",
      "A_practical_and_linguistically-motivated_approach_to_compositional_distributional_semantics\n",
      "Learning_One_Convolutional_Layer_with_Overlapping_Patches\n",
      "Combined_Group_and_Exclusive_Sparsity_for_Deep_Neural_Networks\n",
      "Anchored_Correlation_Explanation__Topic_Modeling_with_Minimal_Domain_Knowledge\n",
      "Combining_Model-Based_and_Model-Free_Updates_for_Trajectory-Centric_Reinforcement_Learning\n",
      "DeClarE__Debunking_Fake_News_and_False_Claims_using_Evidence-Aware_Deep_Learning\n",
      "Strong_NP-Hardness_for_Sparse_Optimization_with_Concave_Penalty_Functions\n",
      "AutoPrognosis__Automated_Clinical_Prognostic_Modeling_via_Bayesian_Optimization_with_Structured_Kernel_Learning\n",
      "Learning_to_Align_the_Source_Code_to_the_Compiled_Object_Code\n",
      "Visual_Attention_Model_for_Name_Tagging_in_Multimodal_Social_Media\n",
      "Convolutional_Neural_Networks_with_Recurrent_Neural_Filters\n",
      "No_Need_to_Pay_Attention__Simple_Recurrent_Neural_Networks_Work!\n",
      "Learning_how_to_Active_Learn__A_Deep_Reinforcement_Learning_Approach\n",
      "Conditional_Generation_and_Snapshot_Learning_in_Neural_Dialogue_Systems\n",
      "Learning_to_Control_the_Specificity_in_Neural_Response_Generation\n",
      "Because_Syntax_Does_Matter__Improving_Predicate-Argument_Structures_Parsing_with_Syntactic_Features\n",
      "Improving_Character-based_Decoding_Using_Target-Side_Morphological_Information_for_Neural_Machine_Translation\n",
      "High-Dimensional_Variance-Reduced_Stochastic_Gradient_Expectation-Maximization_Algorithm\n",
      "Learning_Crosslingual_Word_Embeddings_without_Bilingual_Corpora\n",
      "Efficient_Large-Scale_Neural_Domain_Classification_with_Personalized_Attention\n",
      "MOJITALK__Generating_Emotional_Responses_at_Scale\n",
      "Locally_Private_Hypothesis_Testing\n",
      "Scaling_Up_Sparse_Support_Vector_Machinesby_Simultaneous_Feature_and_Sample_Reduction\n",
      "Differentiable_Compositional_Kernel_Learning_for_Gaussian_Processes\n",
      "Multiple_Instance_Learning_Networks_for_Fine-Grained_Sentiment_Analysis\n",
      "Linear_Spectral_Estimators_and_an_Application_to_Phase_Retrieval\n",
      "SAFFRON__an_Adaptive_Algorithm_for_Online_Control_of_the_False_Discovery_Rate\n",
      "A_Dataset_and_Evaluation_Metrics_for_Abstractive_Compression_of_Sentences_and_Short_Paragraphs\n",
      "Scoring_Lexical_Entailment_with_a_Supervised_Directional_Similarity_Network\n",
      "Multi-Task_Video_Captioning_with_Video_and_Entailment_Generation\n",
      "Learning_Structured_Text_Representations\n",
      "Blind_Justice__Fairness_with_Encrypted_Sensitive_Attributes\n",
      "Uncertainty_Assessment_and_False_Discovery_Rate_Control_in_High-Dimensional_Granger_Causal_Inference\n",
      "Approximation_Guarantees_for_Adaptive_Sampling\n",
      "A_data-driven_model_of_explanations_for_a_chatbot_that_helps_to_practice_conversation_in_a_foreign_language\n",
      "Document_Modeling_with_External_Attention_for_Sentence_Extraction\n",
      "Replicability_Analysis_for_Natural_Language_Processing__Testing_Significance_with_Multiple_Datasets\n",
      "Recurrent_Neural_Networks_as_Weighted_Language_Recognizers\n",
      "Automatic_Annotation_and_Evaluation_of_Error_Types_for_Grammatical_Error_Correction\n",
      "Swag__A_Large-Scale_Adversarial_Dataset_for_Grounded_Commonsense_Inference\n",
      "C3EL__A_Joint_Model_for_Cross-Document_Co-Reference_Resolution_and_Entity_Linking\n",
      "Topic_Identification_and_Discovery_on_Text_and_Speech\n",
      "Variational_Boosting__Iteratively_Refining_Posterior_Approximations\n",
      "Approximate_Leave-One-Out_for_Fast_Parameter_Tuning_in_High_Dimensions\n",
      "WHInter__A_Working_set_algorithm_for_High-dimensional_sparse_second_order_Interaction_models\n",
      "Interactive_Learning_from_Policy-Dependent_Human_Feedback\n",
      "Abstract_Meaning_Representation_Parsing_using_LSTM_Recurrent_Neural_Networks\n",
      "What_you_can_cram_into_a_single_$_!#__vector__Probing_sentence_embeddings_for_linguistic_properties\n",
      "Sharp_Minima_Can_Generalize_For_Deep_Nets\n",
      "Approximation_Algorithms_for_Cascading_Prediction_Models\n",
      "Document-Level_Multi-Aspect_Sentiment_Classification_as_Machine_Comprehension\n",
      "Robust_and_Scalable_Models_of_Microbiome_Dynamics\n",
      "End-to-end_Neural_Coreference_Resolution\n",
      "Supporting_Spoken_Assistant_Systems_with_a_Graphical_User_Interface_that_Signals_Incremental_Understanding_and_Prediction_State\n",
      "High-Order_Low-Rank_Tensors_for_Semantic_Role_Labeling\n",
      "Unsupervised_Learning_of_Distributional_Relation_Vectors\n",
      "Random_Fourier_Features_for_Kernel_Ridge_Regression__Approximation_Bounds_and_Statistical_Guarantees\n",
      "Repeat_before_Forgetting__Spaced_Repetition_for_Efficient_and_Effective_Training_of_Neural_Networks\n",
      "Iterative_Machine_Teaching\n",
      "End-to-End_Learning_for_Structured_Prediction_Energy_Networks\n",
      "Exemplar_Encoder-Decoder_for_Neural_Conversation_Generation(1)\n",
      "Decentralized_Submodular_Maximization__Bridging_Discrete_and_Continuous_Settings\n",
      "Rapid_Adaptation_of_Neural_Machine_Translation_to_New_Languages\n",
      "Logarithmic_Time_One-Against-Some\n",
      "Multi-Turn_Response_Selection_for_Chatbots_with_Deep_Attention_Matching_Network\n",
      "AD3__Attentive_Deep_Document_Dater\n",
      "Multi-Hop_Knowledge_Graph_Reasoning_with_Reward_Shaping\n",
      "OBJ2TEXT__Generating_Visually_Descriptive_Language_from_Object_Layouts\n",
      "The_Ubuntu_Dialogue_Corpus__A_Large_Dataset_for_Research_in_Unstructured_Multi-Turn_Dialogue_Systems\n",
      "Data-Dependent_Stability_of_Stochastic_Gradient_Descent\n",
      "Speaking,_Seeing,_Understanding__Correlating_semantic_models_with_conceptual_representation_in_the_brain\n",
      "Classification_from_Pairwise_Similarity_and_Unlabeled_Data\n",
      "QuantTree__Histograms_for_Change_Detection_in_Multivariate_Data_Streams\n",
      "Tensor_Decomposition_via_Simultaneous_Power_Iteration\n",
      "Why_Swear__Analyzing_and_Inferring_the_Intentions_of_Vulgar_Expressions\n",
      "Automatic_Poetry_Generation_with_Mutual_Reinforcement_Learning\n",
      "BANDITSUM__Extractive_Summarization_as_a_Contextual_Bandit\n",
      "Towards_Understanding_the_Geometry_of_Knowledge_Graph_Embeddings\n",
      "STRUCTVAE__Tree-structured_Latent_Variable_Models_for_Semi-supervised_Semantic_Parsing\n",
      "Lifelong-RL__Lifelong_Relaxation_Labeling_for_Separating_Entities_and_Aspects_in_Opinion_Targets\n",
      "Senti-LSSVM__Sentiment-Oriented_Multi-Relation_Extraction_with_Latent_Structural_SVM\n",
      "Scalable_Wide_and_Deep_Learning_for_Computer_Assisted_Coding\n",
      "Learning_and_Memorization\n",
      "Spectral_Learning_from_a_Single_Trajectory_under_Finite-State_Policies\n",
      "Learning_Deep_ResNet_Blocks_Sequentially_using_Boosting_Theory\n",
      "Drug_Extraction_from_the_Web__Summarizing_Drug_Experiences_with_Multi-Dimensional_Topic_Models\n",
      "On_the_Distribution_of_Lexical_Features_at_Multiple_Levels_of_Analysis\n",
      "Parallel_WaveNet__Fast_High-Fidelity_Speech_Synthesis\n",
      "Adapting_Word_Embeddings_to_New_Languages_with_Morphological_and_Phonological_Subword_Representations\n",
      "Bayesian_Quadrature_for_Multiple_Related_Integrals\n",
      "Supervised_Learning_of_Universal_Sentence_Representations_from_Natural_Language_Inference_Data\n",
      "Stochastic_DCA_for_the_Large-sum_of_Non-convex_Functions_Problem_and_its_Application_to_Group_Variable_Selection_in_Classification\n",
      "Topic-Based_Agreement_and_Disagreement_in_US_Electoral_Manifestos\n",
      "Split_and_Rephrase__Better_Evaluation_and_a_Stronger_Baseline\n",
      "Automatic_Reference-Based_Evaluation_of_Pronoun_Translation_Misses_the_Point\n",
      "Innovation_Pursuit__A_New_Approach_to_the_Subspace_Clustering_Problem\n",
      "Rule-Enhanced_Penalized_Regression_by_Column_Generation__using_Rectangular_Maximum_Agreement\n",
      "Retrieve,_Rerank_and_Rewrite__Soft_Template_Based_Neural_Summarization\n",
      "Deep_Predictive_Coding_Network_for_Object_Recognition\n",
      "Learning_to_Ask_Good_Questions__Ranking_Clarification_Questions_using_Neural_Expected_Value_of_Perfect_Information\n",
      "Fine-Grained_Prediction_of_Syntactic_Typology__Discovering_Latent_Structure_with_Supervised_Learning\n",
      "Variational_Knowledge_Graph_Reasoning\n",
      "A_Fast_and_Scalable_Joint_Estimator_for_Integrating_Additional_Knowledge_in_Learning_Multiple_Related_Sparse_Gaussian_Graphical_Models\n",
      "Coherent_Probabilistic_Forecasts_for_Hierarchical_Time_Series\n",
      "Uncorrelation_and_Evenness__a_New_Diversity-Promoting_Regularizer\n",
      "Differentially_Private_Ordinary_Least_Squares\n",
      "Decipherment_of_Substitution_Ciphers_with_Neural_Language_Models\n",
      "POLY__Mining_Relational_Paraphrases_from_Multilingual_Sentences\n",
      "A_Divergence_Bound_for_Hybrids_of_MCMC_and_Variational_Inference_and_an_Application_to_Langevin_Dynamics_and_SGVI\n",
      "The_NarrativeQA_Reading_Comprehension_Challenge\n",
      "Detecting_Institutional_Dialog_Acts_in_Police_Traffic_Stops\n",
      "Learning_Prototypical_Goal_Activities_for_Locations\n",
      "Fine-Grained_Citation_Span_Detection_for_References_in_Wikipedia\n",
      "Guarantees_for_Greedy_Maximization_of_Non-submodular_Functions_with_Applications\n",
      "Make_the_Minority_Great_Again___First-Order_Regret_Bound_for_Contextual_Bandits\n",
      "Detecting_Gang-Involved_Escalation_on_Social_Media_Using_Context\n",
      "Matrix_Norms_in_Data_Streams__Faster,_Multi-Pass_and_Row-Order\n",
      "Policy_Shaping_and_Generalized_Update_Equations_for_Semantic_Parsing_from_Denotations\n",
      "Deep_Keyphrase_Generation\n",
      "Exploring_Semantic_Properties_of_Sentence_Embeddings\n",
      "Obtaining_referential_word_meanings_from_visual_and_distributional_information__Experiments_on_object_naming\n",
      "Word_Emotion_Induction_for_Multiple_Languages_as_a_Deep_Multi-Task_Learning_Problem\n",
      "Commonsense_for_Generative_Multi-Hop_Question_Answering_Tasks\n",
      "Soft-DTW__a_Differentiable_Loss_Function_for_Time-Series\n",
      "Towards_Dynamic_Computation_Graphs_via_Sparse_Latent_Structure\n",
      "Semantic_Structural_Evaluation_for_Text_Simplification\n",
      "Black-Box_Variational_Inference_for_Stochastic_Differential_Equations\n",
      "A_Corpus_with_Multi-Level_Annotations_of_Patients,_Interventions_and_Outcomes_to_Support_Language_Processing_for_Medical_Literature\n",
      "Improved_Large-Scale_Graph_Learning_through_Ridge_Spectral_Sparsification\n",
      "SparseMAP__Differentiable_Sparse_Structured_Inference\n",
      "Learning_Matching_Models_with_Weak_Supervision_for_Response_Selection_in_Retrieval-based_Chatbots\n",
      "Robust_Distant_Supervision_Relation_Extraction_via_Deep_Reinforcement_Learning\n",
      "Towards_a_General,_Continuous_Model_of_Turn-taking_in_Spoken_Dialogue_using_LSTM_Recurrent_Neural_Networks\n",
      "A_Syntactic_Neural_Model_for_General-Purpose_Code_Generation\n",
      "Testing_Sparsity_over_Known_and_Unknown_Bases\n",
      "Improving_Stochastic_Policy_Gradients_in_Continuous_Control_with_Deep_Reinforcement_Learning_using_the_Beta_Distribution\n",
      "Morphological_Modeling_for_Machine_Translation_of_English-Iraqi_Arabic_Spoken_Dialogs\n",
      "Dependency-based_Hybrid_Trees_for_Semantic_Parsing\n",
      "Clustering_Semi-Random_Mixtures_of_Gaussians\n",
      "Modeling_Discourse_Cohesion_for_Discourse_Parsing_via_Memory_Network\n",
      "Variable-Length_Word_Encodings_for_Neural_Translation_Models\n",
      "A_Discriminative_Graph-Based_Parser_for_the_Abstract_Meaning_Representation\n",
      "Deep_Dyna-Q__Integrating_Planning_for_Task-Completion_Dialogue_Policy_Learning\n",
      "Asking_too_much__The_rhetorical_role_of_questions_in_political_discourse\n",
      "A_Framework_for_Understanding_the_Role_of_Morphology_in_Universal_Dependency_Parsing\n",
      "Large-Scale_Evolution_of_Image_Classifiers\n",
      "Learning_beyond_datasets__Knowledge_Graph_Augmented_Neural_Networks_for_Natural_language_Processing\n",
      "Clustering_by_Sum_of_Norms__Stochastic_Incremental_Algorithm,_Convergence_and_Cluster_Recovery\n",
      "Variational_Network_Inference__Strong_and_Stable_with_Concrete_Support\n",
      "Doubly_Accelerated_Methods_for_Faster_CCA__and_Generalized_Eigendecomposition\n",
      "Addressing_Objects_and_Their_Relations__The_Conversational_Entity_Dialogue_Model\n",
      "Online_Segment_to_Segment_Neural_Transduction\n",
      "Comparison-Based_Random_Forests\n",
      "Toward_Controlled_Generation_of_Text\n",
      "A_Reductions_Approach_to_Fair_Classification\n",
      "Unsupervised_Counselor_Dialogue_Clustering_for_Positive_Emotion_Elicitation_in_Neural_Dialogue_System\n",
      "Invariance_of_Weight_Distributions_in_Rectified_MLPs\n",
      "End-to-end_Graph-based_TAG_Parsing_with_Neural_Networks\n",
      "QMIX__Monotonic_Value_Function_Factorisation_for_Deep_Multi-Agent_Reinforcement_Learning\n",
      "Accelerating_Eulerian_Fluid_Simulation_With_Convolutional_Networks\n",
      "Global_optimization_of_Lipschitz_functions\n",
      "Accurate_Evaluation_of_Segment-level_Machine_Translation_Metrics\n",
      "Abstract_Syntax_Networks_for_Code_Generation_and_Semantic_Parsing\n",
      "Analyzing_Correlated_Evolution_of_Multiple_Features_Using_Latent_Representations\n",
      "Which_Step_Do_I_Take_First__Troubleshooting_with_Bayesian_Models\n",
      "Sequence-to-Action__End-to-End_Semantic_Graph_Generation_for_Semantic_Parsing\n",
      "Prefix_Lexicalization_of_Synchronous_CFGs_using_Synchronous_TAG\n",
      "Marrying_Up_Regular_Expressions_with_Neural_Networks__A_Case_Study_for_Spoken_Language_Understanding\n",
      "Distributed_Batch_Gaussian_Process_Optimization\n",
      "Deep_Generative_Models_for_Relational_Data_with_Side_Information\n",
      "Preferential_Bayesian_Optimization\n",
      "Towards_Decoding_as_Continuous_Optimisation_in_Neural_Machine_Translation\n",
      "Nearly_Optimal_Robust_Subspace_Tracking\n",
      "Mutual_Information_Neural_Estimation\n",
      "THE_REAL_CHALLENGE_2014__PROGRESS_AND_PROSPECTS\n",
      "Self-Bounded_Prediction_Suffix_Tree_via_Approximate_String_Matching\n",
      "Theoretical_Properties_for_Neural_Networks_with_Weight_Matrices_of_Low_Displacement_Rank\n",
      "Weakly_Supervised_Semantic_Parsing_with_Abstract_Examples\n",
      "Active_Heteroscedastic_Regression\n",
      "When_can_Multi-Site_Datasets_be_Pooled_for_Regression__Hypothesis_Tests,_2-consistency_and_Neuroscience_Applications\n",
      "Latent_Structures_for_Coreference_Resolution\n",
      "Incorporating_Uncertainty_into_Deep_Learning_for_Spoken_Language_Assessment\n",
      "Hybrid_Code_Networks__practical_and_efficient_end-to-end_dialog_control_with_supervised_and_reinforcement_learning\n",
      "Using_Automated_Metaphor_Identification_to_Aid_in_Detection_and_Prediction_of_First-Episode_Schizophrenia\n",
      "Optimal_Algorithms_for_Smooth_and_Strongly_ConvexDistributed_Optimization_in_Networks\n",
      "Retrieval-Based_Neural_Code_Generation\n",
      "Learning_to_Detect_Sepsis_with_a_Multitask_Gaussian_Process_RNN_Classifier\n",
      "Noise_Contrastive_Estimation_and_Negative_Sampling_for_Conditional_Models__Consistency_and_Statistical_Efficiency\n",
      "Near-Optimal_Design_of_Experiments_via_Regret_Minimization\n",
      "A_Computational_Cognitive_Model_of_Novel_Word_Generalization\n",
      "Towards_Exploiting_Background_Knowledge_for_Building_Conversation_Systems\n",
      "Learning_Sleep_Stages_from_Radio_Signals__A_Conditional_Adversarial_Architecture\n",
      "Evaluating_Compound_Splitters_Extrinsically_with_Textual_Entailment\n",
      "Learning_to_Act_in_Decentralized_Partially_Observable_MDPs\n",
      "Adversarial_Regression_with_Multiple_Learners\n",
      "Leveraging_Behavioral_and_Social_Information_for_Weakly_Supervised_Collective_Classification_of_Political_Discourse_on_Twitter\n",
      "Joint_Modeling_of_Content_and_Discourse_Relations_in_Dialogues\n",
      "Toward_Fast_and_Accurate_Neural_Discourse_Segmentation\n",
      "Stock_Movement_Prediction_from_Tweets_and_Historical_Prices\n",
      "Similar_but_not_the_Same__Word_Sense_Disambiguation_Improves_Event_Detection_via_Neural_Representation_Matching\n",
      "Dependent_Relational_Gamma_Process_Models_for_Longitudinal_Networks\n",
      "Integrating_Order_Information_and_Event_Relation_for_Script_Event_Prediction\n",
      "Learning_to_Prune__Exploring_the_Frontier_of_Fast_and_Accurate_Parsing\n",
      "A_La_Carte_Embedding__Cheap_but_Effective_Induction_of_Semantic_Feature_Vectors\n",
      "Efficient_Distributed_Learning_with_Sparsity\n",
      "Outta_Control__Laws_of_Semantic_Change_and_Inherent_Biases_in_Word_Representation_Models\n",
      "A_Multidimensional_Lexicon_for_Interpersonal_Stancetaking\n",
      "Coupling_Distributed_and_Symbolic_Execution_for_Natural_Language_Queries\n",
      "World_of_Bits__An_Open-Domain_Platform_for_Web-Based_Agents\n",
      "TVQA__Localized,_Compositional_Video_Question_Answering\n",
      "Tighter_Variational_Bounds_are_Not_Necessarily_Better\n",
      "Bayesian_Optimization_with_Tree-structured_Dependencies\n",
      "Can_a_Suit_of_Armor_Conduct_Electricity__A_New_Dataset_for_Open_Book_Question_Answering\n",
      "Revisiting_Character-Based_Neural_Machine_Translation_with_Capacity_and_Compression\n",
      "The_Predictron__End-To-End_Learning_and_Planning\n",
      "Scalable_Bayesian_Learning_of_Recurrent_Neural_Networks_for_Language_Modeling\n",
      "Adversarial_Multi-task_Learning_for_Text_Classification\n",
      "Evaluating_Natural_Language_Understanding_Services_for_Conversational_Question_Answering_Systems\n",
      "Deal_or_No_Deal__End-to-End_Learning_for_Negotiation_Dialogues(1)\n",
      "Do_Neural_Network_Cross-Modal_Mappings_Really_Bridge_Modalities_\n",
      "Not_All_Character_N_-grams_Are_Created_Equal__A_Study_in_Authorship_Attribution\n",
      "Learning_Diffusion_using_Hyperparameters\n",
      "Decoupling_Strategy_and_Generation_in_Negotiation_Dialogues\n",
      "Quasi-Monte_Carlo_Variational_Inference\n",
      "Diameter-Based_Active_Learning\n",
      "ExB_Text_Summarizer\n",
      "OptNet__Differentiable_Optimization_as_a_Layer_in_Neural_Networks\n",
      "Improved_Transition-Based_Parsing_by_Modeling_Characters_instead_of_Words_with_LSTMs\n",
      "Sequence-to-Sequence_Learning_as_Beam-Search_Optimization\n",
      "Empirical_comparison_of_dependency_conversions_for_RST_discourse_trees\n",
      "Learning_Discourse-level_Diversity_for_Neural_Dialog_Models_using_Conditional_Variational_Autoencoders\n",
      "From_Paraphrase_Database_to_Compositional_Paraphrase_Model_and_Back\n",
      "A_Boo(n)_for_Evaluating_Architecture_Performance\n",
      "Argument_Mining_with_Structured_SVMs_and_RNNs\n",
      "Lexical_Acquisition_through_Implicit_Confirmations_over_Multiple_Dialogues\n",
      "Dynamic_Oracles_for_Top-Down_and_In-Order_Shift-Reduce_Constituent_Parsing\n",
      "Functional_Gradient_Boosting_based_on_Residual_Network_Perception\n",
      "Improving_Viterbi_is_Hard__Better_Runtimes_Imply_Faster_Clique_Algorithms\n",
      "Dual_Fixed-Size_Ordinally_Forgetting_Encoding_(FOFE)_for_Competitive_Neural_Language_Models\n",
      "Post-Inference_Prior_Swapping\n",
      "Reviving_and_Improving_Recurrent_Back-Propagation\n",
      "Where_is_Misty__Interpreting_Spatial_Descriptors_by_Modeling_Regions_in_Space\n",
      "Understanding_Deep_Learning_Performance_through_an_Examination_of_Test_Set_Difficulty__A_Psychometric_Case_Study\n",
      "Stochastic_Language_Generation_in_Dialogue_using_Recurrent_Neural_Networks_with_Convolutional_Sentence_Reranking\n",
      "Modeling_Sparse_Deviations_for_Compressed_Sensing_using_Generative_Models\n",
      "When_do_we_laugh_\n",
      "Incorporating_Latent_Meanings_of_Morphological_Compositions_to_Enhance_Word_Embeddings\n",
      "Leveraging_distributed_representations_and_lexico-syntactic_fixedness_for_token-level_prediction_of_the_idiomaticity_of_English_verb-noun_combinations\n",
      "Evaluating_the_Variance_of_Likelihood-Ratio_Gradient_Estimators\n",
      "Game-Based_Video-Context_Dialogue\n",
      "Language-Aware_Truth_Assessment_of_Fact_Candidates\n",
      "Fair_and_Diverse_DPP-Based_Data_Summarization\n",
      "Learning_a_Lexicon_and_Translation_Model_from_Phoneme_Lattices\n",
      "Humor_Recognition_and_Humor_Anchor_Extraction\n",
      "Phrase-Based___Neural_Unsupervised_Machine_Translation\n",
      "Grammar_Variational_Autoencoder\n",
      "Multilevel_Clustering_via_Wasserstein_Means\n",
      "A_Local_Detection_Approach_for_Named_Entity_Recognition_and_Mention_Detection\n",
      "Efficient_Low-rank_Multimodal_Fusion_with_Modality-Specific_Factors\n",
      "Arc-swift__A_Novel_Transition_System_for_Dependency_Parsing\n",
      "The_Web_as_a_Knowledge-base_for_Answering_Complex_Questions\n",
      "Supervised_All-Words_Lexical_Substitution_using_Delexicalized_Features\n",
      "Deep_Probabilistic_Logic__A_Unifying_Framework_for_Indirect_Supervision\n",
      "Composing_Tree_Graphical_Models_with_Persistent_Homology_Features_for_Clustering_Mixed-Type_Data\n",
      "Extractive_Summarization_with_SWAP-NET__Sentences_and_Words_from_Alternating_Pointer_Networks\n",
      "Exploiting_Cross-Sentence_Context_for_Neural_Machine_Translation\n",
      "Temporal_Information_Extraction_by_Predicting_Relative_Time-lines\n",
      "Do_You_See_What_I_Mean__Visual_Resolution_of_Linguistic_Ambiguities\n",
      "Logical_Inference_on_Dependency-based_Compositional_Semantics\n",
      "Optimal_Distributed_Learning_with_Multi-pass_Stochastic_Gradient_Methods\n",
      "Finding_Convincing_Arguments_Using_Scalable_Bayesian_Preference_Learning\n",
      "Constituent_Parsing_as_Sequence_Labeling\n",
      "Semi-Amortized_Variational_Autoencoders\n",
      "Asymmetric_Tri-training_for_Unsupervised_Domain_Adaptation\n",
      "Conversation_Modeling_on_Reddit_Using_a_Graph-Structured_LSTM\n",
      "Guess_Me_if_You_Can__Acronym_Disambiguation_for_Enterprises\n",
      "Breaking_Locality_Accelerates_Block_Gauss-Seidel\n",
      "Generalized_Agreement_for_Bidirectional_Word_Alignment\n",
      "Dynamic_Regret_of_Strongly_Adaptive_Methods\n",
      "Language_as_a_Latent_Variable__Discrete_Generative_Models_for_Sentence_Compression\n",
      "Hierarchical_Losses_and_New_Resources_for_Fine-grained_Entity_Typing_and_Linking\n",
      "Discovering_Discrete_Latent_Topics_with_Neural_Variational_Inference\n",
      "Active_Testing__An_Efficient_and_Robust_Framework_for_Estimating_Accuracy\n",
      "Changing_the_Level_of_Directness_in_Dialogue_using_Dialogue_Vector_Models_and_Recurrent_Neural_Networks\n",
      "Being_Negative_but_Constructively__Lessons_Learnt_from_Creating_Better_Visual_Question_Answering_Datasets\n",
      "Hierarchical_Neural_Story_Generation\n",
      "Spurious_Local_Minima_are_Common_in_Two-Layer_ReLU_Neural_Networks\n",
      "Leave-one-out_Word_Alignment_without_Garbage_Collector_Effects\n",
      "Fortification_of_Neural_Morphological_Segmentation_Models_for_Polysynthetic_Minimal-Resource_Languages\n",
      "Active_Learning_for_Cost-Sensitive_Classification\n",
      "Understanding_Negation_in_Positive_Terms_Using_Syntactic_Dependencies\n",
      "Predicting_Perceived_Age__Both_Language_Ability_and_Appearance_are_Important\n",
      "Reinforcement_Learning_in_Multi-Party_Trading_Dialog\n",
      "Curiosity-driven_Exploration_by_Self-supervised_Prediction\n",
      "GradNorm__Gradient_Normalization_for_Adaptive_Loss_Balancing_in_Deep_Multitask_Networks\n",
      "Sparse_+_Group-Sparse_Dirty_Models__Statistical_Guarantees_without_Unreasonable_Conditions_and_a_Case_for_Non-Convexity\n",
      "Redundancy_Localization_for_the_Conversationalization_of_Unstructured_Responses\n",
      "DeepPath__A_Reinforcement_Learning_Method_for_Knowledge_Graph_Reasoning\n",
      "Pointwise_HSIC__A_Linear-Time_Kernelized_Co-occurrence_Norm_for_Sparse_Linguistic_Expressions\n",
      "An_Efficient,_Sparsity-Preserving,_Online_Algorithm_for_Low-Rank_Approximation\n",
      "Hierarchical_Quantized_Representations_for_Script_Generation\n",
      "Interpretable_Charge_Predictions_for_Criminal_Cases__Learning_to_Generate_Court_Views_from_Fact_Descriptions\n",
      "Generating_Sentence_Planning_Variations_for_Story_Telling\n",
      "Morphological_Segmentation_Inside-Out\n",
      "Extracting_Commonsense_Properties_from_Embeddings_with_Limited_Human_Guidance\n",
      "Disentangled_Sequential_Autoencoder\n",
      "How_Close_Are_the_Eigenvectors_of_the_Sample_and_Actual_Covariance_Matrices_\n",
      "Bidirectional_Learning_for_Time-series_Models_with_Hidden_Units\n",
      "Adversarial_Removal_of_Demographic_Attributes_from_Text_Data\n",
      "A_Unified_Variance_Reduction-Based_Framework_for_Nonconvex_Low-Rank_Matrix_Recovery\n",
      "Simultaneous_Learning_of_Trees_and_Representations_for_Extreme_Classification_and_Density_Estimation\n",
      "Learning_a_Mixture_of_Two_Multinomial_Logits\n",
      "Personality_Profiling_of_Fictional_Characters_using_Sense-Level_Links_between_Lexical_Resources\n",
      "A_Neural_Network_Approach_to_Context-Sensitive_Generation_of_Conversational_Responses\n",
      "Beyond_Binary_Labels__Political_Ideology_Prediction_of_Twitter_Users\n",
      "A_Unified_View_of_Multi-Label_Performance_Measures\n",
      "Talking_to_the_crowd__What_do_people_react_to_in_online_discussions_\n",
      "Comparing_Constraints_for_Taxonomic_Organization\n",
      "Learning_a_Part-of-Speech_Tagger_from_Two_Hours_of_Annotation\n",
      "Unsupervised_Induction_of_Semantic_Roles_within_a_Reconstruction-Error_Minimization_Framework\n",
      "Prediction_and_Control_with_Temporal_Segment_Models\n",
      "Canonical_Tensor_Decomposition_for_Knowledge_Base_Completion\n",
      "A_Comparison_of_Word_Similarity_Performance_Using_Explanatory_and_Non-explanatory_Texts\n",
      "Equivariance_Through_Parameter-Sharing\n",
      "A_Comparison_between_Count_and_Neural_Network_Models_Based_on_Joint_Translation_and_Reordering_Sequences\n",
      "Using_Lexical_Alignment_and_Referring_Ability_to_Address_Data_Sparsity_in_Situated_Dialog_Reference_Resolution\n",
      "Variational_Autoregressive_Decoder_for_Neural_Response_Generation\n",
      "Delete,_Retrieve,_Generate__a_Simple_Approach_to_Sentiment_and_Style_Transfer\n",
      "Optimal_Densification_for_Fast_and_Accurate_Minwise_Hashing\n",
      "Zero-Shot_Task_Generalization_with_Multi-Task_Deep_Reinforcement_Learning\n",
      "Context_Gates_for_Neural_Machine_Translation\n",
      "Programming_with_a_Differentiable_Forth_Interpreter\n",
      "Deep_Decentralized_Multi-task_Multi-Agent_Reinforcement_Learningunder_Partial_Observability\n",
      "A_Neural_Network_Model_for_Low-Resource_Universal_Dependency_Parsing\n",
      "Adversarial_training_for_multi-context_joint_entity_and_relation_extraction\n",
      "Neural_Metaphor_Detection_in_Context\n",
      "Deep_Bayesian_Active_Learning_with_Image_Data\n",
      "Human_Centered_NLP_with_User-Factor_Adaptation\n",
      "Detecting_Risks_in_the_Banking_System_by_Sentiment_Analysis\n",
      "Learning_to_Reweight_Examples_for_Robust_Deep_Learning\n",
      "Discourse_parsing_for_multi-party_chat_dialogues\n",
      "Sentiment_Classification_towards_Question-Answering_with_Hierarchical_Matching_Network\n",
      "Random_Feature_Expansions_for_Deep_Gaussian_Processes\n",
      "Native_Language_Cognate_Effects_on_Second_Language_Lexical_Choice\n",
      "Semantically_Equivalent_Adversarial_Rules_for_Debugging_NLP_models\n",
      "A_Stacking_Gated_Neural_Architecture_for_Implicit_Discourse_Relation_Classification\n",
      "The_Multilinear_Structure_of_ReLU_Networks\n",
      "Why_Self-Attention__A_Targeted_Evaluation_of_Neural_Machine_Translation_Architectures\n",
      "Sentence_Compression_by_Deletion_with_LSTMs\n",
      "Adaptive_Consensus_ADMM_for_Distributed_Optimization\n",
      "The_Statistical_Recurrent_Unit\n",
      "Automatic_Event_Salience_Identification\n",
      "AFET__Automatic_Fine-Grained_Entity_Typing_by_Hierarchical_Partial-Label_Embedding\n",
      "Lexical_Features_in_Coreference_Resolution__To_be_Used_With_Caution\n",
      "The_Shattered_Gradients_Problem__If_resnets_are_the_answer,_then_what_is_the_question_\n",
      "Exploring_Optimism_and_Pessimism_in_Twitter_Using_Deep_Learning\n",
      "Demand-Weighted_Completeness_Prediction_for_a_Knowledge_Base\n",
      "Neural_Semantic_Parsing_with_Type_Constraints_for_Semi-Structured_Tables\n",
      "Multiplicative_Normalizing_Flows_for_Variational_Bayesian_Neural_Networks\n",
      "Collective_Event_Detection_via_a_Hierarchical_and_Bias_Tagging_Networks_with_Gated_Multi-level_Attention_Mechanisms\n",
      "Improved_Semantic-Aware_Network_Embedding_with_Fine-Grained_Word_Alignment\n",
      "Forest-Based_Neural_Machine_Translation\n",
      "Byzantine-Robust_Distributed_Learning__Towards_Optimal_Statistical_Rates\n",
      "Learning_Polylingual_Topic_Models_from_Code-Switched_Social_Media_Documents\n",
      "Morph-fitting__Fine-Tuning_Word_Vector_Spaces_with_Simple_Language-Specific_Rules\n",
      "On_Calibration_of_Modern_Neural_Networks\n",
      "Evaluating_Theory_of_Mind_in_Question_Answering\n",
      "A_Generative_Parser_with_a_Discriminative_Recognition_Algorithm\n",
      "Data_Summarization_at_Scale_A_Two-Stage_Submodular_Approach\n",
      "Modular_Multitask_Reinforcement_Learning_with_Policy_Sketches\n",
      "JointGAN__Multi-Domain_Joint_Distribution_Learning_with__Generative_Adversarial_Nets\n",
      "A_Dynamic_Programming_Algorithm_for_Computing_N-gram_Posteriors_from_Lattices\n",
      "Density_Level_Set_Estimation_on_Manifolds_with_DBSCAN\n",
      "A_Word-Complexity_Lexicon_and_A_Neural_Readability_Ranking_Model_for_Lexical_Simplification\n",
      "MultiWOZ_-_A_Large-Scale_Multi-Domain_Wizard-of-Oz_Dataset_for_Task-Oriented_Dialogue_Modelling\n",
      "DeepBach__a_Steerable_Model_for_Bach_Chorales_Generation_\n",
      "Learning_Low-Dimensional_Temporal_Representations\n",
      "Orthographic_Features_for_Bilingual_Lexicon_Induction\n",
      "Hyperplane_Clustering_via_Dual_Principal_Component_Pursuit\n",
      "Bilingually-constrained_Synthetic_Data_for_Implicit_Discourse_Relation_Recognition\n",
      "EMNLP_versus_ACL__Analyzing_NLP_Research_Over_Time\n",
      "Efficient_Nonmyopic_Active_Search\n",
      "Quantifying_Context_Overlap_for_Training_Word_Embeddings\n",
      "A_Neural_Model_for_User_Geolocation_and_Lexical_Dialectology\n",
      "Stack-Pointer_Networks_for_Dependency_Parsing\n",
      "Please_Clap__Modeling_Applause_in_Campaign_Speeches\n",
      "Aspect_Level_Sentiment_Classification_with_Deep_Memory_Network\n",
      "Relative_Fisher_Information_and_Natural_Gradient_for_Learning_Large_Modular_Models\n",
      "Stochastic_Generative_Hashing\n",
      "Learning_a_Policy_for_Opportunistic_Active_Learning\n",
      "Implicational_Universals_in_Stochastic_Constraint-Based_Phonology\n",
      "FINET__Context-Aware_Fine-Grained_Named_Entity_Typing\n",
      "A_chance-corrected_measure_of_inter-annotator_agreement_for_syntax\n",
      "Role_play-based_question-answering_by_real_users_for_building_chatbots_with_consistent_personalities\n",
      "Iterative_Amortized_Inference\n",
      "Improving_Multilingual_Named_Entity_Recognition_with_Wikipedia_Entity_Type_Mapping\n",
      "Weakly_Submodular_Maximization_Beyond_Cardinality_Constraints__Does_Randomization_Help_Greedy_\n",
      "CharManteau__Character_Embedding_Models_For_Portmanteau_Creation\n",
      "Modeling_Others_using_Oneself_in_Multi-Agent_Reinforcement_Learning\n",
      "SMAC__Simultaneous_Mapping_and_Clustering_Using_Spectral_Decompositions\n",
      "Bleaching_Text__Abstract_Features_for_Cross-lingual_Gender_Prediction\n",
      "Algorithms_for_`p_Low-Rank_Approximation\n",
      "Learning_Semantic_Composition_to_Detect_Non-compositionality_of_Multiword_Expressions\n",
      "Representation_Learning_for_Grounded_Spatial_Reasoning\n",
      "A_Generative_Model_of_Phonotactics\n",
      "Question-Answer_Driven_Semantic_Role_Labeling__Using_Natural_Language_to_Annotate_Natural_Language\n",
      "Encoding_Temporal_Information_for_Time-Aware_Link_Prediction\n",
      "On_the_Expressive_Power_of_Deep_Neural_Networks\n",
      "Transfer_Learning_via_Learning_to_Transfer\n",
      "A_Minimal_Span-Based_Neural_Constituency_Parser\n",
      "Comparatives,_Quantifiers,_Proportions__a_Multi-Task_Model_for_the_Learning_of_Quantities_from_Vision\n",
      "Speech_segmentation_with_a_neural_encoder_model_of_working_memory\n",
      "Improving_Knowledge_Graph_Embedding_Using_Simple_Constraints\n",
      "Neural-based_Natural_Language_Generation_in_Dialogue_using_RNN_Encoder-Decoder_with_Semantic_Aggregation\n",
      "Adaptive_Document_Retrieval_for_Deep_Question_Answering\n",
      "Deep_Spectral_Clustering_Learning\n",
      "INSPECTRE__Privately_Estimating_the_Unseen\n",
      "Localizing_Moments_in_Video_with_Temporal_Language\n",
      "Evaluating_the_Utility_of_Hand-crafted_Features_in_Sequence_Labelling\n",
      "Sentences_with_Gapping__Parsing_and_Reconstructing_Elided_Predicates\n",
      "Evaluating_the_Stability_of_Embedding-based_Word_Similarities\n",
      "Searching_for_the_X-Factor__Exploring_Corpus_Subjectivity_for_Word_Embeddings\n",
      "A_Laplacian_Framework_for_Option_Discovery_in_Reinforcement_Learning\n",
      "Exploring_Recombination_for_Efficient_Decoding_of_Neural_Machine_Translation\n",
      "Discriminative_Neural_Sentence_Modeling_by_Tree-Based_Convolution\n",
      "Unbiased_Objective_Estimation_in_Predictive_Optimization\n",
      "Towards_an_Automatic_Turing_Test__Learning_to_Evaluate_Dialogue_Responses\n",
      "Natural_Language_Processing_with_Small_Feed-Forward_Networks\n",
      "Dual_Decomposition_Inference_for_Graphical_Models_over_Strings\n",
      "Hierarchical_Neural_Networks_for_Sequential_Sentence_Classification_in_Medical_Scientific_Abstracts\n",
      "Self-Governing_Neural_Networks_for_On-Device_Short_Text_Classification\n",
      "Inductive_Two-layer_Modeling_with_Parametric_Bregman_Transfer\n",
      "Probabilistic_FastText_for_Multi-Sense_Word_Embeddings\n",
      "Learning_Cognitive_Features_from_Gaze_Data_for_Sentiment_and_Sarcasm_Classification_using_Convolutional_Neural_Network\n",
      "Associative_Multichannel_Autoencoder_for_Multimodal_Word_Representation\n",
      "Robust_Probabilistic_Modeling_with_Bayesian_Data_Reweighting\n",
      "APRO__All-Pairs_Ranking_Optimization_for_MT_Tuning\n",
      "Learning_Structured_Perceptrons_for_Coreference_Resolution_with_Latent_Antecedents_and_Non-local_Features\n",
      "Cross-Lingual_Syntactic_Transfer_with_Limited_Resources\n",
      "Latent_Intention_Dialogue_Models\n",
      "Context-aware_Learning_for_Sentence-level_Sentiment_Analysis_with_Posterior_Regularization\n",
      "Reasoning_with_Heterogeneous_Knowledge_for_Commonsense_Machine_Comprehension\n",
      "Discourse_Mode_Identification_in_Essays\n",
      "Learning_Continuous_Semantic_Representations_of_Symbolic_Expressions\n",
      "Efficient_Orthogonal_Parametrisation_of_Recurrent_Neural_Networks__Using_Householder_Reflections\n",
      "Leveraging_Node_Attributes_for_Incomplete_Relational_Data\n",
      "Reduced_Space_and_Faster_Convergence_in_Imperfect-Information_Games_via_Pruning\n",
      "Stochastic_Gradient_MCMC_Methods_for_Hidden_Markov_Models\n",
      "Hybrid_Neural_Network_Alignment_and_Lexicon_Model_in_Direct_HMM_for_Statistical_Machine_Translation\n",
      "Easy-First_Dependency_Parsing_with_Hierarchical_Tree_LSTMs\n",
      "Explaining_Character-Aware_Neural_Networks_for_Word-Level_Prediction__Do_They_Discover_Linguistic_Rules_\n",
      "An_Optimal_Control_Approach_to_Deep_Learning_and__Applications_to_Discrete-Weight_Neural_Networks\n",
      "A_Neural_Local_Coherence_Model_for_Text_Quality_Assessment\n",
      "Learning_Binary_Latent_Variable_Models__A_Tensor_Eigenpair_Approach\n",
      "Stronger_Generalization_Bounds_for_Deep_Nets_via_a_Compression_Approach\n",
      "Prediction_for_the_Newsroom__Which_Articles_Will_Get_the_Most_Comments_\n",
      "Stability_and_Generalization_of_Learning_Algorithms_that_Converge_to_Global_Optima\n",
      "Discovering_User_Groups_for_Natural_Language_Generation\n",
      "Creating_Causal_Embeddings_for_Question_Answering_with_Minimal_Supervision\n",
      "A_Multi-Dimensional_Bayesian_Approach_to_Lexical_Style\n",
      "Online_Learning_with_Abstention\n",
      "Forward_and_Reverse_Gradient-Based_Hyperparameter_Optimization\n",
      "The_State_of_the_Art_in_Semantic_Representation\n",
      "Limits_of_Estimating_Heterogeneous_Treatment_Effects__Guidelines_for_Practical_Algorithm_Design\n",
      "Questionable_Answers_in_Question_Answering_Research__Reproducibility_and_Variability_of_Published_Results\n",
      "Part-of-Speech_Tagging_for_Twitter_with_Adversarial_Neural_Networks\n",
      "Input_Switched_Affine_Networks__An_RNN_Architecture_Designed_for_Interpretability\n",
      "A_quantitative_analysis_of_gender_differences_in_movies_using_psycholinguistic_normatives\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# traverse root directory, and list directories as dirs and files as files\n",
    "for root, dirs, files in os.walk(\"../papers-xmls\"):\n",
    "    #path = root.split(os.sep)\n",
    "    #print((len(path) - 1) * '---', os.path.basename(root))\n",
    "    for file in files:\n",
    "        if(file.endswith(\".tei.xml\")):\n",
    "            x='_'.join(file.split('.')[:-2])\n",
    "            y='_'.join(x.split(' '))\n",
    "            print(y)\n",
    "            #os.mkdir(os.path.join(\"OUT\", y))\n",
    "            os.rename(\"../papers-xmls/\"+file, \"../papers-xmls/\"+y+\".tei.xml\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DCFNet_ Deep Neural Network with Decomposed Convolutional Filters.tei.xml',\n",
       " 'Modeling Semantic Expectation_ Using Script Knowledge for Referent Prediction.tei.xml',\n",
       " 'EQUATION PARSING _ Mapping Sentences to Grounded Equations.tei.xml',\n",
       " 'Multi-task Learning with Labeled and Unlabeled Tasks.tei.xml',\n",
       " 'CoSimRank_ A Flexible _ Efficient Graph-Theoretic Similarity Measure.tei.xml',\n",
       " 'Miscommunication Recovery in Physically Situated Dialogue.tei.xml',\n",
       " 'An Empirical Study of Self-Disclosure in Spoken Dialogue Systems.tei.xml',\n",
       " 'Targeted Syntactic Evaluation of Language Models.tei.xml',\n",
       " 'Supervised Learning of Automatic Pyramid for Optimization-Based Multi-Document Summarization.tei.xml',\n",
       " 'TextFlow_ A Text Similarity Measure based on Continuous Sequences.tei.xml',\n",
       " 'Colorless green recurrent networks dream hierarchically.tei.xml',\n",
       " 'Stochastic Gradient Monomial Gamma Sampler.tei.xml',\n",
       " 'Comparing Dynamics_ Deep Neural Networks versus Glassy Systems.tei.xml',\n",
       " 'RESIDE_ Improving Distantly-Supervised Neural Relation Extraction using Side Information.tei.xml',\n",
       " 'Trainable Greedy Decoding for Neural Machine Translation.tei.xml',\n",
       " 'Bayesian Uncertainty Estimation for Batch Normalized Deep Networks.tei.xml',\n",
       " 'Count-Based Exploration with Neural Density Models.tei.xml',\n",
       " 'The Argument Reasoning Comprehension Task_ Identification and Reconstruction of Implicit Warrants.tei.xml',\n",
       " 'Real-Time Adaptive Image Compression.tei.xml',\n",
       " 'Polyglot Semantic Role Labeling.tei.xml',\n",
       " 'Cross-lingual Abstract Meaning Representation Parsing.tei.xml',\n",
       " 'Learning to Aggregate Ordinal Labels by Maximizing Separating Width.tei.xml',\n",
       " 'Sequential Dialogue Context Modeling for Spoken Language Understanding.tei.xml',\n",
       " 'Delayed Impact of Fair Machine Learning.tei.xml',\n",
       " 'Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data.tei.xml',\n",
       " 'Variational Inference and Model Selection  with Generalized Evidence Bounds.tei.xml',\n",
       " 'Stein Variational Message Passing for Continuous Graphical Models.tei.xml',\n",
       " 'Dance Dance Convolution.tei.xml',\n",
       " 'Inference Suboptimality in Variational Autoencoders.tei.xml',\n",
       " 'Anchoring and Agreement in Syntactic Annotations.tei.xml',\n",
       " 'Beyond Error Propagation in Neural Machine Translation_ Characteristics of Language Also Matter.tei.xml',\n",
       " 'A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization.tei.xml',\n",
       " 'Unfolding and Shrinking Neural Machine Translation Ensembles.tei.xml',\n",
       " 'Tensor Fusion Network for Multimodal Sentiment Analysis.tei.xml',\n",
       " 'Satirical News Detection and Analysis using Attention Mechanism and Linguistic Features.tei.xml',\n",
       " 'Ordinal Graphical Models_ A Tale of Two Approaches.tei.xml',\n",
       " 'The Best of Both Worlds_ Combining Recent Advances in Neural Machine Translation.tei.xml',\n",
       " 'Massively Parallel Algorithms and Hardness for Single-Linkage Clustering under `p Distances.tei.xml',\n",
       " 'Tensor-Train Recurrent Neural Networks for Video Classification.tei.xml',\n",
       " 'Graph Convolutional Encoders for Syntax-aware Neural Machine Translation.tei.xml',\n",
       " 'AMR-to-text Generation with Synchronous Node Replacement Grammar.tei.xml',\n",
       " 'Learning to Disentangle Interleaved Conversational Threads with a Siamese Hierarchical Network and Similarity Ranking.tei.xml',\n",
       " 'Why Neural Translations are the Right Length.tei.xml',\n",
       " 'Knowledgeable Reader_ Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge.tei.xml',\n",
       " 'Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation.tei.xml',\n",
       " 'CARER_ Contextualized Affect Representations for Emotion Recognition.tei.xml',\n",
       " 'Cross-Sentence N -ary Relation Extraction with Graph LSTMs.tei.xml',\n",
       " 'Detecting Egregious Conversations between Customers and Virtual Agents.tei.xml',\n",
       " 'Information-Theory Interpretation of the Skip-Gram Negative-Sampling Objective Function.tei.xml',\n",
       " 'diaNED_ Time-Aware Named Entity Disambiguation for Diachronic Corpora.tei.xml',\n",
       " 'Failures of Gradient-Based Deep Learning.tei.xml',\n",
       " 'Semi-Supervised QA with Generative Domain-Adaptive Nets.tei.xml',\n",
       " 'An Alternative Softmax Operator for Reinforcement Learning.tei.xml',\n",
       " 'On the Sampling Problem for Kernel Quadrature.tei.xml',\n",
       " 'A Framework for Representing Language Acquisition in a Population Setting.tei.xml',\n",
       " 'SQL-Rank_ A Listwise Approach to Collaborative Ranking.tei.xml',\n",
       " 'Rationalizing Neural Predictions.tei.xml',\n",
       " 'Differentially Private Clustering in High-Dimensional Euclidean Spaces.tei.xml',\n",
       " 'Probabilistic Path Hamiltonian Monte Carlo.tei.xml',\n",
       " 'Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation.tei.xml',\n",
       " 'Learning a Neural Semantic Parser from User Feedback.tei.xml',\n",
       " 'Learning Neural Templates for Text Generation.tei.xml',\n",
       " 'Axiomatic Attribution for Deep Networks.tei.xml',\n",
       " 'Subspace Embedding and Linear Regression with Orlicz Norm.tei.xml',\n",
       " 'Predicting News Headline Popularity with Syntactic and Semantic Knowledge Using Multi-Task Learning.tei.xml',\n",
       " 'Analytical Guarantees on Numerical Precision of Deep Neural Networks.tei.xml',\n",
       " 'Matching Citation Text and Cited Spans in Biomedical Literature_ a Search-Oriented Approach.tei.xml',\n",
       " 'Conditional Noise-Contrastive Estimation of Unnormalised Models.tei.xml',\n",
       " 'A Distributional and Orthographic Aggregation Model for English Derivational Morphology.tei.xml',\n",
       " 'Deep-speare_ A joint neural model of poetic language, meter and rhyme.tei.xml',\n",
       " 'Joint Named Entity Recognition and Disambiguation.tei.xml',\n",
       " 'Deriving Boolean structures from distributional vectors.tei.xml',\n",
       " 'Averaged-DQN_ Variance Reduction and Stabilization for Deep Reinforcement Learning.tei.xml',\n",
       " 'Robust Guarantees of Stochastic Greedy Algorithms.tei.xml',\n",
       " 'Implicitly-Defined Neural Networks for Sequence Labeling.tei.xml',\n",
       " 'Finding Syntax in Human Encephalography with Beam Search.tei.xml',\n",
       " 'Network Global Testing by Counting Graphlets.tei.xml',\n",
       " 'Variational Policy for Guiding Point Processes.tei.xml',\n",
       " 'Representation Learning on Graphs with Jumping Knowledge Networks .tei.xml',\n",
       " 'Deep Exhaustive Model for Nested Named Entity Recognition.tei.xml',\n",
       " 'Global Neural CCG Parsing with Optimality Guarantees.tei.xml',\n",
       " 'Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm.tei.xml',\n",
       " 'Any-language frame-semantic parsing.tei.xml',\n",
       " 'Probabilistic Typology_ Deep Generative Models of Vowel Inventories.tei.xml',\n",
       " 'Natural Language Comprehension with the EpiReader.tei.xml',\n",
       " 'Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video.tei.xml',\n",
       " 'Improving Statistical Machine Translation with a Multilingual Paraphrase Database.tei.xml',\n",
       " 'RecipeQA_ A Challenge Dataset for Multimodal Comprehension of Cooking Recipes.tei.xml',\n",
       " 'Multi-Timescale Long Short-Term Memory Neural Network for Modelling Sentences and Documents.tei.xml',\n",
       " 'Bridging Languages through Images with Deep Partial Canonical Correlation Analysis.tei.xml',\n",
       " 'Reasoning about Actions and State Changes by Injecting Commonsense Knowledge.tei.xml',\n",
       " 'Just Sort It! A Simple and Effective Approach to Active Preference Learning.tei.xml',\n",
       " 'Learning to Speed Up Structured Output Prediction.tei.xml',\n",
       " 'Deep Dirichlet Multinomial Regression.tei.xml',\n",
       " 'Input Convex Neural Networks.tei.xml',\n",
       " 'Affect-LM_ A Neural Language Model for Customizable Affective Text Generation.tei.xml',\n",
       " 'A Neural Network for Coordination Boundary Prediction.tei.xml',\n",
       " 'Noisy Natural Gradient as Variational Inference.tei.xml',\n",
       " 'Adaptive Knowledge Sharing in Multi-Task Learning_ Improving Low-Resource Neural Machine Translation.tei.xml',\n",
       " 'Linguistically-Informed Self-Attention for Semantic Role Labeling.tei.xml',\n",
       " 'Lexicosyntactic Inference in Neural Models.tei.xml',\n",
       " 'Sliced Wasserstein Kernel for Persistence Diagrams.tei.xml',\n",
       " 'Projection-free Distributed Online Learning in Networks.tei.xml',\n",
       " 'Robust Structured Estimation with Single-Index Models.tei.xml',\n",
       " 'Improved Relation Extraction with Feature-Rich Compositional Embedding Models.tei.xml',\n",
       " 'Pain-Free Random Differential Privacy with Sensitivity Sampling.tei.xml',\n",
       " 'Joint Reasoning for Temporal and Causal Relations.tei.xml',\n",
       " 'Multilabel Classification with Group Testing and Codes.tei.xml',\n",
       " 'Head-Lexicalized Bidirectional Tree LSTMs.tei.xml',\n",
       " 'Variational Dropout Sparsifies Deep Neural Networks.tei.xml',\n",
       " 'Zipporah_ a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora.tei.xml',\n",
       " 'Rule Extraction for Tree-to-Tree Transducers by Cost Minimization.tei.xml',\n",
       " 'Spectrally Approximating Large Graphs with Smaller Graphs.tei.xml',\n",
       " 'Lazifying Conditional Gradient Algorithms.tei.xml',\n",
       " 'Joint Dimensionality Reduction and Metric Learning_ A Geometric Take.tei.xml',\n",
       " 'Recurrent Predictive State Policy Networks.tei.xml',\n",
       " 'Ultra-Fine Entity Typing.tei.xml',\n",
       " 'Deep Learning in Semantic Kernel Spaces.tei.xml',\n",
       " 'Entity Linking for Queries by Searching Wikipedia Sentences.tei.xml',\n",
       " 'Neural Text Generation in Stories Using Entity Representations as Context.tei.xml',\n",
       " 'Learning unknown ODE models with Gaussian processes.tei.xml',\n",
       " 'Predicting Semantic Relations using Global Graph Properties.tei.xml',\n",
       " 'Does Distributionally Robust Supervised Learning Give Robust Classifiers_.tei.xml',\n",
       " 'Near Optimal Frequent Directions for Sketching Dense and Sparse Matrices.tei.xml',\n",
       " 'Meta-Learning for Low-Resource Neural Machine Translation.tei.xml',\n",
       " 'Deep Linear Networks with Arbitrary Loss_ All Local Minima Are Global.tei.xml',\n",
       " 'Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing.tei.xml',\n",
       " 'Maximum Selection and Ranking under Noisy Comparisons.tei.xml',\n",
       " 'Bag of Experts Architectures for Model Reuse in Conversational Language Understanding.tei.xml',\n",
       " 'NeuralREG_ An end-to-end approach to referring expression generation.tei.xml',\n",
       " 'Kernelized Support Tensor Machines.tei.xml',\n",
       " 'Segmentation for Efficient Supervised Language Annotation with an Explicit Cost-Utility Tradeoff.tei.xml',\n",
       " 'Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification.tei.xml',\n",
       " 'A Principled Framework for Evaluating Summarizers_ Comparing Models of Summary Quality against Human Judgments.tei.xml',\n",
       " 'Approximate Newton Methods and Their Local Convergence.tei.xml',\n",
       " 'Joint Concept Learning and Semantic Parsing from Natural Language Explanations.tei.xml',\n",
       " 'A Nested Attention Neural Hybrid Model for Grammatical Error Correction.tei.xml',\n",
       " 'Incorporating Dialectal Variability for Socially Equitable Language Identification.tei.xml',\n",
       " 'Soft Actor-Critic_ Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.tei.xml',\n",
       " 'Automatically Generating Rhythmic Verse with Neural Networks.tei.xml',\n",
       " 'A Richer Theory of Convex Constrained Optimization with Reduced Projections and Improved Rates.tei.xml',\n",
       " 'Differentiable Dynamic Programming for Structured Prediction and Attention.tei.xml',\n",
       " 'Submodular Hypergraphs_ p-Laplacians, Cheeger Inequalities  and Spectral Clustering.tei.xml',\n",
       " 'A Sense-Based Translation Model for Statistical Machine Translation.tei.xml',\n",
       " 'Author Commitment and Social Power_ Automatic Belief Tagging to Infer the Social Context of Interactions.tei.xml',\n",
       " 'Learning How to Actively Learn_ A Deep Imitation Learning Approach.tei.xml',\n",
       " 'Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification.tei.xml',\n",
       " 'Imitation Learning for Neural Morphological String Transduction.tei.xml',\n",
       " 'Retrieval of the Best Counterargument without Prior Topic Knowledge.tei.xml',\n",
       " 'Deep One-Class Classification.tei.xml',\n",
       " 'Generalized Earley Parser_ Bridging Symbolic Grammars and Sequence Data for Future Prediction.tei.xml',\n",
       " 'Paraphrase to Explicate_ Revealing Implicit Noun-Compound Relations.tei.xml',\n",
       " 'Exact MAP Inference by Avoiding Fractional Vertices.tei.xml',\n",
       " 'A Co-Matching Model for Multi-choice Reading Comprehension.tei.xml',\n",
       " 'Neural Machine Translation with Source-Side Latent Graph Parsing.tei.xml',\n",
       " 'LEAPSANDBOUNDS_ A Method for Approximately Optimal Algorithm Configuration.tei.xml',\n",
       " 'Representing Text for Joint Embedding of Text and Knowledge Bases.tei.xml',\n",
       " 'Modeling Semantic Plausibility by Injecting World Knowledge.tei.xml',\n",
       " 'Concept Transfer Learning for Adaptive Language Understanding.tei.xml',\n",
       " 'Coming to Your Senses_ on Controls and Evaluation Sets in Polysemy Research.tei.xml',\n",
       " 'ZipML_ Training Linear Models with End-to-End Low Precision, and a Little Bit of Deep Learning.tei.xml',\n",
       " 'Bayesian Optimization of Text Representations.tei.xml',\n",
       " 'Deep Neural Machine Translation with Linear Associative Unit.tei.xml',\n",
       " 'Toward Efficient and Accurate Covariance Matrix  Estimation on Compressed Data.tei.xml',\n",
       " 'Sentence Compression for Arbitrary Languages via Multilingual Pivoting.tei.xml',\n",
       " 'Fluency Boost Learning and Inference for Neural Grammatical Error Correction.tei.xml',\n",
       " 'Exploring the Role of Prior Beliefs for Argument Persuasion.tei.xml',\n",
       " 'Adapting Kernel Representations Online Using Submodular Maximization.tei.xml',\n",
       " 'Not All Samples Are Created Equal_  Deep Learning with Importance Sampling.tei.xml',\n",
       " 'Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification.tei.xml',\n",
       " 'Co-Training for Topic Classification of Scholarly Data.tei.xml',\n",
       " 'Pretraining Sentiment Classifiers with Unlabeled Dialog Data.tei.xml',\n",
       " 'KBGAN_ Adversarial Learning for Knowledge Graph Embeddings.tei.xml',\n",
       " 'Detecting Perspectives in Political Debates.tei.xml',\n",
       " 'Scalable Multi-Class Gaussian Process Classification  using Expectation Propagation.tei.xml',\n",
       " 'Sequential Matching Network_ A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots.tei.xml',\n",
       " 'Cut to the Chase_ A Context Zoom-in Network for Reading Comprehension.tei.xml',\n",
       " 'Program Induction by Rationale Generation_ Learning to Solve and Explain Algebraic Word Problems.tei.xml',\n",
       " 'What do Neural Machine Translation Models Learn about Morphology_.tei.xml',\n",
       " 'Fast and Scalable Expansion of Natural Language Understanding Functionality for Intelligent Agents.tei.xml',\n",
       " 'Broad-coverage CCG Semantic Parsing with AMR.tei.xml',\n",
       " 'Capacity Releasing Diffusion for Speed and Locality.tei.xml',\n",
       " 'Linear-Time Constituency Parsing with RNNs and Dynamic Programming.tei.xml',\n",
       " 'Learning Memory Access Patterns.tei.xml',\n",
       " 'Efficient softmax approximation for GPUs.tei.xml',\n",
       " 'Adversarial Contrastive Estimation.tei.xml',\n",
       " 'Phrase-Indexed Question Answering_ A New Challenge for Scalable Document Comprehension.tei.xml',\n",
       " 'Automatic Discovery of the Statistical Types of Variables in a Dataset.tei.xml',\n",
       " 'A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations.tei.xml',\n",
       " 'High-Quality Prediction Intervals for Deep Learning_  A Distribution-Free, Ensembled Approach.tei.xml',\n",
       " 'Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme.tei.xml',\n",
       " 'Zero-Shot Dialog Generation with Cross-Domain Latent Actions.tei.xml',\n",
       " 'Forest-type Regression with General Losses  and Robust Forest.tei.xml',\n",
       " 'Natasha_ Faster Non-Convex Stochastic Optimization  via Strongly Non-Convex Parameter.tei.xml',\n",
       " 'Sample-efficient Actor-Critic Reinforcement Learning with Supervised Data for Dialogue Management.tei.xml',\n",
       " 'Constrained Interacting Submodular Groupings.tei.xml',\n",
       " 'Semi-supervised Chinese Word Segmentation based on Bilingual Information.tei.xml',\n",
       " 'Convex Phase Retrieval without Lifting via PhaseMax.tei.xml',\n",
       " 'Neural Utterance Ranking Model for Conversational Dialogue Systems.tei.xml',\n",
       " 'Parameterized Algorithms for the Matrix Completion Problem.tei.xml',\n",
       " 'An Analysis of Frequency- and Memory-Based Processing Costs.tei.xml',\n",
       " 'Neural Models for Documents with Metadata.tei.xml',\n",
       " 'Stochastic Variance-Reduced Hamilton Monte Carlo Methods.tei.xml',\n",
       " 'Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples.tei.xml',\n",
       " 'Improving Sign Random Projections With Additional Information.tei.xml',\n",
       " 'Understanding and Detecting Supporting Arguments of Diverse Types.tei.xml',\n",
       " 'Label-aware Double Transfer Learning for Cross-Specialty Medical Named Entity Recognition.tei.xml',\n",
       " 'Session-level Language Modeling for Conversational Speech.tei.xml',\n",
       " 'Optimization, Fast and Slow_ Optimally Switching between Local and Bayesian Optimization.tei.xml',\n",
       " 'Topically Driven Neural Language Model.tei.xml',\n",
       " 'Numeracy for Language Models_ Evaluating and Improving their Ability to Predict Numbers.tei.xml',\n",
       " 'Deep Generative Model for Joint Alignment and Word Representation.tei.xml',\n",
       " 'Unpaired Sentiment-to-Sentiment Translation_ A Cycled Reinforcement Learning Approach.tei.xml',\n",
       " 'Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths.tei.xml',\n",
       " 'Frames_ A Corpus for Adding Memory to Goal-Oriented Dialogue Systems.tei.xml',\n",
       " 'Image-to-Markup Generation with Coarse-to-Fine Attention.tei.xml',\n",
       " 'Deep Reinforcement Learning for Mention-Ranking Coreference Models.tei.xml',\n",
       " 'Nonconvex Optimization for Regression with Fairness Constraints.tei.xml',\n",
       " 'A_ CCG Parsing with a Supertag and Dependency Factored Model.tei.xml',\n",
       " 'HUME_ Human UCCA-Based Evaluation of Machine Translation.tei.xml',\n",
       " 'A Step-wise Usage-based Method for Inducing Polysemy-aware Verb Classes.tei.xml',\n",
       " 'Exploiting Sentence Similarities for Better Alignments.tei.xml',\n",
       " 'Neural Related Work Summarization with a Joint Context-driven Attention Mechanism.tei.xml',\n",
       " 'Bounds on the Approximation Power of Feedforward Neural Networks.tei.xml',\n",
       " 'oi-VAE_ Output Interpretable VAEs for Nonlinear Group Factor Analysis.tei.xml',\n",
       " 'Fast, Small and Exact_ Infinite-order Language Modelling with Compressed Suffix Trees.tei.xml',\n",
       " 'Quickshift++_ Provably Good Initializations for Sample-Based Mean Shift.tei.xml',\n",
       " 'Dependency Graph-to-String Translation.tei.xml',\n",
       " 'Stochastic Convex Optimization_ Faster Local Growth Implies Faster Global Convergence.tei.xml',\n",
       " 'Marginal Likelihood Training of BiLSTM-CRF for Biomedical Named Entity Recognition from Disjoint Label Sets.tei.xml',\n",
       " 'Mitigating Bias in Adaptive Data Gathering via Differential Privacy.tei.xml',\n",
       " 'Hierarchical Recurrent Neural Network for Document Modeling.tei.xml',\n",
       " 'Augment and Reduce_ Stochastic Inference for Large Categorical Distributions.tei.xml',\n",
       " 'Is this a wampimuk_ Cross-modal mapping between distributional semantics and the visual world.tei.xml',\n",
       " 'Regularising Non-linear Models Using Feature Side-information.tei.xml',\n",
       " 'On orthogonality and learning recurrent networks with long term dependencies.tei.xml',\n",
       " 'Do latent tree learning models identify meaningful structure in sentences_.tei.xml',\n",
       " 'Neural-Davidsonian Semantic Proto-role Labeling.tei.xml',\n",
       " 'Stochastic Training of Graph Convolutional Networks with Variance Reduction.tei.xml',\n",
       " 'MEC_ Memory-efficient Convolution for Deep Neural Network .tei.xml',\n",
       " 'Tense Manages to Predict Implicative Behavior in Verbs.tei.xml',\n",
       " 'A Birth-Death Process for Feature Allocation.tei.xml',\n",
       " 'Knowledge Base Inference using Bridging Entities.tei.xml',\n",
       " 'Closed-form Marginal Likelihood in Gamma-Poisson Matrix Factorization.tei.xml',\n",
       " 'Improving the Gaussian Mechanism for Differential Privacy_ Analytical Calibration and Optimal Denoising.tei.xml',\n",
       " 'Parsing Speech_ A Neural Approach to Integrating Lexical and Acoustic-Prosodic Information.tei.xml',\n",
       " 'Heuristically Informed Unsupervised Idiom Usage Recognition.tei.xml',\n",
       " 'On The Projection Operator to A Three-view Cardinality Constrained Set(1).tei.xml',\n",
       " 'Fully Character-Level Neural Machine Translation without Explicit Segmentation.tei.xml',\n",
       " 'Developing Bug-Free Machine Learning Systems With Formal Mathematics.tei.xml',\n",
       " 'Probabilistic Boolean Tensor Decomposition.tei.xml',\n",
       " 'Generalizing Word Embeddings using Bag of Subwords.tei.xml',\n",
       " 'Training Classifiers with Natural Language Explanations.tei.xml',\n",
       " 'Improved Neural Relation Detection for Knowledge Base Question Answering.tei.xml',\n",
       " 'Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference.tei.xml',\n",
       " 'Knowledge-Rich Morphological Priors for Bayesian Language Models.tei.xml',\n",
       " 'Measuring the Evolution of a Scientific Field through Citation Frames.tei.xml',\n",
       " 'Neural Cross-Lingual Coreference Resolution And Its Application To Entity Linking.tei.xml',\n",
       " 'Batch IS NOT Heavy_ Learning Word Representations From All Samples.tei.xml',\n",
       " 'The Role of Conversation Context for Sarcasm Detection in Online Interactions.tei.xml',\n",
       " 'A Transition-Based Directed Acyclic Graph Parser for UCCA.tei.xml',\n",
       " 'Breaking NLI Systems with Sentences that Require Simple Lexical Inferences.tei.xml',\n",
       " 'A Binarized Neural Network Joint Model for Machine Translation.tei.xml',\n",
       " 'Joint Mention Extraction and Classification with Mention Hypergraphs.tei.xml',\n",
       " 'Learning Simplifications for Specific Target Audiences.tei.xml',\n",
       " 'When Are Tree Structures Necessary for Deep Learning of Representations_.tei.xml',\n",
       " 'Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks.tei.xml',\n",
       " 'Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model.tei.xml',\n",
       " 'Intra-Sentential Subject Zero Anaphora Resolution using Multi-Column Convolutional Neural Network.tei.xml',\n",
       " 'Learning attention for historical text normalization by learning to pronounce.tei.xml',\n",
       " 'The Weighted Kendall and High-order Kernels for Permutations.tei.xml',\n",
       " 'Fast Information-theoretic Bayesian Optimisation.tei.xml',\n",
       " 'Towards Binary-Valued Gates for Robust LSTM Training.tei.xml',\n",
       " 'Modelling Protagonist Goals and Desires in First-Person Narrative.tei.xml',\n",
       " 'Visualizing and Understanding Neural Machine Translation.tei.xml',\n",
       " 'Detecting and Characterizing Events.tei.xml',\n",
       " 'Selective Inference for Sparse High-Order Interaction Models.tei.xml',\n",
       " 'Controlling Personality-Based Stylistic Variation with Neural Natural Language Generators.tei.xml',\n",
       " 'DOC_ Deep Open Classification of Text Documents.tei.xml',\n",
       " 'Recursive Neural Structural Correspondence Network for Cross-domain Aspect and Opinion Co-Extraction.tei.xml',\n",
       " 'Friendships, Rivalries, and Trysts_ Characterizing Relations between Ideas in Texts.tei.xml',\n",
       " 'Learning Stable Stochastic Nonlinear Dynamical Systems.tei.xml',\n",
       " 'A Spatial Model for Extracting and Visualizing Latent Discourse Structure in Text.tei.xml',\n",
       " 'Cogent_ A Generic Dialogue System Shell Based on a Collaborative Problem Solving Model.tei.xml',\n",
       " 'Learning Important Features Through Propagating Activation Differences.tei.xml',\n",
       " 'Argument Mining_ Extracting Arguments from Online Dialogue.tei.xml',\n",
       " 'XNLI_ Evaluating Cross-lingual Sentence Representations.tei.xml',\n",
       " 'Unimodal Probability Distributions for Deep Ordinal Classification.tei.xml',\n",
       " 'On the Spectrum of Random Features Maps of High Dimensional Data.tei.xml',\n",
       " 'Experiments with crowdsourced re-annotation of a POS tagging data set.tei.xml',\n",
       " 'Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam.tei.xml',\n",
       " 'Local Private Hypothesis Testing_ Chi-Square Tests.tei.xml',\n",
       " 'Comment-to-Article Linking in the Online News Domain.tei.xml',\n",
       " 'A strong baseline for question relevancy ranking.tei.xml',\n",
       " 'Extracting Condition-Opinion Relations Toward Fine-grained Opinion Mining.tei.xml',\n",
       " 'Microblog Conversation Recommendation via Joint Modeling of Topics and Discourse.tei.xml',\n",
       " 'Finding Influential Training Samples for Gradient Boosted Decision Trees.tei.xml',\n",
       " 'Randomized Greedy Inference for Joint Segmentation, POS Tagging and Dependency Parsing.tei.xml',\n",
       " 'A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions.tei.xml',\n",
       " 'Neural Architectures for Multilingual Semantic Parsing.tei.xml',\n",
       " 'Differentiable Programs with Neural Libraries.tei.xml',\n",
       " 'Multicalibration_ Calibration for the (Computationally-Identifiable) Masses.tei.xml',\n",
       " 'Combining Language and Vision with a Multimodal Skip-gram Model.tei.xml',\n",
       " 'Learning Semantic Representations for Nonterminals in Hierarchical Phrase-Based Translation.tei.xml',\n",
       " 'Zero-Shot Transfer Learning for Event Extraction.tei.xml',\n",
       " 'Deep Multi-Task Learning with Shared Memory for Text Classification.tei.xml',\n",
       " 'Understanding Black-box Predictions via Influence Functions.tei.xml',\n",
       " 'CALCS_ Continuously Approximating Longest Common Subsequence for Sequence Level Optimization.tei.xml',\n",
       " 'Provable Variable Selection for Streaming Features.tei.xml',\n",
       " 'Solving Geometry Problems_ Combining Text and Diagram Interpretation.tei.xml',\n",
       " 'A Structured Learning Approach to Temporal Relation Extraction.tei.xml',\n",
       " 'Cross-topic Argument Mining from Heterogeneous Sources.tei.xml',\n",
       " 'Continuous Representation of Location for Geolocation and Lexical Dialectology using Mixture Density Networks.tei.xml',\n",
       " 'Neural Discourse Structure for Text Categorization.tei.xml',\n",
       " 'Accelerated Spectral Ranking.tei.xml',\n",
       " 'Follow the Moving Leader in Deep Learning.tei.xml',\n",
       " 'Surprisingly Easy Hard-Attention for Sequence to Sequence Learning.tei.xml',\n",
       " 'Supersense Tagging for Arabic_ the MT-in-the-Middle Attack.tei.xml',\n",
       " 'Video Prediction with Appearance and Motion Conditions.tei.xml',\n",
       " 'Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning.tei.xml',\n",
       " 'Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning.tei.xml',\n",
       " 'Spatio-temporal Bayesian On-line Changepoint Detection with Model Selection .tei.xml',\n",
       " 'SQuAD_ 100,000+ Questions for Machine Comprehension of Text.tei.xml',\n",
       " 'CORE_ Context-Aware Open Relation Extraction with Factorization Machines.tei.xml',\n",
       " 'Efficient and Consistent Adversarial Bipartite Matching.tei.xml',\n",
       " 'Adaptive Exploration-Exploitation Tradeoff for Opportunistic Bandits.tei.xml',\n",
       " 'Faster Greedy MAP Inference for Determinantal Point Processes.tei.xml',\n",
       " 'An Interpretable Knowledge Transfer Model for Knowledge Base Completion.tei.xml',\n",
       " 'Learning the Structure of Generative Models without Labeled Data.tei.xml',\n",
       " 'Understanding Synthetic Gradients and Decoupled Neural Interfaces.tei.xml',\n",
       " 'Content Selection in Deep Learning Models of Summarization.tei.xml',\n",
       " 'Mapping to Declarative Knowledge for Word Problem Solving.tei.xml',\n",
       " 'Sequence-Level Knowledge Distillation.tei.xml',\n",
       " 'Scene Graph Parsing as Dependency Parsing.tei.xml',\n",
       " 'A Hierarchical Latent Structure for Variational Conversation Modeling.tei.xml',\n",
       " 'Selecting Machine-Translated Data for Quick Bootstrapping of a Natural Language Understanding System.tei.xml',\n",
       " 'Maximum Margin Reward Networks for Learning from Explicit and Implicit Supervision.tei.xml',\n",
       " 'Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space.tei.xml',\n",
       " 'Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution.tei.xml',\n",
       " 'A Unified Syntax-aware Framework for Semantic Role Labeling.tei.xml',\n",
       " 'Learning Policy Representations in Multiagent Systems.tei.xml',\n",
       " 'Learning to Map Context-Dependent Sentences to Executable Formal Queries.tei.xml',\n",
       " 'Multimodal Emoji Prediction.tei.xml',\n",
       " 'Truth of Varying Shades_ Analyzing Language in Fake News and Political Fact-Checking.tei.xml',\n",
       " 'GSOS_ Gauss-Seidel Operator Splitting Algorithm for  Multi-Term Nonsmooth Convex Composite Optimization.tei.xml',\n",
       " 'An Estimation and Analysis Framework for the Rasch Model.tei.xml',\n",
       " 'Deep Pyramid Convolutional Neural Networks for Text Categorization.tei.xml',\n",
       " 'Essentially No Barriers in Neural Network Energy Landscape.tei.xml',\n",
       " 'Improved nearest neighbor search using auxiliary information and priority functions.tei.xml',\n",
       " 'A Multi-Axis Annotation Scheme for Event Temporal Relations.tei.xml',\n",
       " 'Classical Structured Prediction Losses for Sequence to Sequence Learning.tei.xml',\n",
       " 'Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles.tei.xml',\n",
       " 'Deeper Attention to Abusive User Content Moderation.tei.xml',\n",
       " 'Unsupervised Acquisition of Comprehensive Multiword Lexicons using Competition in an n-gram Lattice.tei.xml',\n",
       " 'Identifying Semantic Divergences in Parallel Text without Annotations.tei.xml',\n",
       " 'Learning Neural Representation for CLIR with Adversarial Framework.tei.xml',\n",
       " 'Neural Message Passing for Quantum Chemistry.tei.xml',\n",
       " 'Dual Iterative Hard Thresholding_ From Non-convex Sparse Minimization to Non-smooth Concave Maximization.tei.xml',\n",
       " 'Cross-Lingual Semantic Similarity of Words as the Similarity of Their Semantic Word Responses.tei.xml',\n",
       " 'Reducing Gender Bias in Abusive Language Detection.tei.xml',\n",
       " 'Stochastic Modified Equations  and Adaptive Stochastic Gradient Algorithms.tei.xml',\n",
       " 'iSurvive_ An Interpretable, Event-time Prediction Model for mHealth.tei.xml',\n",
       " 'The Alexa Meaning Representation Language.tei.xml',\n",
       " 'Using Out-of-Domain Data for Lexical Addressee Detection in Human-Human-Computer Dialog.tei.xml',\n",
       " 'Did the Model Understand the Question_.tei.xml',\n",
       " 'Rumor Detection on Twitter with Tree-structured Recursive Neural Networks.tei.xml',\n",
       " 'Learning from Clinical Judgments_ Semi-Markov-Modulated Marked  Hawkes Processes for Risk Prognosis.tei.xml',\n",
       " 'Bayesian Model Selection for Change Point Detection and Clustering.tei.xml',\n",
       " 'A Stylometric Inquiry into Hyperpartisan and Fake News.tei.xml',\n",
       " 'The Dynamics of Learning_ A Random Matrix Approach.tei.xml',\n",
       " 'Learning Lexico-Functional Patterns for First-Person Affect.tei.xml',\n",
       " 'Multichannel End-to-end Speech Recognition.tei.xml',\n",
       " 'Deep Relevance Ranking Using Enhanced Document-Query Interactions.tei.xml',\n",
       " 'Joint Learning for Emotion Classification and Emotion Cause Detection.tei.xml',\n",
       " 'Know-Evolve_ Deep Temporal Reasoning for Dynamic Knowledge Graphs.tei.xml',\n",
       " 'Evaluation Metrics for Machine Reading Comprehension_ Prerequisite Skills and Readability.tei.xml',\n",
       " 'Object Ordering with Bidirectional Matchings for Visual Reasoning.tei.xml',\n",
       " 'SwitchOut_ an Efficient Data Augmentation Algorithm for Neural Machine Translation.tei.xml',\n",
       " 'Human-in-the-Loop Parsing.tei.xml',\n",
       " 'Reasoning with Sarcasm by Reading In-between.tei.xml',\n",
       " 'Learning Structured Natural Language Representations for Semantic Parsing.tei.xml',\n",
       " 'Adversarial Training for Multi-task and Multi-lingual Joint Modeling of Utterance Intent Classification.tei.xml',\n",
       " 'Practical Contextual Bandits with Regression Oracles.tei.xml',\n",
       " 'Tensor Belief Propagation.tei.xml',\n",
       " 'Deletion-Robust Submodular Maximization_ Data Summarization with ``the Right to be Forgotten_.tei.xml',\n",
       " 'Generating Contrastive Referring Expressions.tei.xml',\n",
       " 'A Stochastic Decoder for Neural Machine Translation.tei.xml',\n",
       " 'Solving General Arithmetic Word Problems.tei.xml',\n",
       " 'Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization.tei.xml',\n",
       " 'Stochastic Top-k ListNet.tei.xml',\n",
       " 'Automatic Recognition of Conversational Strategies in the Service of a Socially-Aware Dialog System.tei.xml',\n",
       " 'Neural Transductive Learning and Beyond_ Morphological Generation in the Minimal-Resource Setting.tei.xml',\n",
       " 'A Novel Approach to Part Name Discovery in Noisy Text.tei.xml',\n",
       " 'Deep Attentive Sentence Ordering Network.tei.xml',\n",
       " 'SQL-to-Text Generation with Graph-to-Sequence Model.tei.xml',\n",
       " 'Joint Semantic Synthesis and Morphological Analysis of the Derived Word.tei.xml',\n",
       " 'Online Convolutional Sparse Coding with Sample-Dependent Dictionary.tei.xml',\n",
       " 'On the Power of Over-parametrization in Neural Networks with Quadratic Activation.tei.xml',\n",
       " 'Personalized Review Generation by Expanding Phrases and Attending on Aspect-Aware Representations.tei.xml',\n",
       " 'Parallel Multiscale Autoregressive Density Estimation.tei.xml',\n",
       " 'Density-Driven Cross-Lingual Transfer of Dependency Parsers.tei.xml',\n",
       " 'Bootstrapping Generators from Noisy Data.tei.xml',\n",
       " 'Language Generation via DAG Transduction.tei.xml',\n",
       " 'Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders.tei.xml',\n",
       " 'Signal and Noise Statistics Oblivious Orthogonal Matching Pursuit .tei.xml',\n",
       " 'Distributed and Provably Good Seedings for k-Means in Constant Rounds.tei.xml',\n",
       " 'Cross-Target Stance Classification with Self-Attention Networks.tei.xml',\n",
       " 'PACRR_ A Position-Aware Neural IR Model for Relevance Matching.tei.xml',\n",
       " 'Globally Coherent Text Generation with Neural Checklist Models.tei.xml',\n",
       " 'Neural Taylor Approximations_  Convergence and Exploration in Rectifier Networks.tei.xml',\n",
       " 'Pragmatically Informative Image Captioning with Character-Level Inference.tei.xml',\n",
       " 'Sparse and Constrained Attention for Neural Machine Translation.tei.xml',\n",
       " 'Large-scale Analysis of Counseling Conversations_ An Application of Natural Language Processing to Mental Health.tei.xml',\n",
       " 'Target-Sensitive Memory Networks for Aspect Sentiment Classification.tei.xml',\n",
       " 'Compact, Efficient and Unlimited Capacity_ Language Modeling with Compressed Suffix Trees.tei.xml',\n",
       " 'Fast and Sample Efficient Inductive Matrix Completion via Multi-Phase Procrustes Flow.tei.xml',\n",
       " 'Unifying Task Specification in Reinforcement Learning.tei.xml',\n",
       " 'A Question Answering Approach to Emotion Cause Extraction.tei.xml',\n",
       " 'Prediction under Uncertainty in Sparse Spectrum Gaussian Processes  with Applications to Filtering and Control.tei.xml',\n",
       " 'Ranking Distributions based on Noisy Sorting.tei.xml',\n",
       " 'An Unsupervised Probability Model for Speech-to-Translation Alignment of Low-Resource Languages.tei.xml',\n",
       " 'ProtoNN_ Compressed and Accurate kNN for Resource-scarce Devices.tei.xml',\n",
       " 'Contextual Decision Processes with low Bellman rank are PAC-Learnable.tei.xml',\n",
       " 'Feasible Arm Identification.tei.xml',\n",
       " 'Schema Networks_ Zero-shot Transfer with a Generative Causal Model of Intuitive Physics.tei.xml',\n",
       " 'Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval.tei.xml',\n",
       " 'Multi-lingual neural title generation for e-Commerce browse pages.tei.xml',\n",
       " 'Loss Decomposition for Fast Learning in Large Output Spaces.tei.xml',\n",
       " 'Joint prediction in MST-style discourse parsing for argumentation mining.tei.xml',\n",
       " 'Working Memory Networks_ Augmenting Memory Networks with a Relational Reasoning Module.tei.xml',\n",
       " 'Uniform Convergence Rates for Kernel Density Estimation.tei.xml',\n",
       " 'Recognizing Implicit Discourse Relations via Repeated Reading_ Neural Networks with Multi-Level Attention.tei.xml',\n",
       " 'Probabilistic Recurrent State-Space Models.tei.xml',\n",
       " 'Learning multiview embeddings for assessing dementia.tei.xml',\n",
       " 'Who did What_ A Large-Scale Person-Centered Cloze Dataset.tei.xml',\n",
       " 'Yes, but Did It Work_ Evaluating Variational Inference.tei.xml',\n",
       " 'Residual Unfairness in Fair Machine Learning from Prejudiced Data.tei.xml',\n",
       " 'Knowledge Graph Embedding with Hierarchical Relation Structure.tei.xml',\n",
       " 'Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection.tei.xml',\n",
       " 'Learning Discrete Representations via Information Maximizing Self-Augmented Training.tei.xml',\n",
       " 'Comparing Computational Cognitive Models of Generalization in a Language Acquisition Task.tei.xml',\n",
       " 'Device Placement Optimization with Reinforcement Learning.tei.xml',\n",
       " 'Neural End-to-End Learning for Computational Argumentation Mining.tei.xml',\n",
       " 'Understanding Generalization and Optimization Performance of Deep CNNs.tei.xml',\n",
       " 'Learning Word Representations with Cross-Sentence Dependency for End-to-End Co-reference Resolution.tei.xml',\n",
       " 'Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces.tei.xml',\n",
       " 'Stochastic PCA with `2 and `1 Regularization.tei.xml',\n",
       " 'CRAFTML, an Efficient Clustering-based Random  Forest for Extreme Multi-label Learning .tei.xml',\n",
       " 'Gradient Coding_ Avoiding Stragglers in Distributed Learning.tei.xml',\n",
       " 'High Dimensional Bayesian Optimization with Elastic Gaussian Process.tei.xml',\n",
       " 'Out-of-sample extension of graph adjacency spectral embedding.tei.xml',\n",
       " 'Human-Machine Dialogue as a Stochastic Game.tei.xml',\n",
       " 'Identifying Domain Independent Update Intents in Task Based Dialogs.tei.xml',\n",
       " 'Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation.tei.xml',\n",
       " 'Unsupervised Bilingual Lexicon Induction via Latent Variable Models.tei.xml',\n",
       " 'Reasoning about Pragmatics with Neural Listeners and Speakers.tei.xml',\n",
       " 'MentorNet_ Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels.tei.xml',\n",
       " 'Proportional Allocation_  Simple, Distributed, and Diverse Matching with High Entropy .tei.xml',\n",
       " 'Privacy-preserving Neural Representations of Text.tei.xml',\n",
       " 'A Neural Layered Model for Nested Named Entity Recognition.tei.xml',\n",
       " 'Controlling Output Length in Neural Encoder-Decoders.tei.xml',\n",
       " 'Translating Neuralese.tei.xml',\n",
       " 'Identifying Political Sentiment between Nation States with Social Media.tei.xml',\n",
       " 'Probably Approximately Metric-Fair Learning.tei.xml',\n",
       " 'Before Name-calling_ Dynamics and Triggers of Ad Hominem Fallacies in Web Argumentation.tei.xml',\n",
       " 'An Infinite Hidden Markov Model With Similarity-Biased Transitions.tei.xml',\n",
       " 'Tandem Anchoring_ a Multiword Anchor Approach for Interactive Topic Modeling.tei.xml',\n",
       " 'Document Modeling with Gated Recurrent Neural Network for Sentiment Classification.tei.xml',\n",
       " 'Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement.tei.xml',\n",
       " 'Rates of Convergence of Spectral Methods for Graphon Estimation.tei.xml',\n",
       " 'Counterfactual Data-Fusion for Online Reinforcement Learners.tei.xml',\n",
       " 'Learning Maximum-A-Posteriori Perturbation Models for Structured Prediction in Polynomial Time.tei.xml',\n",
       " 'Dependency-Based Word Embeddings.tei.xml',\n",
       " 'Exploiting Deep Representations for Neural Machine Translation.tei.xml',\n",
       " 'A Multi-sentiment-resource Enhanced Attention Network for Sentiment Classification.tei.xml',\n",
       " 'Deep Tensor Convolution on Multicores.tei.xml',\n",
       " 'Unsupervised Cross-lingual Transfer of Word Embedding Spaces.tei.xml',\n",
       " 'Structured Output Learning with Abstention_ Application to Accurate Opinion Prediction.tei.xml',\n",
       " 'Supervised Attentions for Neural Machine Translation.tei.xml',\n",
       " 'Addressee and Response Selection for Multi-Party Conversation.tei.xml',\n",
       " 'Aspect-augmented Adversarial Networks for Domain Adaptation.tei.xml',\n",
       " 'Joint Event Trigger Identification and Event Coreference Resolution with Structured Perceptron.tei.xml',\n",
       " 'Beyond Filters_ Compact Feature Map for Portable Deep Model.tei.xml',\n",
       " 'Large Margin Neural Language Model.tei.xml',\n",
       " 'Weakly Consistent Optimal Pricing Algorithms in Repeated Posted-Price Auctions with Strategic Buyer.tei.xml',\n",
       " 'Not All Dialogues are Created Equal_ Instance Weighting for Neural Conversational Models.tei.xml',\n",
       " 'Dictionary Learning Based on Sparse Distribution Tomography.tei.xml',\n",
       " 'An Adaptive Test of Independence with Analytic Kernel Embeddings.tei.xml',\n",
       " 'Exploring Neural Text Simplification Models.tei.xml',\n",
       " 'Context-dependent Semantic Parsing for Time Expressions.tei.xml',\n",
       " 'Learning to Discover Sparse Graphical Models.tei.xml',\n",
       " 'Resource-efficient Machine Learning in 2 KB RAM for the Internet of Things.tei.xml',\n",
       " 'Joint Learning for Event Coreference Resolution.tei.xml',\n",
       " 'Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum.tei.xml',\n",
       " 'Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment.tei.xml',\n",
       " 'Accurate Supervised and Semi-Supervised Machine Reading for Long Documents.tei.xml',\n",
       " 'Parseval Networks_ Improving Robustness to Adversarial Examples.tei.xml',\n",
       " 'On the Practical Computational Power of Finite Precision RNNs for Language Recognition.tei.xml',\n",
       " 'Stochastic Proximal Algorithms for AUC Maximization.tei.xml',\n",
       " 'Backpropagating through Structured Argmax using a SPIGOT.tei.xml',\n",
       " 'Self-Paced Co-training.tei.xml',\n",
       " 'Pieces of Eight_ 8-bit Neural Machine Translation.tei.xml',\n",
       " 'Twitter Universal Dependency Parsing for African-American and Mainstream American English.tei.xml',\n",
       " 'Adversarially Regularized Autoencoders.tei.xml',\n",
       " 'Improving Topic Models with Latent Feature Word Representations.tei.xml',\n",
       " 'Hearst Patterns Revisited_ Automatic Hypernym Detection from Large Text Corpora.tei.xml',\n",
       " 'A Dataset for Telling the Stories of Social Media Videos.tei.xml',\n",
       " 'Learning Localized Spatio-Temporal Models From Streaming Data.tei.xml',\n",
       " 'Measuring Sample Quality with Kernels.tei.xml',\n",
       " 'Strong Baselines for Simple Question Answering over Knowledge Graphs with and without Neural Networks.tei.xml',\n",
       " 'Unsupervised Statistical Machine Translation.tei.xml',\n",
       " 'Revealing Common Statistical Behaviors in Heterogeneous Populations.tei.xml',\n",
       " 'Sequence Tutor_ Conservative Fine-Tuning of Sequence Generation Models with KL-control.tei.xml',\n",
       " 'The Galactic Dependencies Treebanks_ Getting More Data by Synthesizing New Languages.tei.xml',\n",
       " 'Evaluating Visual Representations for Topic Understanding and Their Effects on Manually Generated Topic Labels.tei.xml',\n",
       " 'SafeCity_ Understanding Diverse Forms of Sexual Harassment Personal Stories.tei.xml',\n",
       " 'What Action Causes This_ Towards Naive Physical Action-Effect Prediction.tei.xml',\n",
       " 'Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors.tei.xml',\n",
       " 'The glass ceiling in NLP.tei.xml',\n",
       " 'Generating Fine-Grained Open Vocabulary Entity Type Descriptions.tei.xml',\n",
       " 'On the Challenges of Translating NLP Research into Commercial Products.tei.xml',\n",
       " 'SimpleQuestions Nearly Solved_ A New Upperbound and Baseline Approach.tei.xml',\n",
       " 'Neural Episodic Control.tei.xml',\n",
       " 'Bucket Renormalization for Approximate Inference.tei.xml',\n",
       " 'Neural Network based Extreme Classification and Similarity Models for Product Matching.tei.xml',\n",
       " 'Risk Bounds for Transferring Representations With and Without Fine-Tuning.tei.xml',\n",
       " 'Optimal Rates of Sketched-regularized Algorithms for Least-Squares Regression over Hilbert Spaces.tei.xml',\n",
       " 'Coarse-to-Fine Decoding for Neural Semantic Parsing.tei.xml',\n",
       " 'Representations of language in a model of visually grounded speech signal.tei.xml',\n",
       " 'EmoNet_ Fine-Grained Emotion Detection with Gated Recurrent Neural Networks.tei.xml',\n",
       " 'Neural Word Segmentation with Rich Pretraining.tei.xml',\n",
       " 'Deep Temporal-Recurrent-Replicated-Softmax for Topical Trends over Time.tei.xml',\n",
       " 'Universal Neural Machine Translation for Extremely Low Resource Languages.tei.xml',\n",
       " 'Semi-Supervised Classification Based on Classification from Positive and Unlabeled Data.tei.xml',\n",
       " 'Convexified Convolutional Neural Networks.tei.xml',\n",
       " 'FewRel_ A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation.tei.xml',\n",
       " 'Distributed Mean Estimation with Limited Communication.tei.xml',\n",
       " 'From Language to Programs_ Bridging Reinforcement Learning and Maximum Marginal Likelihood.tei.xml',\n",
       " 'Re-revisiting Learning on Hypergraphs_  Confidence Interval and Subgradient Method.tei.xml',\n",
       " 'TWOWINGOS_ A Two-Wing Optimization Strategy for Evidential Claim Verification.tei.xml',\n",
       " 'Efficient Methods for Incorporating Knowledge into Topic Models.tei.xml',\n",
       " 'Understanding Satirical Articles Using Common-Sense.tei.xml',\n",
       " 'Bandits with Delayed, Aggregated Anonymous Feedback.tei.xml',\n",
       " 'Fine-grained Opinion Mining with Recurrent Neural Networks and Word Embeddings.tei.xml',\n",
       " 'Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization.tei.xml',\n",
       " 'Compact Personalized Models for Neural Machine Translation.tei.xml',\n",
       " 'Inter and Intra Topic Structure Learning with Word Embeddings.tei.xml',\n",
       " 'How to Memorize a Random 60-Bit String.tei.xml',\n",
       " 'Learning Sequence Encoders for Temporal Knowledge Graph Completion.tei.xml',\n",
       " 'Fast(er) Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set.tei.xml',\n",
       " 'Learning word-like units from joint audio-visual analysis.tei.xml',\n",
       " 'The Loss Surface of Deep and Wide Neural Networks.tei.xml',\n",
       " 'Sparse Non-negative Matrix Language Modeling.tei.xml',\n",
       " 'Simple and Effective Multi-Paragraph Reading Comprehension.tei.xml',\n",
       " 'Learning to Explain_ An Information-Theoretic Perspective  on Model Interpretation.tei.xml',\n",
       " 'Parsing with Traces_ An O(n) Algorithm and a Structural Representation.tei.xml',\n",
       " 'Measuring abstract reasoning in neural networks.tei.xml',\n",
       " 'Noising and Denoising Natural Language_ Diverse Backtranslation for Grammar Correction.tei.xml',\n",
       " 'TDNN_ A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring.tei.xml',\n",
       " 'Automatic Estimation of Simultaneous Interpreter Performance.tei.xml',\n",
       " 'Attention-based Deep Multiple Instance Learning.tei.xml',\n",
       " 'Hierarchical Structured Model for Fine-to-coarse Manifesto Text Analysis.tei.xml',\n",
       " 'FOIL it! Find One mismatch between Image and Language caption.tei.xml',\n",
       " 'Creating Training Corpora for NLG Micro-Planning.tei.xml',\n",
       " 'Sequence-to-Dependency Neural Machine Translation.tei.xml',\n",
       " 'Dimensionality-Driven Learning with Noisy Labels.tei.xml',\n",
       " 'Scalable Bayesian Rule Lists.tei.xml',\n",
       " 'Collect at Once, Use Effectively_ Making Non-interactive Locally Private Learning Possible.tei.xml',\n",
       " 'Understanding Language Preference for Expression of Opinion and Sentiment_ What do Hindi-English Speakers do on Twitter_.tei.xml',\n",
       " 'End-to-end Active Object Tracking via Reinforcement Learning.tei.xml',\n",
       " 'Extreme Learning to Rank via Low Rank Assumption.tei.xml',\n",
       " 'Augmented CycleGAN_ Learning Many-to-Many Mappings  from Unpaired Data.tei.xml',\n",
       " 'Unifying Text, Metadata, and User Network Representations with a Neural Network for Geolocation Prediction.tei.xml',\n",
       " 'An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis.tei.xml',\n",
       " 'Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling.tei.xml',\n",
       " 'Bayesian Modeling of Lexical Resources for Low-Resource Settings.tei.xml',\n",
       " 'Gradually Updated Neural Networks for Large-Scale Image Recognition.tei.xml',\n",
       " 'What Makes Reading Comprehension Questions Easier_.tei.xml',\n",
       " 'Joint Learning for Targeted Sentiment Analysis.tei.xml',\n",
       " 'Deep Multi-Task Learning for Aspect Term Extraction with Memory Interaction.tei.xml',\n",
       " 'Identifying Best Interventions through Online Importance Sampling.tei.xml',\n",
       " 'Attention-over-Attention Neural Networks for Reading Comprehension.tei.xml',\n",
       " 'Modeling Naive Psychology of Characters in Simple Commonsense Stories.tei.xml',\n",
       " 'Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning.tei.xml',\n",
       " 'Deep Voice_ Real-time Neural Text-to-Speech.tei.xml',\n",
       " 'Language-Guided Adaptive Perception for Efficient Grounded Communication with Robotic Manipulators in Cluttered Environments.tei.xml',\n",
       " 'Position-aware Attention and Supervised Data Improve Slot Filling.tei.xml',\n",
       " 'Variable Typing_ Assigning Meaning to Variables in Mathematical Text.tei.xml',\n",
       " 'Two Methods for Domain Adaptation of Bilingual Tasks_ Delightfully Simple and Broadly Applicable.tei.xml',\n",
       " 'Stochastic Adaptive Quasi-Newton Methods for Minimizing Expected Values.tei.xml',\n",
       " 'The Price of Differential Privacy for Online Learning.tei.xml',\n",
       " 'Differentially Private Database Release via Kernel Mean Embeddings.tei.xml',\n",
       " 'Autoencoder as Assistant Supervisor_ Improving Text Representation for Chinese Social Media Text Summarization.tei.xml',\n",
       " 'Defoiling Foiled Image Captions.tei.xml',\n",
       " 'Generating Topical Poetry.tei.xml',\n",
       " 'Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning.tei.xml',\n",
       " 'Dynamic Word Embeddings.tei.xml',\n",
       " 'Neural Program Synthesis from Diverse Demonstration Videos.tei.xml',\n",
       " 'Adversarial Example Generation with Syntactically Controlled Paraphrase Networks.tei.xml',\n",
       " 'A Structured Syntax-Semantics Interface for English-AMR Alignment.tei.xml',\n",
       " 'Fast Approximate Spectral Clustering for Dynamic Networks.tei.xml',\n",
       " 'Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression.tei.xml',\n",
       " 'Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging.tei.xml',\n",
       " 'Leveraging Well-Conditioned Bases_ Streaming and Distributed Summaries in Minkowski p-Norms.tei.xml',\n",
       " 'Generating Sentences by Editing Prototypes.tei.xml',\n",
       " 'Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources.tei.xml',\n",
       " 'Faster Principal Component Regression  and Stable Matrix Chebyshev Approximation.tei.xml',\n",
       " 'Exemplar Encoder-Decoder for Neural Conversation Generation.tei.xml',\n",
       " 'Improving Regression Performance with Distributional Losses.tei.xml',\n",
       " 'Effective Approaches to Attention-based Neural Machine Translation.tei.xml',\n",
       " 'Strong Baselines for Neural Semi-Supervised Learning under Domain Shift.tei.xml',\n",
       " 'Thompson Sampling for Combinatorial Semi-Bandits.tei.xml',\n",
       " 'Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs.tei.xml',\n",
       " 'Towards Debate Automation_ a Recurrent Model for Predicting Debate Winners.tei.xml',\n",
       " 'Asynchronous Stochastic Gradient Descent with Delay Compensation.tei.xml',\n",
       " 'Learning to Generate Long-term Future via Hierarchical Prediction.tei.xml',\n",
       " 'Joint Modeling of Topics, Citations, and Topical Authority in Academic Corpora.tei.xml',\n",
       " 'Predicate Argument Alignment using a Global Coherence Model.tei.xml',\n",
       " 'Antecedent Selection for Sluicing_ Structure and Content.tei.xml',\n",
       " 'Multimodal Language Analysis in the Wild_ CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph.tei.xml',\n",
       " 'Continuous-Time Flows for Efficient Inference and Density Estimation.tei.xml',\n",
       " 'Neural Segmental Hypergraphs for Overlapping Mention Recognition.tei.xml',\n",
       " 'How Much Information Does a Human Translator Add to the Original_.tei.xml',\n",
       " 'Semantic Role Labeling for Learner Chinese_ the Importance of Syntactic Parsing and L2-L1 Parallel Data.tei.xml',\n",
       " 'Improving Abstraction in Text Summarization.tei.xml',\n",
       " 'Whodunnit_ Crime Drama as a Case for Natural Language Understanding.tei.xml',\n",
       " 'Algebraic Variety Models for High-Rank Matrix Completion.tei.xml',\n",
       " 'Topological Mixture Estimation.tei.xml',\n",
       " 'meProp_ Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting.tei.xml',\n",
       " 'Estimating the unseen from multiple populations.tei.xml',\n",
       " 'Nonparanormal Information Estimation.tei.xml',\n",
       " 'Key-Value Memory Networks for Directly Reading Documents.tei.xml',\n",
       " 'Citation Resolution_ A method for evaluating context-based citation recommendation systems.tei.xml',\n",
       " 'Continual Learning Through Synaptic Intelligence.tei.xml',\n",
       " 'Unsupervised Neural Machine Translation with Weight Sharing.tei.xml',\n",
       " 'Learning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems.tei.xml',\n",
       " 'Fast Bayesian Intensity Estimation for the Permanental Process.tei.xml',\n",
       " 'To Understand Deep Learning We Need to Understand Kernel Learning.tei.xml',\n",
       " 'Uniform Deviation Bounds for k-Means Clustering.tei.xml',\n",
       " 'Structured Multi-Label Biomedical Text Tagging via Attentive Neural Tree Decoding.tei.xml',\n",
       " 'Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification.tei.xml',\n",
       " 'Style Transfer Through Back-Translation.tei.xml',\n",
       " 'Deep Reinforcement Learning for Dialogue Generation.tei.xml',\n",
       " 'Unified Pragmatic Models for Generating and Following Instructions.tei.xml',\n",
       " 'Scalable Generative Models for Multi-label Learning with Missing Labels.tei.xml',\n",
       " 'Gradient descent with identity initialization  efficiently learns positive definite linear transformations  by deep residual networks .tei.xml',\n",
       " 'Sequence to Better Sequence_ Continuous Revision of Combinatorial Structures.tei.xml',\n",
       " 'Which Melbourne_ Augmenting Geocoding with Maps.tei.xml',\n",
       " 'A Neural Attention Model for Sentence Summarization.tei.xml',\n",
       " 'A Two-Stage Parsing Method for Text-Level Discourse Analysis.tei.xml',\n",
       " 'Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces.tei.xml',\n",
       " 'A Neural Architecture for Automated ICD Coding.tei.xml',\n",
       " 'Lost Relatives of the Gumbel Trick.tei.xml',\n",
       " 'A Closer Look at Memorization in Deep Networks.tei.xml',\n",
       " 'Fast Maximization of Non-Submodular, Monotonic Functions on the Integer Lattice.tei.xml',\n",
       " 'A Study of Reinforcement Learning for Neural Machine Translation.tei.xml',\n",
       " 'Learning Joint Semantic Parsers from Disjoint Data.tei.xml',\n",
       " 'Tensor Balancing on Statistical Manifold.tei.xml',\n",
       " 'Learning to Parse and Translate Improves Neural Machine Translation.tei.xml',\n",
       " 'Improving Multi-Modal Representations Using Image Dispersion_ Why Less is Sometimes More.tei.xml',\n",
       " 'Adversarial Feature Matching for Text Generation.tei.xml',\n",
       " 'Neural Response Generation via GAN with an Approximate Embedding Layer.tei.xml',\n",
       " 'Fairness Without Demographics in Repeated Loss Minimization.tei.xml',\n",
       " 'Structured Variational Learning of Bayesian Neural Networks with Horseshoe Priors.tei.xml',\n",
       " 'Graph-Based Seed Set Expansion for Relation Extraction Using Random Walk Hitting Times.tei.xml',\n",
       " 'Accurate Uncertainties for Deep Learning Using Calibrated Regression.tei.xml',\n",
       " 'Do we need bigram alignment models_ On the effect of alignment quality on transduction accuracy in G2P.tei.xml',\n",
       " 'Online and Linear-Time Attention by Enforcing Monotonic Alignments.tei.xml',\n",
       " 'A Meaning-based Statistical English Math Word Problem Solver.tei.xml',\n",
       " 'Covariate Adjusted Precision Matrix Estimation via Nonconvex Optimization.tei.xml',\n",
       " 'Optimization Landscape and Expressivity of Deep CNNs.tei.xml',\n",
       " 'On Approximation Guarantees for Greedy Low Rank Optimization.tei.xml',\n",
       " 'Opinion Recommendation Using A Neural Model.tei.xml',\n",
       " 'Magnetic Hamiltonian Monte Carlo.tei.xml',\n",
       " 'An AMR Aligner Tuned by Transition-based Parser.tei.xml',\n",
       " 'A Visual Attention Grounding Neural Model for Multimodal Machine Translation.tei.xml',\n",
       " 'Mimicking Word Embeddings using Subword RNNs.tei.xml',\n",
       " 'Insertion Position Selection Model for Flexible Non-Terminals in Dependency Tree-to-Tree Machine Translation.tei.xml',\n",
       " 'An Analysis of Action Recognition Datasets for Language and Vision Tasks.tei.xml',\n",
       " 'Latent LSTM Allocation  Joint Clustering and Non-Linear Dynamic Modeling of Sequential Data.tei.xml',\n",
       " 'Embedding Multimodal Relational Data for Knowledge Base Completion.tei.xml',\n",
       " 'Modelling and Optimizing on Syntactic N-Grams for Statistical Machine Translation.tei.xml',\n",
       " 'Scalable Deletion-Robust Submodular Maximization_ Data Summarization with Privacy and Fairness Constraints.tei.xml',\n",
       " 'Overcoming Language Variation in Sentiment Analysis with Social Attention.tei.xml',\n",
       " 'Adaptive Sampling Probabilities for Non-Smooth Optimization.tei.xml',\n",
       " 'Reference Resolution in Situated Dialogue with Learned Semantics.tei.xml',\n",
       " 'Topic Memory Networks for Short Text Classification.tei.xml',\n",
       " 'Word Embedding and WordNet Based Metaphor Identification and Interpretation.tei.xml',\n",
       " 'The Impact of Modeling Overall Argumentation with Tree Kernels.tei.xml',\n",
       " 'Straight to the Tree_ Constituency Parsing with Neural Syntactic Distance.tei.xml',\n",
       " 'Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention.tei.xml',\n",
       " 'Local-to-Global Bayesian Network Structure Learning.tei.xml',\n",
       " 'Sequence-to-sequence Models for Cache Transition Systems.tei.xml',\n",
       " 'Deciding How to Decide_  Dynamic Routing in Artificial Neural Networks.tei.xml',\n",
       " 'Learning Determinantal Point Processes with Moments and Cycles.tei.xml',\n",
       " 'Looking for structure in lexical and acoustic-prosodic entrainment behaviors.tei.xml',\n",
       " 'Language Modeling with Gated Convolutional Networks.tei.xml',\n",
       " 'A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings.tei.xml',\n",
       " 'Data-Efficient Policy Evaluation Through Behavior Policy Search.tei.xml',\n",
       " 'Implicit Regularization in Nonconvex Statistical Estimation_ Gradient Descent Converges Linearly for Phase Retrieval and Matrix Completion.tei.xml',\n",
       " 'Optimal and Adaptive Off-policy Evaluation in Contextual Bandits.tei.xml',\n",
       " 'Domain Attention with an Ensemble of Experts.tei.xml',\n",
       " 'Co-clustering through Optimal Transport.tei.xml',\n",
       " 'Differentially Private Identity and Equivalence Testing of Discrete Distributions.tei.xml',\n",
       " 'DVAE++_ Discrete Variational Autoencoders with Overlapping Transformations.tei.xml',\n",
       " 'Compiling Combinatorial Prediction Games.tei.xml',\n",
       " 'Autoregressive Quantile Networks for Generative Modeling.tei.xml',\n",
       " 'Multi-Relational Question Answering from Narratives_ Machine Reading and Reasoning in Simulated Worlds.tei.xml',\n",
       " 'Doubly Greedy Primal-Dual Coordinate Descent for Sparse Empirical Risk Minimization.tei.xml',\n",
       " 'Scientific Article Summarization Using Citation-Context and Article_s Discourse Structure.tei.xml',\n",
       " 'ConStance_ Modeling Annotation Contexts to Improve Stance Classification.tei.xml',\n",
       " 'A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding.tei.xml',\n",
       " 'DialSQL_ Dialogue Based Structured Query Generation.tei.xml',\n",
       " 'Training Structured Prediction Energy Networks with Indirect Supervision.tei.xml',\n",
       " 'Harvesting Paragraph-Level Question-Answer Pairs from Wikipedia.tei.xml',\n",
       " 'Revisiting the Importance of Encoding Logic Rules in Sentiment Classification.tei.xml',\n",
       " 'Large-scale Semantic Parsing without Question-Answer Pairs.tei.xml',\n",
       " 'Semi-Implicit Variational Inference.tei.xml',\n",
       " 'Tight Regret Bounds for Bayesian Optimization in One Dimension.tei.xml',\n",
       " 'Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions.tei.xml',\n",
       " 'Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering.tei.xml',\n",
       " 'Using pseudo-senses for improving the extraction of synonyms from word embeddings.tei.xml',\n",
       " 'Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning.tei.xml',\n",
       " 'The Lazy Encoder_ A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation.tei.xml',\n",
       " 'Generating Syntactic Paraphrases.tei.xml',\n",
       " 'A Skeleton-Based Model for Promoting Coherence Among Sentences in Narrative Story Generation.tei.xml',\n",
       " 'Relations such as Hypernymy_ Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment.tei.xml',\n",
       " 'A Spectral Approach to Gradient Estimation for Implicit Distributions.tei.xml',\n",
       " 'Improved Variational Autoencoders for Text Modeling using Dilated Convolutions.tei.xml',\n",
       " 'Equivalence of Multicategory SVM and Simplex Cone SVM_  Fast Computations and Statistical Theory.tei.xml',\n",
       " 'Multimodal Frame Identification with Multilingual Evaluation.tei.xml',\n",
       " 'Cognitive Psychology for Deep Neural Networks_  A Shape Bias Case Study .tei.xml',\n",
       " 'Hierarchical Deep Generative Models for Multi-Rate Multivariate Time Series.tei.xml',\n",
       " 'Unsupervised Learning of Morphological Forests.tei.xml',\n",
       " 'Variational Sequential Labelers for Semi-Supervised Learning.tei.xml',\n",
       " 'Large-Scale QA-SRL Parsing.tei.xml',\n",
       " 'Bi-directional Attention with Agreement for Dependency Parsing.tei.xml',\n",
       " 'Optimal Data Set Selection_ An Application to Grapheme-to-Phoneme Conversion.tei.xml',\n",
       " 'Tropical Geometry of Deep Neural Networks.tei.xml',\n",
       " 'A Corpus of Natural Language for Visual Reasoning.tei.xml',\n",
       " 'Approximate Steepest Coordinate Descent.tei.xml',\n",
       " 'Learning Latent Space Models with Angular Constraints.tei.xml',\n",
       " 'A Tree-based Decoder for Neural Machine Translation.tei.xml',\n",
       " 'Learning Unsupervised Word Translations Without Adversaries.tei.xml',\n",
       " 'No Metrics Are Perfect_ Adversarial Reward Learning for Visual Storytelling.tei.xml',\n",
       " 'Toward Zero-shot Entity Recognition in Task-oriented Conversational Agents.tei.xml',\n",
       " 'Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog.tei.xml',\n",
       " 'Mixed batches and symmetric discriminators for GAN training.tei.xml',\n",
       " 'FeUdal Networks for Hierarchical Reinforcement Learning.tei.xml',\n",
       " 'A Semismooth Newton Method for Fast, Generic Convex Programming.tei.xml',\n",
       " 'Learning Infinite Layer Networks Without the Kernel Trick.tei.xml',\n",
       " 'Active Learning with Logged Data.tei.xml',\n",
       " 'Evaluating Spoken Dialogue Processing for Time-Offset Interaction.tei.xml',\n",
       " 'Evaluating Bayesian Models with Posterior Dispersion Indices.tei.xml',\n",
       " 'Heterogeneous Supervision for Relation Extraction_ A Representation Learning Approach.tei.xml',\n",
       " 'Supervised Domain Enablement Attention for Personalized Domain Classification.tei.xml',\n",
       " 'The SENSEI Annotated Corpus_ Human Summaries of Reader Comment Conversations in On-line News.tei.xml',\n",
       " 'Context-Dependent Sentiment Analysis in User-Generated Videos.tei.xml',\n",
       " 'Dynamical Isometry and a Mean Field Theory of RNNs_ Gating Enables Signal Propagation in Recurrent Neural Networks.tei.xml',\n",
       " 'Vancouver Welcomes You! Minimalist Location Metonymy Resolution.tei.xml',\n",
       " 'Grasping the Finer Point_ A Supervised Similarity Network for Metaphor Detection.tei.xml',\n",
       " 'AllSummarizer system at MultiLing 2015_ Multilingual single and multi-document summarization.tei.xml',\n",
       " 'Neural AMR_ Sequence-to-Sequence Models for Parsing and Generation.tei.xml',\n",
       " 'Multi-Class Optimal Margin Distribution Machine.tei.xml',\n",
       " 'Judicious Selection of Training Data in Assisting Language for Multilingual Neural NER.tei.xml',\n",
       " 'Efficient end-to-end learning for quantizable representations.tei.xml',\n",
       " 'Answering Elementary Science Questions by Constructing Coherent Scenes using Background Knowledge.tei.xml',\n",
       " 'Learning to Optimize Combinatorial Functions.tei.xml',\n",
       " 'Towards String-to-Tree Neural Machine Translation.tei.xml',\n",
       " 'Sparse Coding of Neural Word Embeddings for Multilingual Sequence Labeling.tei.xml',\n",
       " 'Automated Curriculum Learning for Neural Networks.tei.xml',\n",
       " 'Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning.tei.xml',\n",
       " 'A Study of Style in Machine Translation_ Controlling the Formality of Machine Translation Output.tei.xml',\n",
       " 'Analogs of Linguistic Structure in Deep Representations.tei.xml',\n",
       " 'Neural Joint Model for Transition-based Chinese Syntactic Analysis.tei.xml',\n",
       " 'On the Limitations of First-Order Approximation in GAN Dynamics.tei.xml',\n",
       " 'A Tabular Method for Dynamic Oracles in Transition-Based Parsing.tei.xml',\n",
       " 'NEWSROOM_ A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies.tei.xml',\n",
       " 'From Patches to Images_ A Nonparametric Generative Model.tei.xml',\n",
       " 'Nonparametric Regression with Comparisons_ Escaping the Curse of Dimensionality with Ordinal Information.tei.xml',\n",
       " 'Representation Tradeoffs for Hyperbolic Embeddings.tei.xml',\n",
       " 'A large annotated corpus for learning natural language inference.tei.xml',\n",
       " 'Affinity-Preserving Random Walk for Multi-Document Summarization.tei.xml',\n",
       " 'Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling.tei.xml',\n",
       " 'Learning to Generate Compositional Color Descriptions.tei.xml',\n",
       " 'SCDV _ Sparse Composite Document Vectors using soft clustering over distributional representations.tei.xml',\n",
       " 'Learning in POMDPs with Monte Carlo Tree Search.tei.xml',\n",
       " 'Long Short-Term Memory Neural Networks for Chinese Word Segmentation.tei.xml',\n",
       " 'Large-Scale Multi-Domain Belief Tracking with Knowledge Sharing.tei.xml',\n",
       " 'StingyCD_ Safely Avoiding Wasteful Updates in Coordinate Descent.tei.xml',\n",
       " 'Deep Density Destructors.tei.xml',\n",
       " 'A Hybrid Convolutional Variational Autoencoder for Text Generation.tei.xml',\n",
       " 'Uncovering Causality from Multivariate Hawkes Integrated Cumulants.tei.xml',\n",
       " 'Sequence Effects in Crowdsourced Annotations.tei.xml',\n",
       " 'On the Generalization of Equivariance and Convolution in Neural Networks  to the Action of Compact Groups.tei.xml',\n",
       " 'Gaussian Mixture Latent Vector Grammars.tei.xml',\n",
       " 'Can Neural Machine Translation be Improved with User Feedback_.tei.xml',\n",
       " 'Learning principled bilingual mappings of word embeddings while preserving monolingual invariance.tei.xml',\n",
       " 'Towards End-to-End Prosody Transfer  for Expressive Speech Synthesis with Tacotron.tei.xml',\n",
       " 'Analyzing the Robustness of Nearest Neighbors to Adversarial Examples.tei.xml',\n",
       " 'Spinning Straw into Gold_ Using Free Text to Train Monolingual Alignment Models for Non-factoid Question Answering.tei.xml',\n",
       " 'Distilling Knowledge for Search-based Structured Prediction.tei.xml',\n",
       " 'Multimodal Language Analysis with Recurrent Multistage Fusion.tei.xml',\n",
       " 'Selective Encoding for Abstractive Sentence Summarization.tei.xml',\n",
       " 'Importance sampling for unbiased on-demand evaluation of knowledge base population.tei.xml',\n",
       " 'Memory-Based Acquisition of Argument Structures and its Application to Implicit Role Detection.tei.xml',\n",
       " 'Variational Inference for Sparse and Undirected Models.tei.xml',\n",
       " 'Discrete-Continuous Mixtures in Probabilistic Programming_ Generalized Semantics and Inference Algorithms.tei.xml',\n",
       " 'Coordinated Multi-Agent Imitation Learning.tei.xml',\n",
       " 'Neural Text Generation from Structured Data with Application to the Biography Domain.tei.xml',\n",
       " 'Multilingual Summarization with Polytope Model.tei.xml',\n",
       " 'Learning Context-Aware Convolutional Filters for Text Processing.tei.xml',\n",
       " 'Document Context Neural Machine Translation with Memory Networks.tei.xml',\n",
       " 'Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation.tei.xml',\n",
       " 'Geodesic Convolutional Shape Optimization.tei.xml',\n",
       " 'A Full Non-Monotonic Transition System for Unrestricted Non-Projective Parsing.tei.xml',\n",
       " 'Robust Cross-lingual Hypernymy Detection using Dependency Context.tei.xml',\n",
       " 'Playing 20 Question Game with Policy-Based Reinforcement Learning.tei.xml',\n",
       " 'Convolutional Neural Network Language Models.tei.xml',\n",
       " 'Learning Structural Kernels for Natural Language Processing.tei.xml',\n",
       " 'Meritocratic Fairness for Cross-Population Selection.tei.xml',\n",
       " 'Automatically Solving Number Word Problems by Semantic Parsing and Reasoning.tei.xml',\n",
       " 'SciDTB_ Discourse Dependency TreeBank for Scientific Abstracts.tei.xml',\n",
       " 'Transformation Networks for Target-Oriented Sentiment Classification.tei.xml',\n",
       " 'simNet_ Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions.tei.xml',\n",
       " 'PreCo_ A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution.tei.xml',\n",
       " 'Entity Commonsense Representation for Neural Abstractive Summarization.tei.xml',\n",
       " 'A Corpus of Sentence-level Revisions in Academic Writing_ A Step towards Understanding Statement Strength in Communication.tei.xml',\n",
       " 'Cross-lingual Opinion Analysis via Negative Transfer Detection.tei.xml',\n",
       " 'Binary Partitions with Approximate Minimum Impurity.tei.xml',\n",
       " 'Zeroshot Multimodal Named Entity Disambiguation for Noisy Social Media Posts.tei.xml',\n",
       " 'Alternative Objective Functions for Training MT Evaluation Metrics.tei.xml',\n",
       " 'Enhancing Drug-Drug Interaction Extraction from Texts by Molecular Structure Information.tei.xml',\n",
       " 'Neural Optimizer Search with Reinforcement Learning.tei.xml',\n",
       " 'Personalized Machine Translation_ Predicting Translational Preferences.tei.xml',\n",
       " 'Solving Partial Assignment Problems using Random Clique Complexes.tei.xml',\n",
       " 'Improving Semantic Parsing with Enriched Synchronous Context-Free Grammar.tei.xml',\n",
       " 'On Learning Sparsely Used Dictionaries from Incomplete Samples.tei.xml',\n",
       " 'Naturalizing a Programming Language via Interactive Learning.tei.xml',\n",
       " 'How Much Reading Does Reading Comprehension Require_ A Critical Investigation of Popular Benchmarks.tei.xml',\n",
       " 'Disfluency Detection Using Multi-step Stacked Learning.tei.xml',\n",
       " 'Towards a Discourse Relation-aware Approach for Chinese-English Machine Translation.tei.xml',\n",
       " 'Attention-based LSTM Network for Cross-Lingual Sentiment Classification.tei.xml',\n",
       " 'Name List Only_ Target Entity Disambiguation in Short Texts.tei.xml',\n",
       " 'Spherical Structured Feature Maps for Kernel Approximation.tei.xml',\n",
       " 'Batched High-dimensional Bayesian Optimization via Structural Kernel Learning.tei.xml',\n",
       " 'Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task.tei.xml',\n",
       " 'Multi-Reference Training with Pseudo-References for Neural Translation and Text Generation.tei.xml',\n",
       " 'Dual Supervised Learning.tei.xml',\n",
       " 'Personalizing Dialogue Agents_ I have a dog, do you have pets too_.tei.xml',\n",
       " 'Globally Induced Forest_ A Prepruning Compression Scheme.tei.xml',\n",
       " 'Adversarial Examples for Evaluating Reading Comprehension Systems.tei.xml',\n",
       " 'Kernel Recursive ABC_ Point Estimation with Intractable Likelihood.tei.xml',\n",
       " 'High-dimensional Non-Gaussian Single Index Models via Thresholded Score Function Estimation.tei.xml',\n",
       " 'Efficient Contextualized Representation_ Language Model Pruning for Sequence Labeling.tei.xml',\n",
       " 'HOTPOTQA_ A Dataset for Diverse, Explainable Multi-hop Question Answering.tei.xml',\n",
       " 'Abstractive Document Summarization with a Graph-Based Attentional Neural Model.tei.xml',\n",
       " 'Confident Multiple Choice Learning.tei.xml',\n",
       " 'Transition-Based Dependency Parsing with Heuristic Backtracking.tei.xml',\n",
       " 'Semi-Supervised Sequence Modeling with Cross-View Training.tei.xml',\n",
       " 'Towards Black-box Iterative Machine Teaching.tei.xml',\n",
       " 'Automatic Measures to Characterise Verbal Alignment in Human-Agent Interaction.tei.xml',\n",
       " 'Colors in Context_ A Pragmatic Neural Model for Grounded Language Understanding.tei.xml',\n",
       " 'Learning Deep Architectures via Generalized Whitened Neural Networks.tei.xml',\n",
       " 'Stochastic Variance Reduction Methods for Policy Evaluation.tei.xml',\n",
       " 'Fine-grained Coordinated Cross-lingual Text Stream Alignment for Endless Language Knowledge Acquisition.tei.xml',\n",
       " 'Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access.tei.xml',\n",
       " 'Dropout Inference in Bayesian Neural Networks with Alpha-divergences.tei.xml',\n",
       " 'On the Limitations of Unsupervised Bilingual Dictionary Induction.tei.xml',\n",
       " 'A Nil-Aware Answer Extraction Framework for Question Answering.tei.xml',\n",
       " 'Asynchronous Stochastic Quasi-Newton MCMC for Non-Convex Optimization.tei.xml',\n",
       " 'Conversations Gone Awry_ Detecting Early Signs of Conversational Failure.tei.xml',\n",
       " 'Bayesian Models of Data Streams with Hierarchical Power Priors.tei.xml',\n",
       " 'Robust Submodular Maximization_  A Non-Uniform Partitioning Approach.tei.xml',\n",
       " 'Token-level and sequence-level loss smoothing for RNN language models.tei.xml',\n",
       " 'Mining Inference Formulas by Goal-Directed Random Walks.tei.xml',\n",
       " 'End-to-End Learning for the Deep Multivariate Probit Model .tei.xml',\n",
       " 'Modeling Empathy and Distress in Reaction to News Stories.tei.xml',\n",
       " 'Syntax-based Rewriting for Simultaneous Machine Translation.tei.xml',\n",
       " 'Semantics as a Foreign Language.tei.xml',\n",
       " 'Batch Bayesian Optimization via Multi-objective Acquisition Ensemble for Automated Analog Circuit Design.tei.xml',\n",
       " 'Semi-Supervised Learning on Data Streams via Temporal Label Propagation.tei.xml',\n",
       " 'Verbal and Nonverbal Clues for Real-life Deception Detection.tei.xml',\n",
       " 'Variational Bayesian dropout_ pitfalls and fixes.tei.xml',\n",
       " 'Semantic Annotation for Microblog Topics Using Wikipedia Temporal Information.tei.xml',\n",
       " 'Competitive Caching with Machine Learned Advice.tei.xml',\n",
       " 'Error-repair Dependency Parsing for Ungrammatical Texts.tei.xml',\n",
       " 'Noise2Noise_ Learning Image Restoration without Clean Data.tei.xml',\n",
       " 'To Attend or not to Attend_ A Case Study on Syntactic Structures for Semantic Relatedness.tei.xml',\n",
       " 'Consistent k-Clustering.tei.xml',\n",
       " 'How NOT To Evaluate Your Dialogue System_ An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation.tei.xml',\n",
       " 'A Primal-Dual Analysis of Global Optimality in Nonconvex Low-Rank  Matrix Recovery.tei.xml',\n",
       " 'Parsing as Language Modeling.tei.xml',\n",
       " 'Going out on a limb_ Joint Extraction of Entity Mentions and Relations without Dependency Trees.tei.xml',\n",
       " 'The University of Alicante at MultiLing 2015_ approach, results and further insights.tei.xml',\n",
       " 'Identifying Semantic Edit Intentions from Revisions in Wikipedia.tei.xml',\n",
       " 'Investigating Capsule Networks with Dynamic Routing for Text Classification.tei.xml',\n",
       " 'Gradient Descent for Sparse Rank-One Matrix Completion for Crowd-Sourced Aggregation of Sparsely Interacting Workers.tei.xml',\n",
       " 'Convolutional Sequence to Sequence Learning.tei.xml',\n",
       " 'Using Personal Traits For Brand Preference Prediction.tei.xml',\n",
       " 'Neural Symbolic Machines_ Learning Semantic Parsers on Freebase with Weak Supervision.tei.xml',\n",
       " 'Neural Open Information Extraction.tei.xml',\n",
       " 'Fitting New Speakers Based on a Short Untranscribed Sample.tei.xml',\n",
       " 'A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.tei.xml',\n",
       " 'Globally Normalized Reader.tei.xml',\n",
       " 'Comprehensive Supersense Disambiguation of English Prepositions and Possessives.tei.xml',\n",
       " 'Efficient and Expressive Knowledge Base Completion Using Subgraph Feature Extraction.tei.xml',\n",
       " 'One-Shot Segmentation in Clutter.tei.xml',\n",
       " 'MISSION_ Ultra Large-Scale Feature Selection using Count-Sketches.tei.xml',\n",
       " 'Semantic Parsing with Semi-Supervised Sequential Autoencoders.tei.xml',\n",
       " 'Predicting Native Language from Gaze.tei.xml',\n",
       " 'Neural Argument Generation Augmented with Externally Retrieved Evidence.tei.xml',\n",
       " 'Learning Word Embeddings for Low-resource Languages by PU Learning.tei.xml',\n",
       " 'Bayesian Boolean Matrix Factorisation.tei.xml',\n",
       " 'Attentive listening system with backchanneling, response generation and flexible turn-taking.tei.xml',\n",
       " 'TAPAS_ Tricks to Accelerate (encrypted) Prediction As a Service.tei.xml',\n",
       " 'Fine-Grained Discourse Structures in Continuation Semantics.tei.xml',\n",
       " 'End-to-End Differentiable Adversarial Imitation Learning.tei.xml',\n",
       " 'Neural Machine Translation via Binary Code Prediction.tei.xml',\n",
       " 'Improving Slot Filling in Spoken Language Understanding with Joint Pointer and Attention.tei.xml',\n",
       " 'Regret Minimization for Partially Observable Deep Reinforcement Learning.tei.xml',\n",
       " 'Attention Strategies for Multi-Source Sequence-to-Sequence Learning.tei.xml',\n",
       " 'Automatic Metric Validation for Grammatical Error Correction.tei.xml',\n",
       " 'On The Projection Operator to A Three-view Cardinality Constrained Set.tei.xml',\n",
       " 'Theoretical Analysis of Sparse Subspace Clustering with Missing Entries.tei.xml',\n",
       " 'Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems.tei.xml',\n",
       " 'Training Millions of Personalized Dialogue Agents.tei.xml',\n",
       " 'Statistical Inference for Incomplete Ranking Data_ The Case of Rank-Dependent Coarsening.tei.xml',\n",
       " 'DARLA_ Improving Zero-Shot Transfer in Reinforcement Learning.tei.xml',\n",
       " 'Neural Relation Extraction with Multi-lingual Attention.tei.xml',\n",
       " 'Probabilistic Submodular Maximization in Sub-Linear Time.tei.xml',\n",
       " 'An Iterative, Sketching-based Framework for Ridge Regression.tei.xml',\n",
       " 'Get To The Point_ Summarization with Pointer-Generator Networks.tei.xml',\n",
       " 'Pushing the Limits of Translation Quality Estimation.tei.xml',\n",
       " 'Interpretable and Compositional Relation Learning by Joint Training with an Autoencoder.tei.xml',\n",
       " 'Tackling the Story Ending Biases in The Story Cloze Test.tei.xml',\n",
       " 'Bilingual Structured Language Models for Statistical Machine Translation.tei.xml',\n",
       " 'WIKIQA_ A Challenge Dataset for Open-Domain Question Answering.tei.xml',\n",
       " 'NetGAN_ Generating Graphs via Random Walks.tei.xml',\n",
       " 'Winning on the Merits_ The Joint Effects of Content and Style on Debate Outcomes.tei.xml',\n",
       " 'Knowledge transfer between speakers for personalised dialogue management.tei.xml',\n",
       " 'NEXUS Network_ Connecting the Preceding and the Following in Dialogue Generation.tei.xml',\n",
       " 'What makes a convincing argument_ Empirical analysis and detecting attributes of convincingness in Web argumentation.tei.xml',\n",
       " 'Sharp Models on Dull Hardware_ Fast and Accurate Neural Machine Translation Decoding on the CPU.tei.xml',\n",
       " 'Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism.tei.xml',\n",
       " 'Interpretable Emoji Prediction via Label-Wise Attention LSTMs.tei.xml',\n",
       " 'Exploiting the Scope of Negations and Heterogeneous Features for Relation Extraction_ A Case Study for Drug-Drug Interaction Extraction.tei.xml',\n",
       " 'Efficient Gradient-Free Variational Inference using Policy Search.tei.xml',\n",
       " 'Gromov-Wasserstein Alignment of Word Embedding Spaces.tei.xml',\n",
       " 'One Vector is Not Enough_ Entity-Augmented Distributed Semantics for Discourse Relations.tei.xml',\n",
       " 'Active Learning for Top-K Rank Aggregation from Noisy Comparisons.tei.xml',\n",
       " 'The Importance of Being Recurrent for Modeling Hierarchical Structure.tei.xml',\n",
       " 'Gradient Boosted Decision Trees for High Dimensional Sparse Output.tei.xml',\n",
       " 'Linguistic Cues to Deception and Perceived Deception in Interview Dialogues.tei.xml',\n",
       " 'Stochastic Video Generation with a Learned Prior.tei.xml',\n",
       " 'Differentially Private Submodular Maximization_ Data Summarization in Disguise.tei.xml',\n",
       " 'Candidates vs. Noises Estimation for Large Multi-Class Classification Problem.tei.xml',\n",
       " 'Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings.tei.xml',\n",
       " 'Distributional vectors encode referential attributes.tei.xml',\n",
       " 'Prox-PDA_ The Proximal Primal-Dual Algorithm for Fast Distributed Nonconvex Optimization and Learning Over Networks.tei.xml',\n",
       " 'Dynamical Isometry and a Mean Field Theory of CNNs_ How to Train 10,000-Layer Vanilla Convolutional Neural Networks.tei.xml',\n",
       " 'On the Implicit Bias of Dropout.tei.xml',\n",
       " 'Enumerating Distinct Decision Trees.tei.xml',\n",
       " 'Sentence Simplification with Deep Reinforcement Learning.tei.xml',\n",
       " 'Automatic Extraction of Implicit Interpretations from Modal Constructions.tei.xml',\n",
       " 'Adaptive HTER Estimation for Document-Specific MT Post-Editing.tei.xml',\n",
       " 'Consistency Analysis for Binary Classification Revisited.tei.xml',\n",
       " 'Nonparametric Bayesian Semi-supervised Word Segmentation.tei.xml',\n",
       " 'Improved Dependency Parsing using Implicit Word Connections Learned from Unlabeled Data.tei.xml',\n",
       " 'Prediction Rule Reshaping.tei.xml',\n",
       " 'Non-Projective Dependency Parsing with Non-Local Transitions.tei.xml',\n",
       " 'Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms.tei.xml',\n",
       " 'Conversational Image Editing_ Incremental Intent Identification in a New Dialogue Task.tei.xml',\n",
       " 'Open Category Detection with PAC Guarantees.tei.xml',\n",
       " 'Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction.tei.xml',\n",
       " 'Deeply AggreVaTeD_ Differentiable Imitation Learning for Sequential Prediction.tei.xml',\n",
       " 'Oracle Complexity of Second-Order Methods for Finite-Sum Problems.tei.xml',\n",
       " 'Deep contextualized word representations.tei.xml',\n",
       " 'Local Density Estimation in High Dimensions.tei.xml',\n",
       " 'Problems in Current Text Simplification Research_ New Data Can Help.tei.xml',\n",
       " 'Safe Element Screening for Submodular Function Minimization.tei.xml',\n",
       " 'A Deep Neural Network Sentence Level Classification Method with Context Information.tei.xml',\n",
       " 'Higher-order logical inference with compositional semantics.tei.xml',\n",
       " 'DR-BiLSTM_ Dependent Reading Bidirectional LSTM for Natural Language Inference.tei.xml',\n",
       " 'Stochastic Bouncy Particle Sampler.tei.xml',\n",
       " 'Has Machine Translation Achieved Human Parity_ A Case for Document-level Evaluation.tei.xml',\n",
       " 'Towards Evaluating Narrative Quality In Student Writing.tei.xml',\n",
       " 'Differentially Private Learning of Undirected Graphical Models Using Collective Graphical Models.tei.xml',\n",
       " 'SimVerb-3500_ A Large-Scale Evaluation Set of Verb Similarity.tei.xml',\n",
       " 'Discourse Complements Lexical Semantics for Non-factoid Answer Reranking.tei.xml',\n",
       " 'Pathologies of Neural Models Make Interpretations Difficult.tei.xml',\n",
       " 'Do Multi-Sense Embeddings Improve Natural Language Understanding_.tei.xml',\n",
       " 'Recurrent Polynomial Network for Dialogue State Tracking with Mismatched Semantic Parsers.tei.xml',\n",
       " 'A Deep Generative Model of Vowel Formant Typology.tei.xml',\n",
       " 'Modeling Linguistic and Personality Adaptation for Natural Language Generation.tei.xml',\n",
       " 'K-means clustering using random matrix sparsification.tei.xml',\n",
       " 'An Empirical Comparison Between N-gram and Syntactic Language Models for Word Ordering.tei.xml',\n",
       " 'Differentially Private Chi-squared Test by Unit Circle Mechanism.tei.xml',\n",
       " 'A Convolutional Encoder Model for Neural Machine Translation.tei.xml',\n",
       " 'Semi-supervised User Geolocation via Graph Convolutional Networks.tei.xml',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../papers-xmls/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir OUT/\"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "y=\"a\"\n",
    "os.mkdir(os.path.join(\"OUT\", y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_files(d, dest):\n",
    "    for key in sorted(d):\n",
    "        #text_file = open(d[key][0][0]+\".txt\", \"w\")\n",
    "        #print(d[key][0])\n",
    "        put=\"\"\n",
    "        if(type(d[key][0])==tuple):\n",
    "            x=d[key][0][0]\n",
    "            y=key+'_'+('_'.join(x.split(' ')))\n",
    "            for i in d[key]:\n",
    "                put=put+\" \"+i[1]    # from subsection take subsection text\n",
    "        else:\n",
    "            x=str(key)\n",
    "            y=x\n",
    "            put=d[key][0]\n",
    "        \n",
    "        print(y)  #filename is y\n",
    "        print(put)\n",
    "        text_file = open(dest+y, \"w\")\n",
    "        n = text_file.write(put)\n",
    "        text_file.close()\n",
    "\n",
    "##for abstract seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    import os\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    # traverse root directory, and list directories as dirs and files as files\n",
    "    for root, dirs, files in os.walk(\"../papers-xmls\"):\n",
    "        #path = root.split(os.sep)\n",
    "        #print((len(path) - 1) * '---', os.path.basename(root))\n",
    "        for file in files:\n",
    "            if(file.endswith(\".tei.xml\")):\n",
    "                x='_'.join(file.split('.')[:-2])\n",
    "                y='_'.join(x.split(' '))\n",
    "                print(y)\n",
    "                #os.mkdir(os.path.join(\"OUT\", y))\n",
    "                #os.rename(\"../papers-xmls/\"+file, \"../papers-xmls/\"+y+\".tei.xml\")\n",
    "                dest=\"OUT/\"+y+\"/\"\n",
    "                print(dest)\n",
    "            \n",
    "                tei_doc = \"../papers-xmls/\"+file\n",
    "                print(tei_doc)\n",
    "                with open(tei_doc, 'r') as tei:\n",
    "                    soup = BeautifulSoup(tei, 'lxml')\n",
    "            \n",
    "                with open(tei_doc, 'r') as tei:\n",
    "                    soup1 = BeautifulSoup(tei, 'xml')\n",
    "                divs_text=[]\n",
    "                divs_text=make_divs(soup)\n",
    "            \n",
    "                maxx=0\n",
    "                headings=[]\n",
    "                headings, maxx=make_head(soup1, maxx)\n",
    "            \n",
    "                d={}\n",
    "                d=make_d(divs_text, headings, maxx)\n",
    "                #make divs_text\n",
    "            \n",
    "                text_file = open(dest+\"abstract\", \"w\")\n",
    "                n = text_file.write(soup.abstract.getText(separator=' ', strip=True))\n",
    "                text_file.close()\n",
    "            \n",
    "                make_files(d, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple_Instance_Learning_Networks_for_Fine-Grained_Sentiment_Analysis\n",
      "OUT/Multiple_Instance_Learning_Networks_for_Fine-Grained_Sentiment_Analysis/\n",
      "../papers-xmls/Multiple_Instance_Learning_Networks_for_Fine-Grained_Sentiment_Analysis.tei.xml\n",
      "1_Introduction\n",
      " Sentiment analysis has become a fundamental area of research in Natural Language Processing thanks to the proliferation of user-generated content in the form of online reviews, blogs, internet forums, and social media. A plethora of methods have been proposed in the literature that attempt to distill sentiment information from text, allowing users and service providers to make opinion-driven decisions. The success of neural networks in a variety of applications (Bahdanau et al., 2015; Le and Mikolov, 2014; Socher et al., 2013) and the availability of large amounts of labeled data have led to an increased focus on sentiment classification. Supervised models are typically trained on documents (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b; Tang et al., 2015; Yang et al., 2016) , sentences (Kim, 2014) , or phrases (Socher et al., 2011; [Rating: ] I had a very mixed experience at The Stand. The burger and fries were good. The chocolate shake was divine: rich and creamy. The drive-thru was horrible. It took us at least 30 minutes to order when there were only four cars in front of us. We complained about the wait and got a half-hearted apology. I would go back because the food is good, but my only hesitation is the wait. Summary + The burger and fries were good + The chocolate shake was divine + I would go back because the food is good -The drive-thru was horrible -It took us at least 30 minutes to order Figure 1 : An EDU-based summary of a 2-out-of-5 star review with positive and negative snippets. Socher et al., 2013) annotated with sentiment labels and used to predict sentiment in unseen texts. Coarse-grained document-level annotations are relatively easy to obtain due to the widespread use of opinion grading interfaces (e.g., star ratings accompanying reviews). In contrast, the acquisition of sentence-or phrase-level sentiment labels remains a laborious and expensive endeavor despite its relevance to various opinion mining applications, e.g., detecting or summarizing consumer opinions in online product reviews. The usefulness of finer-grained sentiment analysis is illustrated in the example of Figure 1 , where snippets of opposing polarities are extracted from a 2-star restaurant review. Although, as a whole, the review conveys negative sentiment, aspects of the reviewer's experience were clearly positive. This goes largely unnoticed when focusing solely on the review's overall rating. In this work, we consider the problem of segmentlevel sentiment analysis from the perspective of Multiple Instance Learning (MIL; Keeler, 1991) . Instead of learning from individually labeled segments, our model only requires document-level supervision and learns to introspectively judge the sentiment of constituent segments. Beyond showing how to utilize document collections of rated reviews to train fine-grained sentiment predictors, we also investigate the granularity of the extracted segments. Previous research (Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Nallapati et al., 2017) has predominantly viewed documents as sequences of sentences. Inspired by recent work in summarization (Li et al., 2016) and sentiment classification (Bhatia et al., 2015) , we also represent documents via Rhetorical Structure Theory's (Mann and Thompson, 1988) Elementary Discourse Units (EDUs). Although definitions for EDUs vary in the literature, we follow standard practice and take the elementary units of discourse to be clauses (Carlson et al., 2003) . We employ a state-of-the-art discourse parser (Feng and Hirst, 2012) to identify them. Our contributions in this work are three-fold: a novel multiple instance learning neural model which utilizes document-level sentiment supervision to judge the polarity of its constituent segments; the creation of SPOT, a publicly available dataset which contains Segment-level POlariTy annotations (for sentences and EDUs) and can be used for the evaluation of MIL-style models like ours; and the empirical finding (through automatic and human-based evaluation) that neural multiple instance learning is superior to more conventional neural architectures and other baselines on detecting segment sentiment and extracting informative opinions in reviews. 1\n",
      "2_Background\n",
      " Our work lies at the intersection of multiple research areas, including sentiment classification, opinion mining and multiple instance learning. We review related work in these areas below. Sentiment Classification Sentiment classification is one of the most popular tasks in sentiment analysis. Early work focused on unsupervised methods and the creation of sentiment lexicons (Turney, 2002; Hu and Liu, 2004; Wiebe et al., 2005; Baccianella et al., 2010) based on which the overall po-larity of a text can be computed (e,g., by aggregating the sentiment scores of constituent words). More recently, Taboada et al. (2011) introduced SO-CAL, a state-of-the-art method that combines a rich sentiment lexicon with carefully defined rules over syntax trees to predict sentence sentiment. Supervised learning techniques have subsequently dominated the literature (Pang et al., 2002; Pang and Lee, 2005; Qu et al., 2010; Xia and Zong, 2010; Wang and Manning, 2012; Le and Mikolov, 2014) thanks to user-generated sentiment labels or large-scale crowd-sourcing efforts (Socher et al., 2013) . Neural network models in particular have achieved state-of-the-art performance on various sentiment classification tasks due to their ability to alleviate feature engineering. Kim (2014) introduced a very successful CNN architecture for sentence-level classification, whereas other work (Socher et al., 2011; Socher et al., 2013) uses recursive neural networks to learn sentiment for segments of varying granularity (i.e., words, phrases, and sentences). We describe Kim's (2014) approach in more detail as it is also used as part of our model. Let x i denote a k-dimensional word embedding of the i-th word in text segment s of length n. The segment's input representation is the concatenation of word embeddings x 1 , . . . , x n , resulting in word matrix X. Let X i:i+j refer to the concatenation of embeddings x i , . . . , x i+j . A convolution filter W  R lk , applied to a window of l words, produces a new feature c i = ReLU(W  X i:i+l + b), where ReLU is the Rectified Linear Unit non-linearity, '' denotes the entrywise product followed by a sum over all elements and b  R is a bias term. Applying the same filter to every possible window of word vectors in the segment, produces a feature map c = [c 1 , c 2 , . . . , c nl+1 ]. Multiple feature maps for varied window sizes are applied, resulting in a fixed-size segment representation v via max-overtime pooling. We will refer to the application of convolution to an input word matrix X, as CNN(X). A final sentiment prediction is produced using a softmax classifier and the model is trained via backpropagation using sentence-level sentiment labels. The availability of large-scale datasets (Diao et al., 2014; Tang et al., 2015) has also led to the development of document-level sentiment classifiers which exploit hierarchical neural representations. These are obtained by first building representations of sentences and aggregating those into a document feature vector (Tang et al., 2015) . Yang et al. (2016) further acknowledge that words and sentences are deferentially important in different contexts. They present a model which learns to attend (Bahdanau et al., 2015) to individual text parts when constructing document representations. We describe such an architecture in more detail as we use it as a point of comparison with our own model. Given document d comprising segments (s 1 , . . . , s m ), a Hierarchical Network with attention (henceforth HIERNET; based on Yang et al., 2016) produces segment representations (v 1 , . . . , v m ) which are subsequently fed into a bidirectional GRU module (Bahdanau et al., 2015) , whose resulting hidden vectors (h 1 , . . . , h m ) are used to produce attention weights (a 1 , . . . , a m ) (see Section 3.2 for more details on the attention mechanism). A document is represented as the weighted average of the segments' hidden vectors v d = i a i h i . A final sentiment prediction is obtained using a softmax classifier and the model is trained via back-propagation using document-level sentiment labels. The architecture is illustrated in Figure 2 (a). In their proposed model, Yang et al. (2016) use bidirectional GRU modules to represent segments as well as documents, whereas we use a more efficient CNN encoder to compose words into segment vectors 2 (i.e., v i = CNN(X i )). Note that models like HIERNET do not naturally predict sentiment for individual segments; we discuss how they can be used for segment-level opinion extraction in Section 5.2. Our own work draws inspiration from representation learning (Tang et al., 2015; Kim, 2014) , especially the idea that not all parts of a document convey sentiment-worthy clues (Yang et al., 2016) . Our model departs from previous approaches in that it provides a natural way of predicting the polarity of individual text segments without requiring segment-level annotations. Moreover, our attention mechanism directly facilitates opinion detection rather than simply aggregating sentence representations into a single document vector. Opinion Mining A standard setting for opinion mining and summarization (Lerman et al., 2009; Carenini et al., 2006; Ganesan et al., 2010; Di Fabbrizio et al., 2014; Gerani et al., 2014 ) assumes a set of documents that contain opinions about some entity of interest (e.g., camera). The goal of the system is to generate a summary that is representative of the average opinion and speaks to its important aspects (e.g., picture quality, battery life, value). Output summaries can be extractive (Lerman et al., 2009) or abstractive (Gerani et al., 2014; Di Fabbrizio et al., 2014) and the underlying systems exhibit varying degrees of linguistic sophistication from identifying aspects (Lerman et al., 2009 ) to using RSTstyle discourse analysis, and manually defined templates (Gerani et al., 2014; Di Fabbrizio et al., 2014) . Our proposed method departs from previous work in that it focuses on detecting opinions in individual documents. Given a review, we predict the polarity of every segment, allowing for the extraction of sentiment-heavy opinions. We explore the usefulness of EDU segmentation inspired by Li et al. (2016) , who show that EDU-based summaries align with near-extractive summaries constructed by news editors. Importantly, our model is trained in a weakly-supervised fashion on large scale document classification datasets without recourse to finegrained labels or gold-standard opinion summaries. Multiple Instance Learning Our models adopt a Multiple Instance Learning (MIL) framework. MIL deals with problems where labels are associated with groups of instances or bags (documents in our case), while instance labels (segment-level polarities) are unobserved. An aggregation function is used to combine instance predictions and assign labels on the bag level. The goal is either to label bags (Keeler and Rumelhart, 1992; Dietterich et al., 1997; Maron and Ratan, 1998) or to simultaneously infer bag and instance labels (Zhou et al., 2009; Wei et al., 2014; Kotzias et al., 2015) . We view segment-level sentiment analysis as an instantiation of the latter variant. Initial MIL efforts for binary classification made the strong assumption that a bag is negative only if all of its instances are negative, and positive otherwise (Dietterich et al., 1997; Maron and Ratan, 1998; Zhang et al., 2002; Andrews and Hofmann, 2004; Carbonetto et al., 2008) . Subsequent work re-laxed this assumption, allowing for prediction combinations better suited to the tasks at hand. Weidmann et al. (2003) introduced a generalized MIL framework, where a combination of instance types is required to assign a bag label. Zhou et al. (2009) used graph kernels to aggregate predictions, exploiting relations between instances in object and text categorization. Xu and Frank (2004) proposed a multiple-instance logistic regression classifier where instance predictions were simply averaged, assuming equal and independent contribution toward bag classification. More recently, Kotzias et al. (2015) used sentence vectors obtained by a pre-trained hierarchical CNN (Denil et al., 2014) as features under an unweighted average MIL objective. Prediction averaging was further extended by Pappas and Popescu-Belis (2014; , who used a weighted summation of predictions, an idea which we also adopt in our work. Applications of MIL are many and varied. MIL was first explored by Keeler and Rumelhart (1992) for recognizing handwritten post codes, where the position and value of individual digits was unknown. MIL techniques have since been applied to drug activity prediction (Dietterich et al., 1997) , image retrieval (Maron and Ratan, 1998; Zhang et al., 2002) , object detection (Zhang et al., 2006; Carbonetto et al., 2008; Cour et al., 2011) , text classification (Andrews and Hofmann, 2004), image captioning (Wu et al., 2015) , paraphrase detection (Xu et al., 2014) , and information extraction (Hoffmann et al., 2011) . When applied to sentiment analysis, MIL takes advantage of supervision signals on the document level in order to train segment-level sentiment predictors. Although their work is not couched in the framework of MIL, Tckstrm and McDonald (2011) show how sentence sentiment labels can be learned as latent variables from document-level annotations using hidden conditional random fields. Pappas and Popescu-Belis (2014) use a multiple instance regression model to assign sentiment scores to specific aspects of products. The Group-Instance Cost Function (GICF), proposed by Kotzias et al. (2015) , averages sentence sentiment predictions during trainng, while ensuring that similar sentences receive similar polarity labels. Their work uses a pre-trained hierarchical CNN to obtain sentence embeddings, but is not trainable end-to-end, in contrast with our proposed network. Additionally, none of the aforementioned efforts explicitly evaluate opinion extraction quality.\n",
      "23\n",
      "Acknowledgments The authors gratefully acknowledge the support of the European Research Council (award number 681760). We thank TACL action editor Ani Nenkova and the anonymous reviewers whose feedback helped improve the present paper, as well as Charles Sutton, Timothy Hospedales, and members of EdinburghNLP for helpful discussions and suggestions.\n",
      "3_Methodology\n",
      " In this section we describe how multiple instance learning can be used to address some of the drawbacks seen in previous approaches, namely the need for expert knowledge in lexicon-based sentiment analysis (Taboada et al., 2011) , expensive finegrained annotation on the segment level (Kim, 2014; Socher et al., 2013) or the inability to naturally predict segment sentiment (Yang et al., 2016) . Under multiple instance learning (MIL), a dataset D is a collection of labeled bags, each of which is a group of unlabeled instances. Specifically, each document d is a sequence (bag) of segments (instances). This sequence d = (s 1 , s 2 , . . . , s m ) is obtained from a document segmentation policy (see Section 4 for details). A discrete sentiment label y d  [1, C] is associated with each document, where the labelset is ordered and classes 1 and C correspond to maximally negative and maximally positive sentiment. It is assumed that y d is an unknown function of the unobserved segment-level labels: y d = f (y 1 , y 2 , . . . , y m ) (1) Probabilistic sentiment classifiers will produce document-level predictions d by selecting the most probable class according to class distribution p d = p (1) d , . . . , p (C) d . In a non-MIL framework a classifier would learn to predict the document's sentiment by directly conditioning on its segments' feature representations or their aggregate: p d =f  (v 1 , v 2 , . . . , v m ) (2) In contrast, a MIL classifier will produce a class distribution p i for each segment and additionally learn to combine these into a document-level prediction: p i = s (v i ) , (3) p d =f  d (p 1 , p 2 , . . . , p m ) . (4) In this work, andf are defined using a single neural network, described below. Hierarchical neural models like HIERNET have been used to predict document-level polarity by first encoding sentences and then combining these representations into a document vector. Hierarchical vector composition produces powerful sentiment predictors, but lacks the ability to introspectively judge the polarity of individual segments. Our Multiple Instance Learning Network (henceforth MILNET) is based on the following intuitive assumptions about opinionated text. Each segment conveys a degree of sentiment polarity, ranging from very negative to very positive. Additionally, segments have varying degrees of importance, in relation to the overall opinion of the author. The overarching polarity of a text is an aggregation of segment polarities, weighted by their importance. Thus, our model attempts to predict the polarity of segments and decides which parts of the document are good indicators of its overall sentiment, allowing for the detection of sentiment-heavy opinions. An illustration of MILNET is shown in Figure 2 (b); the model consists of three components: a CNN segment encoder, a softmax segment classifier and an attentionbased prediction weighting module. v i = CNN(X i ) is produced for each segment, using the CNN architecture described in Section 2. Segment Classification Obtaining a separate representation v i for every segment in a document allows us to produce individual segment sentiment predictions p i = p (1) i , . . . , p (C) i . This is achieved using a softmax classifier: p i = softmax(W c v i + b c ) , (5) where W c and b c are the classifier's parameters, shared across all segments. Individual distributions p i are shown in Figure 2 (b) as small bar-charts. In the simplest case, document-level predictions can be produced by taking the average of segment class distributions: p (c) d = 1 / m i p (c) i , c  [1, C] . This is, however, a crude way of combining segment sentiment, as not all parts of a document convey important sentiment clues. We opt for a segment attention mechanism which rewards text units that are more likely to be good sentiment predictors. Our attention mechanism is based on a bidirectional GRU component (Bahdanau et al., 2015) and 21 The starters were quite bland. I didn't enjoy most of them, but the burger was brilliant! inspired by Yang et al. (2016) . However, in contrast to their work, where attention is used to combine sentence representations into a single document vector, we utilize a similar technique to aggregate individual sentiment predictions. We first use separate GRU modules to produce forward and backward hidden vectors, which are then concatenated:   h i =    GRU(v i ), (6)   h i =    GRU(v i ), (7) h i = [   h i ,   h i ], i  [1, m] . (8) The importance of each segment is measured with the aid of a vector h a , as follows: h i = tanh(W a h i + b a ) , (9) a i = exp(h T i h a ) i exp(h T i h a ) , (10) where Equation (9) defines a one-layer MLP that produces an attention vector for the i-th segment. Attention weights a i are computed as the normalized similarity of each h i with h a . Vector h a , which is randomly initialized and learned during training, can be thought of as a trained key, able to recognize sentiment-heavy segments. The attention mechanism is depicted in the dashed box of Figure 2 , with attention weights shown as shaded circles. Finally, we obtain a document-level distribution over sentiment labels as the weighted sum of segment distributions (see top of Figure 2 (b)): p (c) d = i a i p (c) i , c  [1, C] . (11) Training The model is trained end-to-end on documents with user-generated sentiment labels. We use the negative log likelihood of the document-level prediction as an objective function: L =  d log p (y d ) d (12)\n",
      "4_Polarity-based_Opinion_Extraction\n",
      " After training, our model can produce segment-level sentiment predictions for unseen texts in the form of class probability distributions. A direct application of our method is opinion extraction, where highly positive and negative snippets are selected from the original document, producing extractive sentiment summaries, as described below. Polarity Scoring In order to extract opinion summaries, we need to rank segments according to their sentiment polarity. We introduce a method that takes our model's confidence in the prediction into account, by reducing each segment's class probability distribution p i to a single real-valued polarity score. To achieve this, we first define a real-valued class weight vector w = w (1) , . . . , w (C) | w (c)  [1, 1] that assigns uniformly-spaced weights to the ordered labelset, such that w (c+1)  w (c) = 2 C1 . For example, in a 5-class scenario, the class weight vector would be w = 1, 0.5, 0, 0.5, 1 . We compute the polarity score of a segment as the dot-product of the probability distribution p i with vector w: polarity(s i ) = c p (c) i w (c)  [1, 1] (13) Gated Polarity As a way of increasing the effectiveness of our method, we introduce a gated extension that uses the attention mechanism of our model to further differentiate between segments that carry 22 significant sentiment cues and those that do not: gated-polarity(s i ) = a i  polarity(s i ) , (14) where a i is the attention weight assigned to the i-th segment. This forces the polarity scores of segments the model does not attend to closer to 0. An illustration of our polarity scoring function is provided in Figure 3 , where the class predictions (top) of three restaurant review segments are mapped to their corresponding polarity scores (bottom). We observe that our method produces the desired result; segments 1 and 2 convey negative sentiment and receive negative scores, whereas the third segment is mapped to a positive score. Although the same discrete class label is assigned to the first two, the second segment's score is closer to 0 (neutral) as its class probability mass is more evenly distributed. As mentioned earlier, one of the hypotheses investigated in this work regards the use of subsentential units as the basis of extraction. Specifically, our model was applied to sentences and Elementary Discourse Units (EDUs), obtained from a Rhetorical Structure Theory (RST) parser (Feng and Hirst, 2012). According to RST, documents are first segmented into EDUs corresponding roughly to independent clauses which are then recursively combined into larger discourse spans. This results in a tree representation of the document, where connected nodes are characterized by discourse relations. We only utilize RST's segmentation, and leave the potential use of the tree structure to future work. The example in Figure 3 illustrates why EDUbased segmentation might be beneficial for opinion extraction. The second and third EDUs correspond to the sentence: I didn't enjoy most of them, but the burger was brilliant. Taken as a whole, the sentence conveys mixed sentiment, whereas the EDUs clearly convey opposing sentiment.\n",
      "5_Experimental_Setup\n",
      " In this section we describe the data used to assess the performance of our model. We also give details on model training and comparison systems. Our models were trained on two large-scale sentiment classification collections. The Yelp'13 corpus was introduced in Tang et al. (2015) and contains customer reviews of local businesses, each associated with human ratings on a scale from 1 (negative) to 5 (positive). The IMDB corpus of movie reviews was obtained from Diao et al. (2014) ; each review is associated with user ratings ranging from 1 to 10. Both datasets are split into training (80%), validation (10%) and test (10%) sets. A summary of statistics for each collection is provided in Table 1 . In order to evaluate model performance on the segment level, we constructed a new dataset named SPOT (as a shorthand for Segment POlariTy) by annotating documents from the Yelp'13 and IMDB collections. Specifically, we sampled reviews from each collection such that all document-level classes are represented uniformly, and the document lengths are representative of the respective corpus. Documents were segmented into sentences and EDUs, resulting in two segment-level datasets per collection. Statistics are summarized in Table 2 . Each review was presented to three Amazon Mechanical Turk (AMT) annotators who were asked to judge the sentiment conveyed by each segment (i.e., sentence or EDU) as negative, neutral, or pos- itive. We assigned labels using a majority vote or a fourth annotator in the rare cases of no agreement (< 5%). Figure 4 shows the distribution of segment labels for each document-level class. As expected, documents with positive labels contain a larger number of positive segments compared to documents with negative labels and vice versa. Neutral segments are distributed in an approximately uniform manner across document classes. Interestingly, the proportion of neutral EDUs is significantly higher compared to neutral sentences. The observation reinforces our argument in favor of EDU segmentation, as it suggests that a sentence with positive or negative overall polarity may still contain neutral EDUs. Discarding neutral EDUs, could therefore lead to more concise opinion extraction compared to relying on entire sentences. We further experimented on two collections introduced by Kotzias et al. (2015) which also originate from the YELP'13 and IMDB datasets. Each collection consists of 1,000 randomly sampled sentences annotated with binary sentiment labels. On the task of segment classification we compared MILNET, our multiple instance learning network, against the following methods: Majority: Majority class applied to all instances. State-of-the-art lexicon-based system that classifies segments into positive, neutral, and negative classes (Taboada et al., 2011) . Fully-supervised CNN segment classifier trained on SPOT's labels (Kim, 2014) . The Group-Instance Cost Function model introduced in Kotzias et al. (2015) . This is an unweighted average prediction aggregation MIL method that uses sentence features from a pretrained convolutional neural model. HIERNET: HIERNET does not explicitly generate individual segment predictions. Segment polarity scores are obtained by assigning the documentlevel prediction to every segment. We can then produce finer-grained polarity distinctions via gating, using the model's attention weights. We further illustrate the differences between HI-ERNET and MILNET in Figure 5 , which includes short descriptions and simplified equations for each model. MILNET naturally produces distinct segment polarities, while HIERNET assigns a single polarity score to every segment. In both cases, gating is a further means of identifying neutral segments. Finally, we differentiate between variants of HI-ERNET and MILNET according to: Polarity source: Controls whether we assign polarities via segment-specific or document-wide predictions. HIERNET only allows for documentwide predictions. MILNET can use both. Attention: We use models without gating (no subscript), with gating (gt subscript) as well as models trained with the attention mechanism disabled, falling back to simple averaging (avg subscript). We trained MILNET and HIERNET using Adadelta (Zeiler, 2012) for 25 epochs. Mini-batches of 200 documents were organized based on the reviews' segment and document lengths so the amount of padding was minimized. We used 300-dimensional pre-trained word2vec embeddings. We tuned hyperparameters on the validation sets of the document classification collections, resulting in the following configuration (unless otherwise noted). For the CNN segment encoder, we used window sizes of 3, 4 and 5 words with 100 feature maps per window size, resulting in 300-dimensional segment vectors. The GRU hidden vector dimensions for each direction were set to 50 and the attention vector dimensionality to 100. We used L2-normalization and dropout to regularize the softmax classifiers and additional dropout on the internal GRU connections. Real-valued polarity scores produced by the two models are mapped to discrete labels using two appropriate thresholds t 1 , t 2  [1, 1], so that a segment s is classified as negative if polarity(s) < t 1 , positive if polarity(s) > t 2 or neutral otherwise. 3 To evaluate performance, we use macro-averaged F1 which is unaffected by class imbalance. We select optimal thresholds using 10-fold cross-validation and report mean scores across folds. The fully-supervised convolutional segment classifier (Seg-CNN) uses the same window size and feature map configuration as our segment encoder. Seg-CNN was trained on SPOT using segment labels directly and 10-fold cross-validation (identical folds as in our main models). Seg-CNN is not directly comparable to MILNET (or HIERNET) due to differences in supervision type (segment vs. document labels) and training size (1K-2K segment labels vs. 250K document labels). However, the comparison is indicative of the utility of fine-grained sentiment predictors that do not rely on expensive segment-level annotations.\n",
      "6_Results\n",
      " We evaluated models in two ways. We first assessed their ability to classify segment polarity in reviews using the newly created SPOT dataset and, additionally, the sentence corpora of Kotzias et al. (2015) . Our second suite of experiments focused on opinion extraction: we conducted a judgment elicitation study to determine whether extracts produced by MILNET are useful and of higher quality compared to HIERNET and other baselines. We were also interested to find out whether EDUs provide a better basis for opinion extraction than sentences. Table 3 summarizes our results. The first block in the table reports the performance of the majority class baseline. The second block considers models that do not utilize segment-level predictions, namely HIERNET which assigns polarity scores to segments using its document-level predictions, as well as the variant of MILNET which similarly uses document-level predictions only (Equation (11) ). In the third block, MILNET's segment-level predictions are used. Each block further differentiates between three levels of attention integration, as previ- Table 3 : Segment classification results (in macroaveraged F1).  indicates that the system in question is significantly different from MILNET gt (approximate randomization test (Noreen, 1989) , p < 0.05). ously described. The final block shows the performance of SO-CAL and the Seg-CNN classifier. When considering models that use documentlevel supervision, MILNET with gated, segmentspecific polarities obtains the best classification performance across all four datasets. Interestingly, it performs comparably to Seg-CNN, the fullysupervised segment classifier, which provides additional evidence that MILNET can effectively identify segment polarity without the need for segmentlevel annotations. Our model also outperforms the strong SO-CAL baseline in all but one datasets which is remarkable given the expert knowledge and linguistic information used to develop the latter. Document-level polarity predictions result in lower classification performance across the board. Differences between the standard hierarchical and multiple instance networks are less pronounced in this case, as MILNET loses the advantage of producing segment-specific sentiment predictions. Models without attention perform worse in most cases. The use of gated polarities benefits all model configurations, indicating the method's ability to selectively focus on segments with significant sentiment cues. We further analyzed the polarities assigned by MILNET and HIERNET to positive, negative, and Table 5 : Accuracy scores on the sentence classification datasets introduced in Kotzias et al. (2015) . neutral segments. Figure 6 illustrates the distribution of polarity scores produced by the two models on the Yelp'13 dataset (sentence segmentation). In the case of negative and positive sentences, both models demonstrate appropriately skewed distributions. However, the neutral class appears to be particularly problematic for HIERNET, where polarity scores are scattered across a wide range of values. In contrast, MILNET is more successful at identifying neutral sentences, as its corresponding distribution has a single mode near zero. Attention gating addresses this issue by moving the polarity scores of sentiment-neutral segments towards zero. This is illustrated in Table 4 where we observe that gated variants of both models do a better job at identifying neutral segments. The effect is very significant for HIERNET, while MILNET benefits slightly and remains more effective overall. Similar trends were observed in all four SPOT datasets. In order to examine the effect of training size, we trained multiple models using subsets of the original document collections. We trained on five random subsets for each training size, ranging from 100 documents to the full training set, and tested segment classification performance on SPOT. The results, averaged across trials, are presented in Figure 7 . With the exception of the IMDB EDU-segmented dataset, MILNET only requires a few thousand training documents to outperform the supervised Seg-CNN. HI-ERNET follows a similar curve, but is inferior to MILNET. A reason for MILNET's inferior performance on the IMDB corpus (EDU-split) can be lowquality EDUs, due to the noisy and informal style of language used in IMDB reviews. Finally, we compared MILNET against the GICF model (Kotzias et al., 2015) on their Yelp and IMDB sentence sentiment datasets. 4 Their model requires sentence embeddings from a pre-trained neural model. We used the hierarchical CNN from their work (Denil et al., 2014) and, additionally, pre-trained HIERNET and MILNET sentence embeddings. The results in Table 5 show that MILNET outperforms all variants of GIFC. Our models also seem to learn better sentence embeddings, as they improve GICF's performance on both collections. 4 GICF only handles binary labels, which makes it unsuitable for the full-scale comparisons in Table 3 . Here, we binarize our training datasets and use same-sized sentence embeddings for all four models (R 150 for Yelp, R 72 for IMDB). Informativeness Table 6 : Human evaluation results (in percentages).  indicates that the system in question is significantly different from MILNET (sign-test, p < 0.01). Polarity In our opinion extraction experiments, AMT workers (all native English speakers) were shown an original review and a set of extractive, bullet-style summaries, produced by competing systems using a 30% compression rate. Participants were asked to decide which summary was best according to three criteria: Informativeness (Which summary best captures the salient points of the review?), Polarity (Which summary best highlights positive and negative comments?) and Coherence (Which summary is more coherent and easier to read?). Subjects were allowed to answer \"Unsure\" in cases where they could not discriminate between summaries. We used all reviews from our SPOT dataset and collected three responses per document. We ran four judgment elicitation studies: one comparing HIERNET and MILNET when summarizing reviews segmented as sentences, a second one comparing the two models with EDU segmentation, a third which compares EDU-and sentence-based summaries produced by MILNET, and a fourth where EDU-based summaries from MILNET were compared to a LEAD (the first N words from each document) and a RAN-DOM (random EDUs) baseline. ] As with any family-run hole in the wall, service can be slow. What the staff lacked in speed, they made up for in charm. The food was good, but nothing wowed me. I had the Pierogis while my friend had swedish meatballs. Both dishes were tasty, as were the sides. One thing that was disappointing was that the food was a a little cold (lukewarm). The restaurant itself is bright and clean. I will go back again when i feel like eating outside the box.  ence for MILNET across criteria. The second block shows significant preference for MILNET against HIERNET on informativeness and polarity, whereas HIERNET was more often preferred in terms of coherence, although the difference is not statistically significant. The third block compares sentence and EDU summaries produced by MILNET. EDU summaries were perceived as significantly better in terms of informativeness and polarity, but not coherence. This is somewhat expected as EDUs tend to produce more terse and telegraphic text and may seem unnatural due to segmentation errors. In the fourth block we observe that participants find MIL-NET more informative and better at distilling polarity compared to the LEAD and RANDOM (EDUs) baselines. We should point out that the LEAD system is not a strawman; it has proved hard to outperform by more sophisticated methods (Nenkova, 2005) , particularly on the newswire domain. Example EDU-and sentence-based summaries produced by gated variants of HIERNET and MIL-NET are shown in Figure 8 , with attention weights and polarity scores of the extracted segments shown in round and square brackets respectively. For both granularities, HIERNET's positive document-level prediction results in a single polarity score assigned to every segment, and further adjusted using the corresponding attention weights. The extracted segments are informative, but fail to capture the negative sentiment of some segments. In contrast, MIL-NET is able to detect positive and negative snippets via individual segment polarities. Here, EDU segmentation produced a more concise summary with a clearer grouping of positive and negative snippets.\n",
      "7_Conclusions\n",
      " In this work, we presented a neural network model for fine-grained sentiment analysis within the framework of multiple instance learning. Our model can be trained on large scale sentiment classification datasets, without the need for segment-level labels. As a departure from the commonly used vector-based composition, our model first predicts sentiment at the sentence-or EDU-level and subsequently combines predictions up the document hierarchy. An attention-weighted polarity scoring technique provides a natural way to extract sentimentheavy opinions. Experimental results demonstrate the superior performance of our model against more conventional neural architectures. Human evaluation studies also show that MILNET opinion extracts are preferred by participants and are effective at capturing informativeness and polarity, especially when using EDU segments. In the future, we would like to focus on multi-document, aspect-based extraction (Cao et al., 2017) and ways of improving the coherence of our summaries by taking into account more fine-grained discourse information (Daum III and Marcu, 2002) . 28\n",
      "Fine-Grained_Prediction_of_Syntactic_Typology__Discovering_Latent_Structure_with_Supervised_Learning\n",
      "OUT/Fine-Grained_Prediction_of_Syntactic_Typology__Discovering_Latent_Structure_with_Supervised_Learning/\n",
      "../papers-xmls/Fine-Grained_Prediction_of_Syntactic_Typology__Discovering_Latent_Structure_with_Supervised_Learning.tei.xml\n",
      "1_Introduction\n",
      " Descriptive linguists often characterize a human language by its typological properties. For instance, English is an SVO-type language because its basic clause order is Subject-Verb-Object (SVO), and also a prepositional-type language because its adpositions normally precede the noun. Identifying basic word order must happen early in the acquisition of syntax, and presumably guides the initial interpretation of sentences and the acquisition of a finer-grained grammar. In this paper, we propose a method for doing this. While we focus on word order, one could try similar methods for other typological classifications (syntactic, morphological, or phonological) . The problem is challenging because the language's true word order statistics are computed from syntax trees, whereas our method has access only to a POS-tagged corpus. Based on these POS sequences alone, we predict the directionality of each type of dependency relation. We define the directionality to be a real number in [0, 1]: the fraction of tokens of this relation that are \"right-directed,\" in the sense that the child (modifier) falls to the right of its parent (head). For example, the dobj relation points from a verb to its direct object (if any), so a directionality of 0.9-meaning that 90% of dobj dependencies are right-directed-indicates a dominant verb-object order. (See Table 1 for more such examples.) Our system is trained to predict the relative frequency of rightward dependencies for each of 57 dependency types from the Universal Dependencies project (UD). We assume that all languages draw on the same set of POS tags and dependency relations that is proposed by the UD project (see 3), so that our predictor works across languages. Why do this? Liu (2010) has argued for using these directionality numbers in [0, 1] as fine-grained and robust typological descriptors. We believe that these directionalities could also be used to help define an initializer, prior, or regularizer for tasks like grammar induction or syntax-based machine translation. Finally, the vector of directionalities-or the feature vector that our method extracts in order to predict the directionalities-can be regarded as a language embedding computed from the POStagged corpus. This language embedding may be useful as an input to multilingual NLP systems, such as the cross-linguistic neural dependency parser of Ammar et al. (2016) . In fact, some multilingual NLP systems already condition on typological properties looked up in the World Atlas of Language Structures, or WALS (Dryer and Haspelmath, 2013) , as (Dryer and Haspelmath, 2013) , and how they affect the directionality of Universal Dependencies relations. we review in 8. However, WALS does not list all properties of all languages, and may be somewhat inconsistent since it collects work by many linguists. Our system provides an automatic alternative as well as a methodology for generalizing to new properties. More broadly, we are motivated by the challenge of determining the structure of a language from its superficial features. Principles & Parameters theory (Chomsky, 1981; Chomsky and Lasnik, 1993) famously-if controversially-hypothesized that human babies are born with an evolutionarily tuned system that is specifically adapted to natural language, which can predict typological properties (\"parameters\") by spotting telltale configurations in purely linguistic input. Here we investigate whether such a system can be tuned by gradient descent. It is at least plausible that useful superficial features do exist: e.g., if nouns often precede verbs but rarely follow verbs, then the language may be verb-final.\n",
      "2_Approach\n",
      " We depart from the traditional approach to latent structure discovery, namely unsupervised learning. Unsupervised syntax learners in NLP tend to be rather inaccurate-partly because they are failing to maximize an objective that has many local optima, and partly because that objective does not capture all the factors that linguists consider when assigning syntactic structure. Our remedy in this paper is a supervised approach. We simply imitate how linguists have analyzed other languages. This supervised objective goes beyond the log-likelihood of a PCFGlike model given the corpus, because linguists do not merely try to predict the surface corpus. Their dependency annotations may reflect a cross-linguistic theory that considers semantic interpretability and equivalence, rare but informative phenomena, consistency across languages, a prior across languages, and linguistic conventions (including the choice of latent labels such as dobj). Our learner does not consider these factors explicitly, but we hope it will identify correlates (e.g., using deep learning) that can make similar predictions. Being supervised, our objective should also suffer less from local optima. Indeed, we could even set up our problem with a convex objective, such as (kernel) logistic regression, to predict each directionality separately. Why hasn't this been done before? Our setting presents unusually sparse data for supervised learning, since each training example is an entire language. The world presumably does not offer enough natural languages-particularly with machine-readable corpora-to train a good classifier to detect, say, Object-Verb-Subject (OVS) languages, especially given the class imbalance problem that OVS languages are empirically rare, and the non-IID problem that the available OVS languages may be evolutionarily related. 1 We mitigate this issue by training on the Galactic Dependencies treebanks (Wang and Eisner, 2016) , a collection of more than 50,000 human-like synthetic languages. The treebank of each synthetic language is generated by stochastically permuting the subtrees in a given real treebank to match the word order of other real languages. Thus, we have many synthetic languages that are Object-Verb like Hindi but also Noun-Adjective like French. We know the true directionality of each synthetic language and we would like our classifier to predict that directionality, just as it would for a real language. We will show that our system's accuracy benefits from fleshing out the training set in this way, which can be seen as a form of regularization. A possible criticism of our work is that obtaining the input POS sequences requires human annotators, and perhaps these annotators could have answered the typological classification questions as well. Indeed, this criticism also applies to most work on grammar induction. We will show that our system is at least robust to noise in the input POS sequences ( 7.4). In the future, we hope to devise similar methods that operate on raw word sequences.\n",
      "25\n",
      "Acknowledgements This work was funded by the U.S. National Science Foundation under Grant No. 1423276. We are grateful to the state of Maryland for providing indispensable computing resources via the Maryland Advanced Research Computing Center (MARCC). We thank the Argo lab members for useful discussions. Finally, we thank TACL action editor Mark Steedman and the anonymous reviewers for high-quality suggestions, including the EC baseline and the binary classification evaluation.\n",
      "3_Data\n",
      " We use two datasets in our experiment: UD: Universal Dependencies version 1.2 (et al., 2015) A collection of dependency treebanks for 37 languages, annotated in a consistent style with POS tags and dependency relations. GD: Galactic Dependencies version 1.0 (Wang and Eisner, 2016) A collection of projective dependency treebanks for 53,428 synthetic languages, using the same format as UD. The treebank of each synthetic language is generated from the UD treebank of some real language by stochastically permuting the dependents of all nouns and/or verbs to match the dependent orders of other real UD languages. Using this \"mix-and-match\" procedure, the GD collection fills in gaps in the UD collectionwhich covers only a few possible human languages.\n",
      "4_Task_Formulation\n",
      " We now formalize the setup of the fine-grained typological prediction task. Let R be the set of universal relation types, with N = |R|. We use r  to denote a rightward dependency token with label r  R. Input for language L: A POS-tagged corpus u. (\"u\" stands for \"unparsed.\") Output for language L: Our system predicts p(| r, L), the probability that a token in language L of an r-labeled dependency will be right-oriented. It predicts this for each dependency relation type r  R, such as r = dobj. Thus, the output is a vector of predicted probabilitiesp  [0, 1] N . Training: We set the parameters of our system using a collection of training pairs (u, p * ), each of which corresponds to some UD or GD training language L. Here p * denotes the true vector of probabilities as empirically estimated from L's treebank. Evaluation: Over pairs (u, p * ) that correspond to held-out real languages, we evaluate the expected loss of the predictionsp. We use -insensitive loss 2 with  = 0.1, so our evaluation metric is rR p * (r | L)  loss  (p(| r, L), p * (| r, L)) (1) where  loss  (p, p * )  max(|p  p * |  , 0)  p * (| r, L) = count L ( r ) count L (r) is the empirical estimate of p(| r, L). p(| r, L) is the system's prediction of p * The aggregate metric (1) is an expected loss that is weighted by p * (r | L) = count L (r) r R count L (r ) , to emphasize relation types that are more frequent in L. Why this loss function? We chose an L1-style loss, rather than L2 loss or log-loss, so that the aggregate metric is not dominated by outliers. We took  > 0 in order to forgive small errors: if some predicted directionality is already \"in the ballpark,\" we prefer to focus on getting other predictions right, rather than fine-tuning this one. Our intuition is that errors <  inp's elements will not greatly harm downstream tasks that analyze individual sentences, and might even be easy to correct by grammar reestimation (e.g., EM) that usesp as a starting point. In short, we have the intuition that if our predictedp achieves small loss  on the frequent relation types, thenp will be helpful for downstream tasks, although testing that intuition is beyond the scope of this paper. One could tune  on a downstream task.\n",
      "5_Simple_\"Expected_Count\"_Baseline\n",
      " Before launching into our full models, we warm up with a simple baseline heuristic called expected count (EC), which is reminiscent of Principles & Parameters. The idea is that if ADJs tend to precede nearby NOUNs in the sentences of language L, then amod probably tends to point leftward in L. After all, the training languages show that when ADJ and NOUN are nearby, they are usually linked by amod. Fleshing this out, EC estimates directionalities a p(| r, L) = ecount L ( r ) ecount L ( r ) + ecount L ( r ) (2) ecount L ( r ) = uu 1i<j|u| ji<w p( r | u i , u j ) (3) ecount L ( r ) = uu 1i<j|u| ji<w p( r | u i , u j ) (4) Here u ranges over tag sequences (sentences) of u, and w is a window size that characterizes \"nearby.\" 3 In other words, we ask: given that u i and u j are nearby tag tokens in the test corpus u, are they likely to be linked? Formulas (3)-(4) count such a pair as a \"soft vote\" for r  if such pairs tended to be linked by r  in the treebanks of the training languages, 4 and a \"soft vote\" for r  if they tended to be linked by r . Training: For any two tag types t, t in the universal POS tagset T , we simply use the training treebanks to get empirical estimates of p( | t, t ), taking p( r | t, t ) = L s L  count L (t r  t ) L s L  count L (t, t ) (5) and similarly for p( r | t, t ). This can be interpreted as the (unsmoothed) fraction of (t, t ) within a wword window where t is the r-type parent of t , computed by micro-averaging over languages. To get a fair average over languages, equation (5) downweights the languages that have larger treebanks, yielding a weighted micro-average in which we define the weight s L = 1/ tT ,t T count L (t, t ). As we report later in Table 5 , even this simple supervised heuristic performs significantly better than state-of-the-art grammar induction systems. However, it is not a trained heuristic: it has no free parameters that we can tune to optimize our evaluation metric. For example, it can pay too much attention to tag pairs that are not discriminative. We therefore proceed to build a trainable, feature-based system.\n",
      "6_Proposed_Model_Architecture\n",
      " To train our model, we will try to minimize the evaluation objective (1) averaged over the training lan-3 In our experiment, we chose w = 8 by cross-validation over w = 2, 4, 8, 16, . 4 Thus, the EC heuristic examines the correlation between relations and tags in the training treebanks. But our methods in the next section will follow the formalization of 4: they do not examine a training treebank beyond its directionality vector p * . logistic Figure 1 : Basic predictive architecture from equations (6)-(7). bW and bV are suppressed for readability. guages, plus a regularization term given in 6.4. 5 Our predicted directionality for relation r will b p(| r, L) = 1/(1 + exp(s(u) r )) (6) s(u) is a parametric function (see 6.2 below) that maps u to a score vector in R N . Relation type r should get positive or negative score according to whether it usually points right or left. The formula above converts each score to a directionalitya probability in (0, 1)-using a logistic transform. To score all dependency relation types given the corpus u, we use a feed-forward neural network with one hidden layer ( Figure 1 ): s(u) = V (W (u) + b W ) + b V (7) (u) extracts a d-dimensional feature vector from the corpus u (see 6.3 below). W is a h  d ma- trix that maps (u) into a h-dimensional space and b W is a h-dimensional bias vector.  is an elementwise activation function. V is a N  h matrix whose rows can be regarded as learned embeddings of the dependency relation types. b V is a N -dimensional bias vector that determines the default rightwardness of each relation type. We give details in 7.5. The hidden layer (W (u) + b W ) can be regarded as a latent representation of the language's word order properties, from which potentially correlated predictionsp are extracted. Our current feature vector (u) considers only the POS tag sequences for the sentences in the unparsed corpus u. Each sentence is augmented with a special boundary tag # at the start and end. We explore both hand-engineered features and neural features. Hand-engineered features. Recall that 5 considered which tags appeared near one another in a given order. We now devise a slew of features to measure such co-occurrences in a variety of ways. By training the weights of these many features, our system will discover which ones are actually predictive. Let g(t | j)  [0, 1] be some measure (to be defined shortly) of the prevalence of tag t near token j of corpus u. We can then measure the prevalence of t, both overall and just near tokens of tag s: 6  t = mean j g(t | j) (8)  t|s = mean j: T j =s g(t | j) (9) where T j denotes the tag of token j. We now define versions of these quantities for particular prevalence measures g. Given w > 0, let the right window W j denote the sequence of tags T j+1 , . . . , T j+w (padding this sequence with additional # symbols if it runs past the end of j's sentence). We define quantities  w t|s and  w t via (8)-(9), using a version of g(t | j) that measures the fraction of tags in W j that equal t. Also, for b  {1, 2}, we define  w,b t|s and  w,b t using a version of g(t | j) that is 1 if W j contains at least b tokens of t, and 0 otherwise. For each of these quantities, we also define a corresponding mirror-image quantity (denoted by negating w > 0) by computing the same feature on a reversed version of the corpus. We also define \"truncated\" versions of all quantities above, denoted by writingover the w. In these, we use a truncated window j , obtained from W j by removing any suffix that starts with # 6 In practice, we do backoff smoothing of these means. This avoids subsequent division-by-0 errors if tag t or s has count 0 in the corpus, and it regularizes  t|s /t toward 1 if t or s is rare. Specifically, we augment the denominators by adding , while augmenting the numerator in (8) by adding   meanj,t g(t | j) (unsmoothed) and the numerator in (9) by adding  times the smoothed t from (8).  > 0 is a hyperparameter (see 7.5).  or with a copy of tag T j (that is, s). 7 As an example, 8 ,2 N|V asks how often a verb is followed by at least 2 nouns, within the next 8 words of the sentence and before the next verb. A high value of this is a plausible indicator of a VSO-type or VOS-type language. We include the following features for each tag pair s, t and each w  {1, 3, 8, 100, 1, 3, 8, 100,1,3,8,1 00, 1, 3, 8, 1 00}: 8  w t ,  w t|s ,  w t|s   w s ,  w t|s // w t ,  w t // w t|s ,  w t|s // w t|s where we define x//y = min(x/y, 1) to prevent unbounded feature values, which can result in poor generalization. Notice that for w = 1,  w t|s is bigram conditional probability,  w t|s  w s is bigram joint probability, the log of  w t|s / w t is bigram pointwise mutual information, and  w t|s / w t|s measures how much more prevalent t is to the right of s than to the left. By also allowing other values of w, we generalize these features. Finally, our model also uses versions of these features for each b  1, 2. Neural features. As an alternative to the manually designed  function above, we consider a neural approach to detect predictive configurations in the sentences of u, potentially including complex longdistance configurations. Linguists working with Principles & Parameters theory have supposed that a single telltale sentence-a trigger-may be enough to determine a typological parameter, at least given the settings of other parameters (Gibson and Wexler, 1994; Frank and Kapur, 1996) . We map each corpus sentence u i to a finitedimensional real vector f i by using a gated recurrent unit (GRU) network (Cho et al., 2014) , a type of recurrent neural network that is a simplified variant of an LSTM network (Hochreiter and Schmidhuber, 1997) . The GRU reads the sequence of one-hot embeddings of the tags in u i (including the boundary symbols #). We omit the part of the GRU that computes an output sequence, simply taking f i to be the final hidden state vector. The parameters are trained jointly with the rest of our typology prediction system, so the training procedure attempts to discover predictively useful configurations. The various elements of f i attempt to detect various interesting configurations in sentence u i . Some might be triggers (which call for max-pooling over sentences); others might provide softer evidence (which calls for mean-pooling). For generality, therefore, we define our feature vector (u) by softpooling of the sentence vectors f i (Figure 2 ). The tanh gate in the GRU implies f ik  (1, 1) and we transform this to the positive quantity f ik = f ik +1 2  (0, 1). Given an \"inverse temperature\" , define 9   k = mean i (f ik )  1/ (10) This   k is a pooled version of f ik , ranging from max-pooling as    (i.e., does f ik fire strongly on any sentence i?) to min-pooling as   . It passes through arithmetic mean at  = 1 (i.e., how strongly does f ik fire on the average sentence i?), geometric mean as   0 (this may be regarded as an arithmetic mean in log space), and harmonic mean at  = 1 (an arithmetic mean in reciprocal space). Our final  is a concatenation of the   vectors for   {4, 2, 1, 0, 1, 2, 4}. We chose these  values experimentally, using cross-validation. Combined model. We also consider a model s(u) =  s H (u) + (1  ) s N (u) (11) where s H (u) is the score assigned by the handfeature system, s N (u) is the score assigned by the 9 For efficiency, we restrict the mean to i  1e4 (the first 10,000 sentences). neural-feature system, and   [0, 1] is a hyperparameter to balance the two. s H (u) and s N (u) were trained separately. At test time, we use (11) to combine them linearly before the logistic transform (6). This yields a weighted-product-of-experts model. Length thresholding. By default, our feature vector (u) is extracted from those sentences in u with length  40 tokens. In 7.3, however, we try concatenating this feature vector with one that is extracted in the same way from just sentences with length  10. The intuition (Spitkovsky et al., 2010) is that the basic word order of the language can be most easily discerned from short, simple sentences. Initialization. We initialize the model of (6)- (7) so that the estimated directionalityp(| r, L), regardless of L, is initially a weighted mean of r's directionalities in the training languages, namel p r  L w L (r) p * (| r, L) (12) where w L (r)  p * (r|L) L p * (r|L ) (13) This is done by setting V = 0 and the bias (b V ) r = logp r 1pr , clipped to the range [10, 10]. As a result, we make sensible initial predictions even for rare relations r, which allows us to converge reasonably quickly even though we do not update the parameters for rare relations as often. We initialize the recurrent connections in the GRU to random orthogonal matrices. All other weight matrices in Figure 1 and the GRU use \"Xavier initialization\" (Glorot and Bengio, 2010) . All other bias weight vectors are initialized to 0. Regularization. We add an L2 regularizer to the objective. When training the neural network, we use dropout as well. All hyperparameters (regularization coefficient, dropout rate, etc.) are tuned via crossvalidation; see 7.5. Optimization. We use different algorithms in different feature settings. With scoring functions that use only hand features, we adjust the feature weights by stochastic gradient descent (SGD). With scoring functions that include neural features, we use RMSProp (Tieleman and Hinton, 2012) . Wang and Eisner (2016) . (Our \"Train,\" on which we do 5-fold crossvalidation, contains both their \"Train\" and \"Dev\" languages.)\n",
      "7_Experiments\n",
      "  We hold out 17 UD languages for testing (Table 2) . For training, we use the remaining 20 UD languages and tune the hyperparameters with 5-fold crossvalidation. That is, for each fold, we train the system on 16 real languages and evaluate on the remaining 4. When augmenting the 16 real languages with GD languages, we include only GD languages that are generated by \"mixing-and-matching\" those 16 languages, which means that we add 16  17  17 = 4624 synthetic languages. 10 Each GD treebank u provides a standard split into train/dev/test portions. In this paper, we primarily restrict ourselves to the train portions (saving the gold trees from the dev and test portions to tune and evaluate some future grammar induction system that consults our typological predictions). For example, we write u train for the POS-tagged sentences in the \"train\" portion, and p * train for the empirical probabilities derived from their gold trees. We always train the model to predict p * train from u train on each training language. To evaluate on a held-out language during cross-validation, we can measure how well the model predicts p * train given u train . 11 For our fi-10 Why 161717? As Wang and Eisner (2016, 5) explain, each GD treebank is obtained from the UD treebank of some substrate language S by stochastically permuting the dependents of verbs and nouns to respect typical orders in the superstrate languages RV and RN respectively. There are 16 choices for S. There are 17 choices for RV (respectively RN), including RV = S (\"self-permutation\") and RV =  (\"no permutation\"). 11 In actuality, we measured how well it predicts p * dev given udev. That was a slightly less sensible choice. It may have harmed our choice of hyperparameters, since dev is smaller than train and therefore p * dev tends to have greater sampling error. Another concern is that our typology system, having been specifically tuned to predict p * dev , might provide an unrealistically accurate estimate of p * dev to some future grammar induction system that is being cross-validated against the same dev set, harming that system's choice of hyperparameters as well. Table 3 shows the cross-validation losses (equation (1)) that are achieved by different scoring architectures. We compare the results when the model is trained on real languages (the \"UD\" column) versus on real languages plus synthetic languages (the \"+GD\" column). The s H models here use a subset of the handengineered features, selected by the experiments in 7.3 below and corresponding to Table 4 line 8. Although Figure 1 and equation (7) presented an \"depth-1\" scoring network with one hidden layer, Table 3 also evaluates \"depth-d\" architectures with d hidden layers. The depth-0 architecture simply predicts each directionality separately using logistic regression (although our training objective is not the usual convex log-likelihood objective). Some architectures are better than others. We note that the hand-engineered features outperform the neural features-though not significantly, since they make complementary errors-and that combining them is best. However, the biggest benefit comes from augmenting the training data with GD languages; this consistently helps more than changing the architecture. Table 4 : Cross-validation losses with different subsets of handengineered features from 6.3. \"\" is a baseline with no features (bias feature only), so it makes the same prediction for all languages. \"conditional\" =  w t|s features, \"joint\" =  w t|s   w s features, \"PMI\" =  w t|s // w t and  w t // w t|s features, \"asymmetry\" =  w t|s // w t|s features, \"b\" are the features superscripted by b, and \"t\" are the features with truncated window. \"+\" means concatenation.The \"Length\" field refers to length thresholding (see 6.4). The system in the starred row is the one that we selected for row 2 of Table 3 . To understand the contribution of different handengineered features, we performed forward selection tests on the depth-1 system, including only some of the features. In all cases, we trained in the \"+GD\" condition. The results are shown in Table 4. Any class of features is substantially better than baseline, but we observe that most of the total benefit can be obtained with just PMI or asymmetry features. Those features indicate, for example, whether a verb tends to attract nouns to its right or left. We did not see a gain from length thresholding. We also tested our directionality prediction system on noisy input (without retraining it on noisy input). Specifically, we tested the depth-1 s H system. This time, when evaluating on the dev split of a held-out language, we provided a noisy version of that input corpus that had been retagged by an automatic POS tagger (Nguyen et al., 2014) , which was trained on just 100 gold-tagged sentences from the train split of that language. The average tagging accuracy over the 20 languages was only 77.26%. Nonetheless, the \"UD\"-trained and \"+GD\"-trained systems got respective losses of 0.052 and 0.041-nearly as good as in Table 3 , which used gold POS tags. EC  UD +GD loss 0.166 0.139 0.098 0.083 0.080 0.039 Table 5 : Cross-validation average expected loss of the two grammar induction methods, MS13 (Mareek and Straka, 2013) and N10 (Naseem et al., 2010) , compared to the EC heuristic of 5 and our architecture of 6 (the version from the last line of Table 3 ). In these experiments, the dependency relation types are ordered POS pairs. N10 harnesses prior linguistic knowledge, but its improvement upon MS13 is not statistically significant. Both grammar induction systems are significantly worse than the rest of the systems, including even our two baseline systems, namely EC (the \"expected count\" heuristic from 5) and  (the no-feature baseline system from Table 4 line 0). Like N10, these baselines make use of some cross-linguistic knowledge, which they extract in different ways from the training treebanks. Among our own 4 systems, EC is significantly worse than all others, and +GD is significantly better than all others. (Note: When training the baselines, we found that including the +GD languages-a bias-variance tradeoff-harmed EC but helped . The table reports the better result in each case.) For each result in Tables 3-4, the hyperparameters were chosen by grid search on the cross-validation objective (and the table reports the best result). For the remaining experiments, we select the depth-1 combined models (11) for both \"UD\" and \"+GD,\" as they are the best models according to Table 3 . The hyperparameters for the selected models are as follows: When training with \"UD,\" we took  = 1 (which ignores s N ), with hidden layer size h = 256,  = sigmoid, L2 coeff = 0 (no L2 regularization), and dropout = 0.2. When training with \"+GD,\" we took  = 0.7, with different hyperparameters for the two interpolated models: s H uses h = 128,  = sigmoid, L2 coeff = 0, and dropout = 0.4, while s N uses h = 128, emb size = 128, rnn size = 32,  = relu, L2 coeff = 0, and dropout = 0.2. For both \"UD\" and \"+GD\", we use  = 1 for the smoothing in footnote 6. Grammar induction is an alternative way to predict word order typology. Given a corpus of a language, we can first use grammar induction to parse it into dependency trees, and then estimate the directionality of each dependency relation type based on these (approximate) trees. However, what are the dependency relation types? Current grammar induction systems produce unlabeled dependency edges. Rather than try to obtain 154 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 LHeldout wL(r) loss(pdev(|r, L), p * dev (|r, L)) (see (13) ). a UD label like r = amod for each edge, we label the edge deterministically with a POS pair such as r = (parent = NOUN, child = ADJ). Thus, we will attempt to predict the directionality of each POS-pair relation type. For comparison, we retrain our supervised system to do the same thing. For the grammar induction system, we try the implementation of DMV with stop-probability estimation by Mareek and Straka (2013) , which is a common baseline for grammar induction (Le and Zuidema, 2015) because it is language-independent, reasonably accurate, fast, and convenient to use. We also try the grammar induction system of Naseem et al. (2010) , which is the state-of-the-art system on UD (Noji et al., 2016) . Naseem et al. (2010) 's method, like ours, has prior knowledge of what typical human languages look like. Table 5 shows the results. Compared to Mareek and Straka (2013) , Naseem et al. (2010) gets only a small (insignificant) improvement-whereas our \"UD\" system halves the loss, and the \"+GD\" system halves it again. Even our baseline systems are significantly more accurate than the grammar induction systems, showing the effectiveness of casting the problem as supervised prediction. Beyond reporting the aggregate cross-validation loss over the 20 training languages, we break down the cross-validation predictions by relation type. Figure 4 : The y coordinate is the average loss of our model (Table 4 line 8), just as in Figure 3 , whereas the x coordinate is the average loss of a simple baseline model  that ignores the input corpus (Table 4 line 0). Relations whose directionality varies more by language have higher baseline loss. Relations that beat the baseline fall below the diagonal line. The marker size for each relation is proportional to the x-axis in Figure 3 . predictable. Figure 4 shows that our success is not just because the task is easy-on relations whose directionality varies by language, so that a baseline method does poorly, our system usually does well. To show that our system is behaving well across languages and not just on average, we zoom in on 5 relation types that are particularly common or of particular interest to linguistic typologists. These 5 relations together account for 46% of all relation tokens in the average language: nmod = noun-nominal modifier order, nsubj = subject-verb order (feature 82A in the World Atlas of Language Structures), dobj = object-verb order (83A), amod = adjectivenoun order (87A), and case = placement of both adpositions and case markers (85A). As shown in Figure 5 , most points in the first five plots fall in or quite near the desired region. We are pleased to see that the predictions are robust when the training data is unbalanced. For example, the case relation points leftward in most real languages, yet our system can still predict the right directionality of \"hi\", \"et\" and \"fi.\" The credit goes to the diversity of our training set, which contains various synthetic case-right languages: the system fails on these three languages if we train on real languages only. That said, apparently our training set is still not diverse enough to do well on the outlier \"ar\" (Arabic); see Figure 4 in Wang and Eisner (2016) . Figure 5 : Scatterplots of predicted vs. true directionalities (by cross-validation). In the plot for relation type r, each language appears as a marker at (p * ,p) (see 4), with the marker size proportional to wL(r) (see (13) ). Points that fall between the solid lines (|p  p * |  ) are considered \"correct,\" by the definition of -insensitive loss. The last plot (bottom right) shows worse predictions for case when the model is trained on UD only. Besides -insensitive loss, we also measured how the systems perform on the coarser task of binary classification of relation direction. We say that relation r is dominantly \"rightward\" in language L iff p * (| r, L) > 0.5. We say that a system predicts \"rightward\" according to whetherp(| r, L) > 0.5. We evaluate whether this binary prediction is correct for each of the 20 most frequent relations r, for each held-out language L, using 5-fold crossvalidation over the 20 training languages L as in the previous experiment. Tables 6 and 7 respectively summarize these results by relation (equal average Table 6 : Accuracy on the simpler task of binary classification of relation directionality. The most common relations are shown first: the \"Rate\" column gives the average rate of the relation across the 20 training languages (like the x coordinate in Fig. 3 ). over languages) and by language (equal average over relations). Keep in mind that these systems had not been specifically trained to place relations on the correct side of the artificial 0.5 boundary. Binary classification is an easier task. It is easy because, as the  column in Table 6 indicates, most relations have a clear directionality preference shared by most of the UD languages. As a result, the better models with more features have less opportunity to help. Nonetheless, they do perform better, and the EC heuristic continues to perform worse. In particular, EC fails significantly on dobj and iobj. This is because nsubj, dobj, and iobj often have different directionalities (e.g., in SVO languages), but the EC heuristic will tend to predict the same direction for all of them, according to whether NOUNs tend to precede nearby VERBs. All previous experiments were conducted by crossvalidation on the 20 training languages. We now train the system on all 20, and report results on the 17 blind test languages ( Table 7 : Accuracy on the simpler task of binary classification of relation directionality for each training language. A detailed comparison shows that EC is significantly worse than UD and +GD, and that  is significantly worse than +GD (paired permutation test over the 20 languages, p < 0.05). The improvement from UD to +GD is insignificant, which suggests that this is an easier task where weak models might suffice. relations that appear only in test data. The results range from good to excellent, with synthetic data providing consistent and often large improvements. These results could potentially be boosted in the future by using an even larger and more diverse training set. In principle, when evaluating on any one of our 37 real languages, one could train a system on all of the other 36 (plus the galactic languages derived from them), not just 20. Moreover, the Universal Dependencies collection has continued to grow beyond the 37 languages used here ( 3). Finally, our current setup extracts only one training example from each (real or synthetic) language. One could easily generate a variant of this example each time the language is visited during stochastic optimization, by bootstrap-resampling its training corpus (to add \"natural\" variation) or subsampling it (to train the predictor to work on smaller corpora). In the case of a synthetic language, one could also generate a corpus of new trees each time the language is visited (by re-running the stochastic permutation procedure, instead of reusing the particular permutation released by the Galactic Dependencies project). Table 8 : Our final comparison on the 17 test languages appears at left. We ask whether the average expected loss on these 17 real target languages is reduced by augmenting the training pool of 20 UD languages with +20*21*21 GD languages. For completeness, we extend the table with the cross-validation results on the training pool. The \"Avg.\" lines report the average of 17 test or 37 training+testing languages. We mark both \"+GD\" averages with \"*\" as they are significantly better than their \"UD\" counterparts (paired permutation test by language, p < 0.05).\n",
      "8_Related_Work\n",
      " Typological properties can usefully boost the performance of cross-linguistic systems (Bender, 2009; O'Horan et al., 2016) . These systems mainly aim to annotate low-resource languages with help from models trained on similar high-resource languages. Naseem et al. (2012) introduce a \"selective sharing\" technique for generative parsing, in which a Subject-Verb language will use parameters shared with other Subject-Verb languages. Tckstrm et al. (2013) and Zhang and Barzilay (2015) extend this idea to discriminative parsing and gain further improvements by conjoining regular parsing features with typological features. The cross-linguistic neural parser of Ammar et al. (2016) conditions on typological features by supplying a \"language embedding\" as input. Zhang et al. (2012) use typological properties to convert language-specific POS tags to UD POS tags, based on their ordering in a corpus. Moving from engineering to science, lin-guists seek typological universals of human language (Greenberg, 1963; Croft, 2002; Song, 2014; Hawkins, 2014) , e.g., \"languages with dominant Verb-Subject-Object order are always prepositional.\" Dryer and Haspelmath (2013) characterize 2679 world languages with 192 typological properties. Their WALS database can supply features to NLP systems (see previous paragraph) or gold standard labels for typological classifiers. Daum III and Campbell (2007) take WALS as input and propose a Bayesian approach to discover new universals. Georgi et al. (2010) impute missing properties of a language, not by using universals, but by backing off to the language's typological cluster. Murawaki (2015) use WALS to help recover the evolutionary tree of human languages; Daum III (2009) considers the geographic distribution of WALS properties. Attempts at automatic typological classification are relatively recent. Lewis and Xia (2008) predict typological properties from induced trees, but guess those trees from aligned bitexts, not by monolingual grammar induction as in 7.6. Liu (2010) and Futrell et al. (2015) show that the directionality of (gold) dependencies is indicative of \"basic\" word order and freeness of word order. Those papers predict typological properties from trees that are automatically (noisily) annotated or manually (expensively) annotated. An alternative is to predict the typology directly from raw or POS-tagged text, as we do. Saha Roy et al. (2014) first explored this idea, building a system that correctly predicts adposition typology on 19/23 languages with only word cooccurrence statistics. Zhang et al. (2016) evaluate semi-supervised POS tagging by asking whether the induced tag sequences can predict typological properties. Their prediction approach is supervised like ours, although developed separately and trained on different data. They more simply predict 6 binaryvalued WALS properties, using 6 independent binary classifiers based on POS bigram and trigrams. Our task is rather close to grammar induction, which likewise predicts a set of real numbers giving the relative probabilities of competing syntactic configurations. Most previous work on grammar induction begins with maximum likelihood estimation of some generative model-such as a PCFG (Lari and Young, 1990; Carroll and Charniak, 1992) or dependency grammar (Klein and Manning, 2004 )-though it may add linguistically-informed inductive bias (Ganchev et al., 2010; Naseem et al., 2010) . Most such methods use local search and must wrestle with local optima (Spitkovsky et al., 2013) . Finegrained typological classification might supplement this approach, by cutting through the initial combinatorial challenge of establishing the basic wordorder properties of the language. In this paper we only quantify the directionality of each relation type, ignoring how tokens of these relations interact locally to give coherent parse trees. Grammar induction methods like EM could naturally consider those local interactions for a more refined analysis, when guided by our predicted global directionalities.\n",
      "9_Conclusions_and_Future_Work\n",
      " We introduced a typological classification task, which attempts to extract quantitative knowledge about a language's syntactic structure from its surface forms (POS tag sequences). We applied supervised learning to this apparently unsupervised problem. As far as we know, we are the first to utilize synthetic languages to train a learner for real languages: this move yielded substantial benefits. 12 Figure 5 shows that we rank held-out languages rather accurately along a spectrum of directionality, for several common dependency relations. Table 8 shows that if we jointly predict the directionalities of all the relations in a new language, most of those numbers will be quite close to the truth (low aggregate error, weighted by relation frequency). This holds promise for aiding grammar induction. Our trained model is robust when applied to noisy POS tag sequences. In the future, however, we would like to make similar predictions from raw word sequences. That will require features that abstract away from the language-specific vocabulary. Although recurrent neural networks in the present paper did not show a clear advantage over handengineered features, they might be useful when used with word embeddings. Finally, we are interested in downstream uses. Several NLP tasks have benefited from typological features ( 8). By using end-to-end training, our methods could be tuned to extract features (existing or novel) that are particularly useful for some task.\n",
      "Sequence_Effects_in_Crowdsourced_Annotations\n",
      "OUT/Sequence_Effects_in_Crowdsourced_Annotations/\n",
      "../papers-xmls/Sequence_Effects_in_Crowdsourced_Annotations.tei.xml\n",
      "1_Introduction\n",
      " NLP research relies heavily on annotated datasets for training and evaluation. The design of the annotation task can influence the decisions made by annotators in subtle ways: besides the actual features of the instance being annotated, annotators are also influenced by factors such as the user interface, wording of the question, and familiarity with the task or domain. When collecting NLP annotations, care is usually taken to ensure that the annotations are of high quality, through careful design of label sets, annotation guidelines and training of annotators (Hovy et al., 2006) , methods for aggregating annotations (Passonneau and Carpenter, 2014) , and intuitive user interfaces (Stenetorp et al., 2012) . Crowdsourcing has emerged as a cheaper, faster alternative to expert NLP annotations (Snow et al., 2008; Callison-Burch and Dredze, 2010; Graham et al., 2017) , although it entails additional effort to filter out unskilled or opportunistic workers, e.g. through the collection of redundant repeated judgements for each instance, or including some trap questions with known answers (Callison-Burch and Dredze, 2010; Hofeld et al., 2014) . In most annotation exercises, the order of presentation of instances is randomised to remove bias due to similarities in topic, style and vocabulary (Koehn and Monz, 2006; Bojar et al., 2016) . When crowdsourcing judgements, the normal practise (as used in the datasets we analyse) is for the item ordering to be randomised in creating a \"HIT\" (i.e. a single collection of items presented to a crowdworker for judgement), and then to have each HIT annotated by multiple workers, for quality control purposes. The order of items is generally fixed across all annotators of an individual HIT (Snow et al., 2008; Graham et al., 2017) . In this paper, we show that worker scores are affected by sequence bias, whereby the order of presentation can affect individuals' assessment of an item. Since all workers see the instances in the same order, this affects any other inferences made from the data, including aggregated assessment or inferences about individual annotators (such as their overall quality or individual thresholds). Possible explanations for sequence effects include: Gambler's fallacy: Once annotators have developed an idea of the distribution of scores/labels, they can come to expect even small sequences to follow the distribution. In particular, in binary annotation tasks, if they expect that True (1) and False (0) items are equally likely, then they believe the sequence 00000 (100% False and 0% True) is less likely than the sequence 01010 (50% False and 50% True). So if they assign 0 to an item, they may approach the next item with a prior belief that it is more likely to be a 1 than a 0. Chen et al. (2016) showed evidence for the gambler's fallacy in decisions of loan officers, asylum judges, and baseball umpires. Sequential contrast effects: A high quality item may raise the bar for the next item. On the other hand, a bad item may make the next item seem better in comparison (Kenrick and Gutierres, 1980; Hartzmark and Shue, to appear) Assimilation and anchoring: The annotator uses their score of the previous item as an anchor, and adjusts the score of the current item from this anchor, based on perceived similarities and differences with the previous item. If they focus on similarities between the previous and current instance, the annotations show an assimilation effect (Geiselman et al., 1984; Damisch et al., 2006) . Anchoring effects may decrease as people gain experience and expertise in the task (Wilson et al., 1996) .\n",
      "10\n",
      "Acknowledgments We thank the anonymous reviewers for their valuable feedback. This work was supported in part by the Australian Research Council.\n",
      "2_Methodology\n",
      " We test whether the annotation of an instance is correlated with the annotation on previous instances, conditioned on control variables such as the gold standard (i.e. expert annotations 1 ), based on the following linear model: Y i,t =  0 +  1 Y i,t1 +  2 Gold +  (1) where Y i,t is the annotation given by an annotator i to an instance t, and  is white Gaussian noise with zero mean. We use linear regression for continuous data and logistic regression for binary data. 2 If there is no dependence between consecutive instances, and annotators assign labels/scores based only on the aspects of the current instance, then the data can be explained from the gold score (learning a positive  2 value) and bias term ( 0 ), with  1 set to zero. When we use the ground truth as a control, if  1 is non-zero, it is evidence of mistakes being made by annotators due to sequential bias. A positive value of  1 can be explained by priming or anchoring, and a negative value with sequential contrast effects or the gambler's fallacy. Accordingly, we test the statistical significance of 1 For the Machine Translation dataset described in Section 3.3, we use the mean of at least fifteen crowd workers as a proxy for expert annotations. 2  is not included in the case of logistic regression Table 1 : Autocorrelation coefficient  1 for RTE and TEMPORAL data. Stars denote statistical significance: * = 0.05, * * = 0.01, and * * * = 0.001. Task All Good Moderate RTE 0.102 0.169 * 0.192 * * TEMPORAL 0.198 0.567 * * * 0.511 * * * the  1 = 0 to determine whether sequencing effects are present in crowdsourced text corpora.\n",
      "3_Experiments\n",
      " We analyse several influential datasets that have been constructed through crowdsourcing, including both binary and continuous annotation tasks: recognising textual entailment, event ordering, affective text analysis, and machine translation evaluation. First, we examine the recognising textual entailment (\"RTE\") and event temporal ordering (\"TEMPORAL\") datasets from Snow et al. (2008) . In the RTE task, annotators are presented with two sentences, and are asked to judge whether the second text can be inferred from the first. With the TEMPORAL dataset, they are shown two sentences describing events, and asked to indicate which of the two events occurred first. Both datasets include both expert annotations and crowdsourced annotations constructed using Amazon Mechanical Turk (\"MTurk\"). On MTurk, each RTE HIT contains 20 instances, and each TEMPORAL HIT contains 10 instances, which the workers see in sequential order. For both tasks, each HIT was annotated by 10 workers. We use logistic regression on worker labels against labels on the previous instance in the current HIT, with the expert judgements as a control variable. We also add an additional control, namely the percentage of True labels assigned by the worker overall, which accounts for the overall annotator bias. To calculate this, we use scores by the worker excluding the current score, to avoid giving the model any information about the current instance. As shown in Table 1 , over all workers (\"All\"), we find a small negative autocorrelation for both the RTE and TEMPORAL tasks. One possibility is that this is biased by opportunistic workers who assign the same label to all instances in the HIT, for which we would not expect any sequential bias effects. When we exclude these workers (\"Moderate\"), the autocorrelation increases, and is highly statistically significant. We also show results for workers with at least 60% accuracy when compared to expert annotations (\"Good\"), and observe a similar effect. In the affective text analysis task (\"AFFECTIVE\"), annotators are asked to rate news headlines for anger, disgust, fear, joy, sadness, and surprise on a continuous scale of 0-100. Besides these emotions, they are asked to rate sentences for (emotive) valence, i.e., how strongly negative or positive they are (100 to +100). In this dataset, there are 100 headlines divided into 10 HITs, with 10 workers annotating each HIT (Snow et al., 2008) . We test for autocorrelation of scores of each aspect individually, controlling for the expert scores and worker correlation with the expert scores. We also look separately at datasets of good and bad workers, based on whether the correlation with the expert annotations is greater than 0.5. For individual emotions, we do not observe any significant autocorrelation (p  0.05). As there are only 1000 annotations per emotion, we also look at results when combining data for all aspects. Though we find a statistically significant negative autocorrelation for scores of the full dataset, this disappears when we filter out bad workers (Table 2) . Given the difficulty of this very subjective task, it is likely that many of workers considered 'bad' might have simply found this task too difficult or arbitrary, and thus become more prone to sequence effects. When evaluating machine translation (\"MT\"), we tend to focus on adequacy: the extent to which the meaning of the reference translation is captured in the MT output. In the method of Graham et al. (2015) -the current best-practise, as adopted by WMT (Bojar et al., 2016 ) -annotators are asked to judge the adequacy of translations using a 100point sliding scale which is initialised at the mid point. There are 3 marks on the scale dividing it into 4 quarters to aid workers with internal calibration. They are given no other instructions or  guidelines. In this paper, we base our analysis on the adequacy dataset of Graham et al. (2015) , on Spanish-English newswire data from WMT 2013 (Bojar et al., 2013) . The dataset consists of 12 HITS of 100 sentence pairs each; each HIT is annotated by at least 15 workers. HITs are designed to include quality control items to filter out poor quality scores. In addition to 70 MT system translations, each HIT contains degraded versions of 10 of these translations, 10 reference translations by a human expert corresponding to 10 of these translations, and repeats of another 10 translations. Good workers are assumed to give high scores to the references, similar scores to the pair of repeats, and high scores to the MT system translations when compared to corresponding degraded translations. Workers who submitted scores of clearly bad quality were rejected. For the remaining workers, the Wilcoxon rank-sum test is used to test whether the score difference between the repeat judgements is less than the score difference between translations and the corresponding degraded versions. We divide these workers into \"good\" and \"moderate\" based on the threshold of p < 0.05. To eliminate differences due to different internal scales, every individual worker's scores are standardised by subtracting the mean and dividing by the standard deviation of their scores. Following Graham et al. (2015) , we use the average of standardised scores of at least 15 good workers as the ground truth. We refer to the final dataset as \"MT adeq \". Results As this is a (practically) continuous output, we use a linear regression model, whereby the current score is predicted based on the previous score, with the mean of all worker scores as control. We also controlled for worker correlation with mean score, and position of the sentence in the HIT, but these were not significant and did not affect the autocorrelation. As seen in Table 3 , we see a small but significant positive autocorrelation for good workers. The bias is much stronger with  1st Tertile 0.044 * * * 0.063 * * * 0.179 * * * 2nd Tertile 0.032 * * * 0.034 * * * 0.173 * * * 3rd Tertile 0.015 * * 0.014 * 0.225 * * * Table 4 : MT adeq dataset: Regression coefficient  1 of adequacy scores with the previous score. We also show results for translations in the first, second or third tertile based on the position of the sentence of the HIT bad (rejected) workers. An interesting question is whether the bias changes as workers annotate more data, which could be ascribed to learning through the task, calibrating their internal scales, or becoming fatigued on a monotonous task. Each HIT consists of 100 sentences, and we divide the dataset into 3 equal groups based on the position of sentence in the HIT. As shown in Table 4 , for good and moderate workers, the bias is stronger in the first group of sentences annotated, decreases in the second, and is much smaller in the last. This could be because workers are familiarising themselves with the task earlier on, and calibrating their scale. There is no such trend with bad quality scores, possibly because the workers are not putting in sufficient effort to produce accurate scores. Next we assess the impact of the bias in the worst case situation. We discretize scores into low, middle and high based on equal-frequency binning, and divide the dataset into 3 groups based on the score assigned to the previous sentence. As shown in Table 5 we can see that the sentences in the \"low\" partition and the \"high\" partition have a difference of 0.18, which is highly significant; 3 moreover, this difference is likely to be sufficiently large to alter the rankings of systems in an evaluation. The bias remains even when we increase the number of workers and use the average score, as all workers scored the translations in the same order. This shows that the mean is also affected by 3 p < 0.001 using Welch's two-sample t-test Table 5 : MT adeq dataset: Translations following a low quality translation receive a lower score than those following a good translation: \"All\" is the mean score of all sentences in the dataset, where each sentence score is calculated as the average of N (standardised) worker scores. \"Low\", \"Middle\", and \"High\" are mean scores of sentences where the previous sentence annotated is of low, medium and high quality, resp. \"H  L\" is the difference between the average high and low scores. sequence bias. Thus, it is theoretically possible to exploit sequence bias to artificially deflate (or inflate) a specific system's computed score by ordering a HIT such that the system's output is seen consistently immediately after a bad (or good) output.\n",
      "4_Discussion_and_Conclusions\n",
      " We have shown significant sequence effects across several independent crowdsourced datasets: a negative autocorrelation in the RTE and TEMPO-RAL datasets, and a positive autocorrelation in the MT adeq dataset. The negative autocorrelation can be attributed either to sequential contrast effects or the gambler's fallacy. These effects were not significant for the AFFECTIVE dataset, perhaps due to the nature of the annotation task, whereby annotations of one emotion are separated by six other annotations, thus limiting the potential for sequencing effects. It is also possible that the dataset is too small to obtain statistical significance. MT judgements are subjective, and when people are asked to rate them on a continuous scale, they need time to calibrate their scale. We show that the sequential bias decreases for better workers as they annotate more sentences in the HIT, indicating a learning effect. Since the ordering of the systems is random, system scores obtained by averaging scores of all sentences translated by the system would be unbiased, assuming a sufficiently large sample of sentences. Thus we do not expect sequential bias to have a marked effect on system rankings or other macro-level conclusions on the basis of this data. However, the scores of in-dividual translations remain biased, which augurs poorly for the use of these annotations at the sentence level, such as when used in error analysis or for training automatic metrics. Sequence problems can be easily addressed by adequate randomisation -providing each individual worker with a separate dataset that has been randomised, such that no two workers see the same ordered data. In this way sequence bias effects can be considered as independent noise sources, rather than a systematic bias, and consequently the aggregate results over several workers will remain unbiased. This study has shown that sequence bias is real, and can distort evaluation and annotation exercises with crowd-workers. We limited our scope to binary and continuous responses, however it is likely that sequence effects are prevalent for multinomial and structured outputs, e.g., in discourse and parsing, where priming is known to have a significant effect (Reitter et al., 2006) . Another important question for future work is whether sequence bias is detectable in expert annotators, not just crowd workers.\n",
      "DR-BiLSTM__Dependent_Reading_Bidirectional_LSTM_for_Natural_Language_Inference\n",
      "OUT/DR-BiLSTM__Dependent_Reading_Bidirectional_LSTM_for_Natural_Language_Inference/\n",
      "../papers-xmls/DR-BiLSTM__Dependent_Reading_Bidirectional_LSTM_for_Natural_Language_Inference.tei.xml\n",
      "1_Introduction\n",
      " Natural Language Inference (NLI; a.k.a. Recognizing Textual Entailment, or RTE) is an important and challenging task for natural language understanding (MacCartney and Manning, 2008) . The goal of NLI is to identify the logical relationship (entailment, neutral, or contradiction) between a premise and a corresponding hypothesis. Table 1 shows few example relationships from the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) . Recently, NLI has received a lot of attention from the researchers, especially due to the avail-P a A senior is waiting at the Relationship window of a restaurant that serves sandwiches. A person waits to be Entailment served his food. A man is looking to order Neutral a grilled cheese sandwich. A man is waiting in line Contradiction for the bus. a P, Premise. b H, Hypothesis. ability of large annotated datasets like SNLI (Bowman et al., 2015) . Various deep learning models have been proposed that achieve successful results for this task (Gong et al., 2017; Wang et al., 2017; Chen et al., 2017; Yu and Munkhdalai, 2017a; Parikh et al., 2016; Zhao et al., 2016; Sha et al., 2016) . Most of these existing NLI models use attention mechanism to jointly interpret and align the premise and hypothesis. Such models use simple reading mechanisms to encode the premise and hypothesis independently. However, such a complex task require explicit modeling of dependency relationships between the premise and the hypothesis during the encoding and inference processes to prevent the network from the loss of relevant, contextual information. In this paper, we refer to such strategies as dependent reading. There are some alternative reading mechanisms available in the literature (Sha et al., 2016; Rocktschel et al., 2015) that consider dependency aspects of the premise-hypothesis relationships. However, these mechanisms have two major limitations:  So far, they have only explored dependency aspects during the encoding stage, while ignoring its benefit during inference.  Such models only consider encoding a hy-pothesis depending on the premise, disregarding the dependency aspects in the opposite direction. We propose a dependent reading bidirectional LSTM (DR-BiLSTM) model to address these limitations. Given a premise u and a hypothesis v, our model first encodes them considering dependency on each other (u|v and v|u) . Next, the model employs a soft attention mechanism to extract relevant information from these encodings. The augmented sentence representations are then passed to the inference stage, which uses a similar dependent reading strategy in both directions, i.e. u  v and v  u. Finally, a decision is made through a multi-layer perceptron (MLP) based on the aggregated information. Our experiments on the SNLI dataset show that DR-BiLSTM achieves the best single model and ensemble model performance obtaining improvements of a considerable margin of 0.4% and 0.3% over the previous state-of-the-art single and ensemble models, respectively. Furthermore, we demonstrate the importance of a simple preprocessing step performed on the SNLI dataset. Evaluation results show that such preprocessing allows our single model to achieve the same accuracy as the state-of-the-art ensemble model and improves our ensemble model to outperform the state-of-the-art ensemble model by a remarkable margin of 0.7%. Finally, we perform an extensive analysis to clarify the strengths and weaknesses of our models.\n",
      "2_Related_Work\n",
      " Early studies use small datasets while leveraging lexical and syntactic features for NLI (Mac-Cartney and Manning, 2008) . The recent availability of large-scale annotated datasets (Bowman et al., 2015; Williams et al., 2017) has enabled researchers to develop various deep learning-based architectures for NLI. Parikh et al. (2016) propose an attention-based model (Bahdanau et al., 2014 ) that decomposes the NLI task into sub-problems to solve them in parallel. They further show the benefit of adding intra-sentence attention to input representations. Chen et al. (2017) explore sequential inference models based on chain LSTMs with attentional input encoding and demonstrate the effectiveness of syntactic information. We also use similar attention mechanisms. However, our model is distinct from these models as they do not benefit from dependent reading strategies. Rocktschel et al. (2015) use a word-by-word neural attention mechanism while Sha et al. (2016) propose re-read LSTM units by considering the dependency of a hypothesis on the information of its premise (v|u) to achieve promising results. However, these models suffer from weak inferencing methods by disregarding the dependency aspects from the opposite direction (u|v). Intuitively, when a human judges a premise-hypothesis relationship, s/he might consider back-and-forth reading of both sentences before coming to a conclusion. Therefore, it is essential to encode the premise-hypothesis dependency relations from both directions to optimize the understanding of their relationship. Wang et al. (2017) propose a bilateral multiperspective matching (BiMPM) model, which resembles the concept of matching a premise and hypothesis from both directions. Their matching strategy is essentially similar to our attention mechanism that utilizes relevant information from the other sentence for each word sequence. They use similar methods as Chen et al. (2017) for encoding and inference, without any dependent reading mechanism. Although NLI is well studied in the literature, the potential of dependent reading and interaction between a premise and hypothesis is not rigorously explored. In this paper, we address this gap by proposing a novel deep learning model (DR-BiLSTM). Experimental results demonstrate the effectiveness of our model.\n",
      "3_Model\n",
      " Our proposed model (DR-BiLSTM) is composed of the following major components: input encoding, attention, inference, and classification. Let u = [u 1 ,    , u n ] and v = [v 1 ,    , v m ] be the given premise with length n and hypothesis with length m respectively, where u i , v j  R r is an word embedding of r-dimensional vector. The task is to predict a label y that indicates the logical relationship between premise u and hypothesis v. RNNs are the natural solution for variable length sequence modeling, consequently, we utilize a bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) for encoding the given sentences. For ease of presentation, we only describe how we encode u depending on v. The same procedure is utilized for the reverse direction (v|u). To dependently encode u, we first process v using the BiLSTM. Then we read u through the BiL-STM that is initialized with previous reading final states (memory cell and hidden state). Here we represent a word (e.g. u i ) and its context depending on the other sentence (e.g. v). Equations 1 and 2 formally represent this component. v = BiLSTM(v, 0) u,  = BiLSTM(u, s v ) (1) u, s u = BiLSTM(u, 0) v,  = BiLSTM(v, s u ) (2) where {  R n2d ,  R n2d , s u } and {v  R m2d ,v  R m2d , s v } are the independent reading sequences, dependent reading sequences, and BiLSTM final state of independent reading of u and v respectively. Note that, \"\" in these equations means that we do not care about the associated variable and its value. BiLSTM inputs are the word embedding sequences and initial state vectors. andv are passed to the next layer as the output of the input encoding component. The proposed encoding mechanism yields a richer representation for both premise and hypothesis by taking the history of each other into account. Using a max or average pooling over the independent and dependent readings does not further improve our model. This was expected since dependent reading produces more promising and relevant encodings. We employ a soft alignment method to associate the relevant sub-components between the given premise and hypothesis. In deep learning models, such purpose is often achieved with a soft attention mechanism. Here we compute the unnormalized attention weights as the similarity of hidden states of the premise and hypothesis with Equation 3 (energy function). e ij = iv T j , i  [1, n], j  [1, m] (3) where i andv j are the dependent reading hidden representations of u and v respectively which are computed earlier in Equations 1 and 2. Next, for each word in either premise or hypothesis, the relevant semantics in the other sentence is extracted and composed according to e ij . Equations 4 and 5 provide formal and specific details of this procedure. u i = m j=1 exp(e ij ) m k=1 exp(e ik )v j , i  [1, n] (4) v j = n i=1 exp(e ij ) n k=1 exp(e kj ) i , j  [1, m] (5) where i represents the extracted relevant information ofv by attending to i while j represents the extracted relevant information of by attending tov j . To further enrich the collected attentional information, a trivial next step would be to pass the concatenation of the tuples ( i , i ) or (v j , j ) which provides a linear relationship between them. However, the model would suffer from the absence of similarity and closeness measures. Therefore, we calculate the difference and element-wise product for the tuples ( i , i ) and (v j , j ) that represent the similarity and closeness information respectively (Chen et al., 2017; Kumar et al., 2016) . The difference and element-wise product are then concatenated with the computed vectors, ( i , i ) or (v j , j ), respectively. Finally, a feedforward neural layer with ReLU activation function projects the concatenated vectors from 8ddimensional vector space into a d-dimensional vector space (Equations 6 and 7). This helps the model to capture deeper dependencies between the sentences besides lowering the complexity of vector representations. a i = [ i , i , i  i , i  i ] p i = ReLU(W p a i + b p ) (6) b j = [v j , j ,v j  j ,v j  j ] q j = ReLU(W p b j + b p ) (7) Here stands for element-wise product while W p  R 8dd and b p  R d are the trainable weights and biases of the projector layer respectively. During this phase, we use another BiLSTM to aggregate the two sequences of computed matching vectors, p and q from the attention stage (Section 3.2). This aggregation is performed in a sequential manner to avoid losing effect of latent variables that might rely on the sequence of matching vectors. Instead of aggregating the sequences of matching vectors individually, we propose a similar dependent reading approach for the inference stage. We employ a BiLSTM reading process (Equations 8 and 9) similar to the input encoding step discussed in Section 3.1. But rather than passing just the dependent reading information to the next step, we feed both independent reading (p andq) and dependent reading (p andq) to a max pooling layer, which selects maximum values from each sequence of independent and dependent readings (p i andp i ) as shown in Equations 10 and 11. The main intuition behind this architecture is to maximize the inferencing ability of the model by considering both independent and dependent readings.q , s q = BiLSTM(q, 0) p,  = BiLSTM(p, s q ) (8) p, s p = BiLSTM(p, 0) q,  = BiLSTM(q, s p ) (9) p = MaxPooling(p,p) (10) q = MaxPooling(q,q) (11) Here {p  R n2d ,p  R n2d , s p } and {q  R m2d ,q  R m2d , s q } are the independent reading sequences, dependent reading sequences, and BiLSTM final state of independent reading of p and q respectively. BiLSTM inputs are the word embedding sequences and initial state vectors. Finally, we convertp  R n2d andq  R m2d to fixed-length vectors with pooling, U  R 4d and V  R 4d . As shown in Equations 12 and 13, we employ both max and average pooling and describe the overall inference relationship with concatenation of their outputs. U = [MaxPooling(p), AvgPooling(p)] (12) V = [MaxPooling(q), AvgPooling(q)] (13) Here, we feed the concatenation of U and V ([U, V ]) into a multilayer perceptron (MLP) classifier that includes a hidden layer with tanh activation and softmax output layer. The model is trained in an end-to-end manner. Output = MLP([U, V ]) (14) 4 Experiments and Evaluation\n",
      "4_Dataset\n",
      " The Stanford Natural Language Inference (SNLI) dataset contains 570K human annotated sentence pairs. The premises are drawn from the Flickr30k (Plummer et al., 2015) corpus, and then the hypotheses are manually composed for each relationship class (entailment, neutral, contradiction, and -). The \"-\" class indicates that there is no consensus decision among the annotators, consequently, we remove them during the training and evaluation following the literature. We use the same data split as provided in Bowman et al. (2015) to report comparable results with other models. We use pre-trained 300-D Glove 840B vectors (Pennington et al., 2014) to initialize our word embedding vectors. All hidden states of BiLSTMs during input encoding and inference have 450 dimensions (r = 300 and d = 450). The weights are learned by minimizing the log-loss on the training data via the Adam optimizer (Kingma and Ba, 2014). The initial learning rate is 0.0004. To avoid overfitting, we use dropout (Srivastava et al., 2014) with the rate of 0.4 for regularization, which is applied to all feedforward connections. During training, the word embeddings are updated to learn effective representations for the NLI task. We use a fairly small batch size of 32 to provide more exploration power to the model. Our observation indicates that using larger batch sizes hurts the performance of our model. Ensemble methods use multiple models to obtain better predictive performance. Previous works typically utilize trivial ensemble strategies by either using majority votes or averaging the probability distributions over the same model with different initialization seeds (Wang et al., 2017; Gong et al., 2017) . By contrast, we use weighted averaging of the probability distributions where the weight of each model is learned through its performance on the SNLI development set. Furthermore, the differences between our models in the ensemble originate from: 1) variations in the number of dependent readings (i.e. 1 and 3 rounds of dependent reading), 2) projection layer activation (tanh and Figure 2 : Performance of n ensemble models reported for training (red, top), development (blue, middle), and test (green, bottom) sets of SNLI. For n number of models, the best performance on the development set is used as the criteria to determine the final ensemble. The best performance on development set (89.22%) is observed using 6 models and is henceforth considered as our final DR-BiLSTM (Ensemble) model. The main intuition behind this design is that the effectiveness of a model may depend on the complexity of a premise-hypothesis instance. For a simple instance, a simple model could perform better than a complex one, while a complex instance may need further consideration toward disambiguation. Consequently, using models with different rounds of dependent readings in the encoding stage should be beneficial. Figure 2 demonstrates the observed performance of our ensemble method with different number of models. The performance of the models are reported based on the best obtained accuracy on the development set. We also study the effectiveness of other ensemble strategies e.g. majority voting, and averaging the probability distributions. But, our ensemble strategy performs the best among them (see Section 1 in the supplementary material for additional details). We perform a trivial preprocessing step on SNLI to recover some out-of-vocabulary words found in the development set and test set. Note that our vocabulary contains all words that are seen in the training set, so there is no out-of-vocabulary word in it. The SNLI dataset is not immune to human errors, specifically, misspelled words. We noticed that misspelling is the main reason for some of the observed out-of-vocabulary words. Consequently, we simply fix the unseen misspelled words using Microsoft spell-checker (other approaches like edit distance can also be used). Moreover, while dealing with an unseen word during evaluation, we try to: 1) replace it with its lower case, or 2) split the word when it contains a \"-\" (e.g. \"marsh-like\") or starts with \"un\" (e.g. \"unloading\"). If we still could not find the word in our vocabulary, we consider it as an unknown word. In the next subsection, we demonstrate the importance and impact of such trivial preprocessing (see Section 2 in the supplementary material for additional details). Table 2 shows the accuracy of the models on training and test sets of SNLI. The first row represents a baseline classifier presented by Bowman et al. (2015) that utilizes handcrafted features. All other listed models are deep-learning based. The gap between the traditional model and deep learning models demonstrates the effectiveness of deep learning methods for this task. We also report the estimated human performance on the SNLI dataset, which is the average accuracy of five annotators in comparison to the gold labels (Gong et al., 2017) . It is noteworthy that recent deep learning models surpass the human performance in the NLI task. As shown in Table 2 , previous deep learning models (rows 2-19) can be divided into three categories: 1) sentence encoding based models (rows 2-7), 2) single inter-sentence attention-based models (rows 8-16), and 3) ensemble inter-sentence attention-based models (rows 17-19). We can see that inter-sentence attention-based models perform better than sentence encoding based models, which supports our intuition. Natural language inference requires a deep interaction between the premise and hypothesis. Inter-sentence attention-based approaches can provide such interaction while sentence encoding based models fail to do so. To further enhance the modeling of interaction between the premise and hypothesis for efficient disambiguation of their relationship, we introduce the dependent reading strategy in our proposed DR-BiLSTM model. The results demonstrate the effectiveness of our model. DR-BiLSTM (Single) et al., 2015) 83.9% 80.6% (Vendrov et al., 2015) 98.8% 81.4% (Mou et al., 2016) 83.3% 82.1% (Bowman et al., 2016) 89.2% 83.2% (Liu et al., 2016b) 84.5% 84.2% (Yu and Munkhdalai, 2017a) 86.2% 84.6% (Rocktschel et al., 2015) 85.3% 83.5% (Wang and Jiang, 2016) 92.0% 86.1% (Liu et al., 2016a) 88.5% 86.3% (Parikh et al., 2016) 90.5% 86.8% (Yu and Munkhdalai, 2017b) 88.5% 87.3% (Sha et al., 2016) 90.7% 87.5% (Wang et al., 2017) (Single) 90.9% 87.5% (Chen et al., 2017) (Single) 92.6% 88.0% (Gong et al., 2017) Table 2 : Accuracies of the models on the training set and test set of SNLI. DR-BiLSTM (Ensemble) achieves the accuracy of 89.3%, the best result observed on SNLI, while DR-BiLSTM (Single) obtains the accuracy of 88.5%, which considerably outperforms the previous non-ensemble models. Also, utilizing a trivial preprocessing step yields to further improvements of 0.4% and 0.3% for single and ensemble DR-BiLSTM models respectively. achieves 88.5% accuracy on the test set which is noticeably the best reported result among the existing single models for this task. Note that the difference between DR-BiLSTM and Chen et al. (2017) is statistically significant with a p-value of < 0.001 over the Chi-square test 1 . To further improve the performance of NLI systems, researchers have built ensemble models. Previously, ensemble systems obtained the best performance on SNLI with a huge margin. Table 2 shows that our proposed single model achieves competitive results compared to these reported ensemble models. Our ensemble model considerably outperforms the current state-of-the-art by obtaining 89.3% accuracy. Up until this point, we discussed the performance of our models where we have not con-sidered preprocessing for recovering the out-ofvocabulary words. In Table 2 , \"DR-BiLSTM (Single) + Process\", and \"DR-BiLSTM (Ensem.) + Process\" represent the performance of our models on the preprocessed dataset. We can see that our preprocessing mechanism leads to further improvements of 0.4% and 0.3% on the SNLI test set for our single and ensemble models respectively. In fact, our single model (\"DR-BiLSTM (Single) + Process\") obtains the state-of-the-art performance over both reported single and ensemble models by performing a simple preprocessing step. Furthermore, \"DR-BiLSTM (Ensem.) + Process\" outperforms the existing state-of-the-art remarkably (0.7% improvement). For more comparison and analyses, we use \"DR-BiLSTM (Single)\" and \"DR-BiLSTM (Ensemble)\" as our single and ensemble models in the rest of the paper. We conducted an ablation study on our model to examine the importance and effect of each major component. Then, we study the impact of BiL-STM dimensionality on the performance of the development set and training set of SNLI. We investigate all settings on the development set of the SNLI dataset. Table 3 shows the ablation study results on the development set of SNLI along with the statistical significance test results in comparison to the proposed model, DR-BiLSTM. We can see that all modifications lead to a new model and their differ-ences are statistically significant with a p-value of < 0.001 over Chi square test. Table 3 shows that removing any part from our model hurts the development set accuracy which indicates the effectiveness of these components. Among all components, three of them have noticeable influences: max pooling, difference in the attention stage, and dependent reading. Most importantly, the last four study cases in Table 3 (rows 8-11) verify the main intuitions behind our proposed model. They illustrate the importance of our proposed dependent reading strategy which leads to significant improvement, specifically in the encoding stage. We are convinced that the importance of dependent reading in the encoding stage originates from its ability to focus on more important and relevant aspects of the sentences due to its prior knowledge of the other sentence during the encoding procedure. Figure 3 shows the behavior of the proposed model accuracy on the training set and development set of SNLI. Since the models are selected based on the best observed development set accuracy during the training procedure, the training accuracy curve (red, top) is not strictly increasing. Figure 3 demonstrates that we achieve the best performance with 450-dimensional BiLSTMs. In other words, using BiLSTMs with lower dimensionality causes the model to suffer from the lack of space for capturing proper information and dependencies. On the other hand, using higher dimensionality leads to overfitting which hurts the performance on the development set. Hence, we use 450-dimensional BiLSTM in our proposed model. We first investigate the performance of our models categorically. Then, we show a visualization of the energy function in the attention stage (Equation 3) for an instance from the SNLI test set. To qualitatively evaluate the performance of our models, we design a set of annotation tags that can be extracted automatically. This design is inspired by the reported annotation tags in Williams et al. (2017) . The specifications of our annotation tags are as follows:  High Overlap: premise and hypothesis sentences share more than 70% tokens.  Regular Overlap: sentences share between 30% and 70% tokens.  Low Overlap: sentences share less than 30% tokens.  Long Sentence: either sentence is longer than 20 tokens.  Regular Sentence: premise or hypothesis length is between 5 and 20 tokens.  Short Sentence: either sentence is shorter than 5 tokens.  Negation: negation is present in a sentence.  Quantifier: either of the sentences contains one of the following quantifiers: much, enough, more, most, less, least, no, none, some, any, many, few, several, almost, nearly.  Belief: either of the sentences contains one of the following belief verbs: know, believe, understand, doubt, think, suppose, recognize, forget, remember, imagine, mean, agree, disagree, deny, promise. Table 4 shows the frequency of aforementioned annotation tags in the SNLI test set along with the performance (accuracy) of ESIM (Chen et al., 2017) , DR-BiLSTM (Single), and DR-BiLSTM (Ensemble). Table 4 can be divided into four major categories: 1) gold label data, 2) word overlap, 3) sentence length, and 4) occurrence of special words. We can see that DR-BiLSTM (Ensemble) performs the best in all categories which matches our expectation. Moreover, DR-BiLSTM (Single) (Chen et al., 2017) , DR-BiLSTM (DR(S)) and Ensemble DR-BiLSTM (DR(E)) on the SNLI test set. performs noticeably better than ESIM in most of the categories except \"Entailment\", \"High Overlap\", and \"Long Sentence\", for which our model is not far behind (gaps of 0.2%, 0.5%, and 0.9%, respectively). It is noteworthy that DR-BiLSTM (Single) performs better than ESIM in more frequent categories. Specifically, the performance of our model in \"Neutral\", \"Negation\", and \"Quantifier\" categories (improvements of 1.4%, 3.5%, and 1.9%, respectively) indicates the superiority of our model in understanding and disambiguating complex samples. Our investigations indicate that ESIM generates somewhat uniform attention for most of the word pairs while our model could effectively attend to specific parts of the given sentences and provide more meaningful attention. In other words, the dependent reading strategy enables our model to achieve meaningful representations, which leads to better attention to obtain further gains on such categories like Negation and Quantifier sentences (see Section 3 in the supplementary material for additional details). Finally, we show a visualization of the normalized attention weights (energy function, Equation 3) of our model in Figure 4 . We show a sentence pair, where the premise is \"Male in a blue jacket decides to lay the grass.\", and the hypothesis is \"The guy in yellow is rolling on the grass.\", and its logical relationship is contradiction. Figure 4 indicates the model's ability in attending to critical pairs of words like <Male, guy>, <decides, rolling>, and <lay, rolling>. Finally, high attention between {decides, lay} and\n",
      "5_Conclusion\n",
      " We propose a novel natural language inference model (DR-BiLSTM) that benefits from a dependent reading strategy and achieves the state-of-theart results on the SNLI dataset. We also introduce a sophisticated ensemble strategy and illustrate its effectiveness through experimentation. Moreover, we demonstrate the importance of a simple preprocessing step on the performance of our proposed models. Evaluation results show that the preprocessing step allows our DR-BiLSTM (single) model to outperform all previous single and ensemble methods. Similar superior performance is also observed for our DR-BiLSTM (ensemble) model. We show that our ensemble model outperforms the existing state-of-the-art by a considerable margin of 0.7%. Finally, we perform an extensive analysis to demonstrate the strength and weakness of the proposed model, which would pave the way for further improvements in this domain.\n",
      "AD3__Attentive_Deep_Document_Dater\n",
      "OUT/AD3__Attentive_Deep_Document_Dater/\n",
      "../papers-xmls/AD3__Attentive_Deep_Document_Dater.tei.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_Introduction\n",
      " Many natural language processing tasks require document creation time (DCT) information as a useful additional metadata. Tasks such as information retrieval (Li and Croft, 2003; Dakka et al., 2008) , temporal scoping of events and facts (Allan et al., 1998; Talukdar et al., 2012b) , document summarization (Wan, 2007) and analysis (de Jong et al., 2005a) require precise and validated creation time of the documents. Most of the documents obtained from the Web either contain DCT that cannot be trusted or contain no DCT information at all (Kanhabua and Nrvg, 2008) . Thus, predicting the time of these documents based on their content is an important task, often referred to as Document Dating. A few generative approaches (de Jong et al., 2005b; Kanhabua and Nrvg, 2008) as well as a discriminative model (Chambers, 2012) have been previously proposed for this task. Kotsakos et al. (2014) employs term-burstiness resulting in improved precision on this task. Recently proposed NeuralDater (Vashishth et al., 2018) uses a graph convolution network (GCN) based approach for document dating, outperforming all previous models by a significant margin. NeuralDater extensively uses the syntactic and temporal graph structure present within the document itself. Motivated by NeuralDater, we explicitly develop two different methods: a) Attentive Context Model, and b) Ordered Event Model. The first component tries to accumulate knowledge across documents, whereas the latter uses the temporal structure of the document for predicting its DCT. Motivated by the effectiveness of attention based models in different NLP tasks (Yang et al., 2016a; Bahdanau et al., 2014) , we incorporate attention in our method in a principled fashion. We use attention not only to capture context but also for feature aggregation in the graph convolution network (Hamilton et al., 2017) . Our contributions are as follows.  We propose Attentive Deep Document Dater (AD3), the first attention-based neural model for time-stamping documents.  We devise a novel method for label based attentive graph convolution over directed graphs and use it for the document dating task.  Through extensive experiments on multiple real-world datasets, we demonstrate AD3's effectiveness over previously proposed methods. OE-GCN provides the probability scores over the years given the encoded DCT, while AC-GCN provides the probability scores given the context of the document. Both the models are trained separately. made for document time-stamping task include statistical language models proposed by de Jong et al. (2005b) and Kanhabua and Nrvg (2008) . (Chambers, 2012) use temporal and hand-crafted features extracted from documents to predict DCT. They propose two models, one of which learns the probabilistic constraints between year mentions and the actual creation time, whereas the other one is a discriminative model trained on hand-crafted features. Kotsakos et al. (2014) propose a termburstiness (Lappas et al., 2009 ) based statistical method for the task. Vashishth et al. (2018) propose a deep learning based model which exploits the temporal and syntactic structure in documents using graph convolutional networks (GCN). Event Ordering System: The task of extracting temporally rich events and time expressions and ordering between them is introduced in the TempEval challenge (UzZaman et al., 2013; Verhagen et al., 2010) . Various approaches (Mc-Dowell et al., 2017; Mirza and Tonelli, 2016) made for solving the task use sieve-based archi-tectures, where multiple classifiers are ranked according to their precision and their predictions are weighted accordingly resulting in a temporal graph structure. A method to extract temporal ordering among relational facts was proposed in (Talukdar et al., 2012a) . Graph Convolutional Network (GCN): GCN (Kipf and Welling, 2016) is the extension of convolutional networks over graphs. In different NLP tasks such as semantic-role labeling , neural machine translation (Bastings et al., 2017) , and event detection (Nguyen and Grishman, 2018), GCNs have proved to be effective. We extensively use GCN for capturing both syntactic and temporal aspect of the document. Attention Network: Attention networks have been well exploited for various tasks such as document classification (Yang et al., 2016b) , question answering (Yang et al., 2016a) , machine translation (Bahdanau et al., 2014; Vaswani et al., 2017) . Recently, attention over graph structure has been shown to work well by Velikovi et al. (2018) . Taking motivation from them, we deploy an attentive convolutional network on temporal graph for the document dating problem.\n",
      "21\n",
      "Acknowledgments This work is supported by the Ministry of Human Resource Development (MHRD), Government of India.\n",
      "3_Background:_GCN_&_NeuralDater\n",
      " The task of document dating can be modeled as a multi-class classification problem. Following prior work, we shall focus on DCT prediction at the year-granularity in this paper. In this section, we summarize the previous state-of-the-art model NeuralDater (Vashishth et al., 2018) , before moving onto our method. An overview of graph convolutional network (GCN) (Kipf and Welling, 2016) is also necessary as it is used in NeuralDater as well as in our model. GCN for Undirected Graph: Consider an undirected graph, G = (V, E), where V and E are the set of n vertices and set of edges respectively. Matrix X  R nm , whose rows are input representation of node u, where x u  R m ,  u  V, is the input feature matrix. The output hidden representation h v  R d of a node v after a single layer of graph convolution operation can be obtained by considering only the immediate neighbours of v, as formulated in (Kipf and Welling, 2016) . In order to capture information at multi-hop distance, one can stack layers of GCN, one over another. GCN for Directed Graph: Consider a labelled edge from node u to v with label l(u, v), denoted collectively as (u, v, l(u, v) ). Based on the assumption that information in a directed edge need not only propagate along its direction, Marcheggiani and Titov (2017) added opposite edges viz., for each (u, v, l(u, v) ), (v, u, l(u, v) 1 ) is added to the edge list. Self loops are also added for passing the current embedding information. When GCN is applied over this modified directed graph, the embedding of the node v after k th layer will be, h k+1 v = f   uN (v) W k l(u,v) h k u + b k l(u,v)   . We note that the parameters W k l(u,v) and b k l (u,v) in this case are edge label specific. h k u is the input to the k th layer. Here, N (v) refers to the set of neighbours of v, according to the updated edge list and f is any non-linear activation function (e.g., ReLU: f (x) = max(0, x)). In this sub-section, we provide a brief overview of the components of the NeuralDater (Vashishth et al., 2018) . Given a document D with n tokens w 1 , w 2 ,    w n , NeuralDater extracts a temporally rich embedding of the document in a principled way as explained below: Bi-directional LSTM is employed for embedding each word with its context. The GloVe representation of the words X  R nk is transformed to a context aware representation H cntx  R nk to get the context embedding. This is essentially shown as the Bi-LSTM in Figure 1 . In this step, the context embeddings are further processed using GCN over the dependency parse tree of the sentences in the document, in order to capture long range connection among words. The syntactic dependency structure is extracted by Stanford CoreNLP's dependency parser (Manning et al., 2014) . NeuralDater follows the same formulation of GCN for directed graph as described in Section 3.1, where additional edges are added to the graph to model the information flow. Again following , Neu-ralDater does not allocate separate weight matrices for different types of dependency edge labels, rather it considers only three type of edges: a) edges that exist originally, b) the reverse edges that are added explicitly, and c) self loops. The S-GCN portion of Figure 1 represents this component. More formally, H cntx  R nk is transformed to H syn  R nksyn by applying S-GCN. In this layer, NeuralDater exploits the Event-Time graph structure present in the document. CATENA (Mirza and Tonelli, 2016) , current state-of-the-art temporal and causal relation extraction algorithm, produces the temporal graph from the event time annotation of the document. GCN applied over this Event-Time graph, namely T-GCN, chooses n T number of tokens out of total n tokens from the document for further revision in their embeddings. Note that T is the total number of events and time mentions present in the document. A special node DCT is added to the graph and its embedding is jointly learned. Note that this layer learns both label and direction specific parameters. Finally, the DCT embedding concatenated with the average pooled syntactic embedding is fed to a softmax layer for classification. This whole procedure is trained jointly.\n",
      "4_Attentive_Deep_Document_Dater_(AD3):_Proposed_Method\n",
      " In this section, we describe Attentive Deep Document Dater (AD3), our proposed method. AD3 is inspired by NeuralDater, and shares many of its components. Just like in NeuralDater, AD3 also leverages two main types of signals from the document -syntactic and event-time -to predict the document's timestamp. However, there are crucial differences between the two systems. Firstly, instead of concatenating embeddings learned from these two sources as in NeuralDater, AD3 treats these two models completely separate and combines them at a later stage. Secondly, unlike Neu-ralDater, AD3 employs attention mechanisms in each of these two models. We call the resulting models Attentive Context Model (AC-GCN) and Ordered Event Model (OE-GCN). These two models are described in Section 4.1 and Section 4.2, respectively. Recent success of attention-based deep learning models for classification (Yang et al., 2016b) , question answering (Yang et al., 2016a) , and machine translation (Bahdanau et al., 2014) have motivated us to use attention during document dating. We extend the syntactic embedding model of Neu-ralDater (Section 3.2.2) by incorporating an attentive pooling layer. We call the resulting model AC-GCN. This model (right side in Figure 1 ) has two major components.  Context Embedding and Syntactic Embedding: Following NeuralDater, we used Bi-LSTM and S-GCN to capture context and long-range syntactic dependencies in the document (Please refer to Section 3.2.1, Section 3.2.2 for brief description). The syntactic embedding, H syn  R nksyn is then fed to an Attention Network for further processing. Note that, k syn is the dimension of the output of Syntactic-GCN and n is the number of tokens in the document.  Attentive Embedding: In this layer, we learn the representation for the whole document through word level attention network. We learn a context vector, u s  R s with respect to which we calculate attention for each token. Finally, we aggregate the token features with respect to their attention weights in order to represent the document. More formally, let h syn t  R ksyn be the syntactic representation of the t th token in the document. We take non-linear projection of it in R s with W s  R sksyn . Attention weight  t for t th token is calculated with respect to the context vector u T t as follows. u t = tanh(W s h syn t ),  t = exp(u T t u s ) t exp(u T t u s ) . Finally, the document representation for the AC-GCN is computed as shown below. d ACGCN = t  t h syn t This representation is fed to a softmax layer for the final classification. The final probability distribution over years predicted by the AC-GCN is given below. P ACGCN (y|D) = Softmax(W  d ACGCN + b). The OE-GCN model is shown on the left side of Figure 1 . Just like in AC-GCN, context and syntactic embedding is also part of OE-GCN. The syntactic embedding is fed to the Attentive Graph Convolution Network (AT-GCN) where the graph is obtained from the time-event ordering algorithm CATENA (Mirza and Tonelli, 2016) . We describe these components in detail below. We use the same process used in NeuralDater (Vashishth et al., 2018) for procuring the Temporal Graph from the document. CATENA (Mirza and Tonelli, 2016) Let E T be the edge list of the Temporal Graph. Similar to Vashishth et al., 2018) , we also add reverse edges for each of the existing edge and self loops for passing current node information as explained in Section 3.1. The new edge list E T is shown below. E T = E T  {(j, i, l(i, j) 1 ) | (i, j, l(i, j))  E T }  {(i, i, self) | i  V)}. The reverse edges are added with reverse labels like AFTER 1 , BEFORE 1 etc . Finally, we get 10 labels for our temporal graph and we denote the set of edge labels by L. Since the temporal graph is automatically generated, it is likely to have incorrect edges. Ideally, we would like to minimize the influence of such noisy edges while computing temporal embedding. In order to suppress the noisy edges in the Temporal Graph and detect important edges for reasoning, we use attentive graph convolution (Hamilton et al., 2017) over the Event-Time graph. The attention mechanism learns the aggregation function jointly during training. Here, the main objective is to calculate the attention over the neighbouring nodes with respect to the current node for a given label. Then the embedding of the current node is updated by mixing neighbouring node embedding according to their attention scores. In this respect, we propose a label-specific attentive graph convolution over directed graphs. Let us consider an edge in the temporal graph from node i to node j with type l, where l  L and L is the label set. The label set L can be divided broadly into two coarse labels as done in Section 3.2.2. The attention weights are specific to only these two type of edges to reduce parameter and prevent overfitting. For illustration, if there exists an edge from node i to j then the edge types will be,  L(i, j) =  if (i, j, l(i, j))  E T , i.e., if the edge is an original event-time edge.  L(i, j) =  if (i, j, l(i, j) 1 )  E T , i.e., if the edge is added later. First, we take a linear projection (W atten L(i,j)  R F ksyn ) of both the nodes in R F in order to map before before a before both of them in the same direction-specific space. The concatenated vector [W atten L(i,j)  h i ; W atten L(i,j)  h j ], signifies the importance of the node j w.r.t. node i. A non linear transformation of this concatenation can be treated as the importance feature vector between i and j. e ij = tanh[W atten L(i,j)  h i ; W atten L(i,j)  h j ]. Now, we compute the attention weight of node j for node i with respect to a direction-specific context vector a L(i,j)  R 2F , as follows.  l(i,j) ij = exp a T L(i,j) e ij kN l(i,) i exp a T L(i,j) e ik , where,  l(i,j) ij = 0 if node i and j is not connected through label l. N l(i,) denotes the subset of the neighbourhood of node i with label l only. Please note that, although the linear transform weight (W atten L(i,j)  R F ksyn ) is specific to the coarse labels L, but for each finer label l  L we get these convex weights of attentions. Figure  2 illustrates the above description w.r.t. edge type BEFORE. Valid Accuracy (%)  Figure 3 : Variation of validation accuracy with  (for APW dataset). We observe that AC-GCN and OE-GCN are both important for the task as we get optimal  = 0.52. Finally, the feature aggregation is done according to the attention weights. Prior to that, another label specific linear transformation is taken to perform the convolution operation. Then, the updated feature for node i is calculated as follows. h k+1 i = f lL jN l(i,) i  l(i,j) ij W l(i,j) h j + b l(i,j) . where,  ii = 1, N l(i,) denotes the subset of the neighbourhood of node i with label l only. Note that,  l(i,j) ij = 0 when j /  N l(i,) . To illustrate formally, from Figure 2 , we see that weight  1 and  2 is calculated specific to label type BEFORE and the neighbours which are connected through BE-FORE is being multiplied with W bef ore prior to aggregation in the ReLU block. Now, after applying attentive graph convolution network, we only consider the representation of Document Creation Time (DCT), h DCT , as the document representation itself. h DCT is now passed through a fully connected layer prior to softmax. Prediction of the OE-GCN for the document D will be given as P OEGCN (y|D) = Softmax(W  d DCT + b). In this section, we propose an unified model by mixing both AC-GCN and OE-GCN. Even on validation data, we see that performance of both the models differ to a large extent. This significant difference (McNemar test p < 0.000001) motivated the unification. We take convex combination of the output probabilities of the two models  as shown below. P joint (y|D) = P ACGCN (y|D) + (1  )P OEGCN (y|D). The combination hyper-parameter  is tuned on the validation data. We obtain the value of  to be 0.52 (Figure 3 ) and 0.54 for APW and NYT datasets, respectively. This depicts that the two models are capturing significantly different aspects of documents, resulting in a substantial improvement in performance when combined.\n",
      "5_Experimental_Setup\n",
      " Dataset: Experiments are carried out on the Associated Press Worldstream (APW) and New York Times (NYT) sections of the Gigaword corpus (Parker et al., 2011) . We have used the same 8:1:1 split as Vashishth et al. (2018) for all the models. For quantitative details please refer to Table 1 . Evaluation Criteria: In accordance with prior work (Chambers, 2012; Kotsakos et al., 2014; Vashishth et al., 2018) the final task is to predict the publication year of the document. We give a brief description of the baselines below. Baseline Methods:  MaxEnt-Joint (Chambers, 2012) : This method engineers several hand-crafted temporally influenced features to classify the document using MaxEnt Classifier.  BurstySimDater (Kotsakos et al., 2014) : This is a purely statistical method which uses lexical similarity and term burstiness (Lappas et al., 2009) for dating documents in arbitrary length time frame. For our experiments, we used a time frame length of 1 year.  NeuralDater (Vashishth et al., 2018) : This is the first deep neural network based approach for the document dating task. Details are provided in Section 3.2.  Israel's consumer price index increased by 1.2 percent in December, bringing the overall inflation rate for 1995 to 8.1 percent, well within the government's target rate for the year, officials said Friday. Israel radio said that it was the lowest annual inflation rate in twenty years. Hyperparameters: We use 300-dimensional GloVe embeddings and 128-dimensional hidden state for both GCNs and BiLSTM with 0.8 dropout. We use Adam (Kingma and Ba, 2014) with 0.001 learning rate for training. For OE-GCN we use 2-layers of AT-GCN. 1-layer of S-GCN is used for both the models.\n",
      "6_Results\n",
      "  In this section, we compare the effectiveness of our method with that of prior work. The deep network based NeuralDater model in (Vashishth et al., 2018) outperforms previous feature engi-   neered (Chambers, 2012) and statistical methods (Kotsakos et al., 2014 ) by a large margin. We observe a similar trend in our case. Compared to the state-of-the-art model NeuralDater, we gain, on an average, a 3.7% boost in accuracy on both the datasets (Table 2) . Among individual models, OE-GCN performs at par with NeuralDater, while AC-GCN outperforms it. The empirical results imply that AC-GCN by itself is effective for this task. The relatively worse performance of OE-GCN can be attributed to the fact that it only focuses on the Event-Time information and leaves out most of the contextual information. However, it captures various different (p < 0.000001, McNemar's test, 2-tailed) aspects of the document for classification, which motivated us to propose an ensemble of the two models. This explains the significant boost in performance of AD3 over NeuralDater as well as the individual models. It is worth mentioning that although AC-GCN and OE-GCN do not provide significant boosts in accuracy, their predictions have considerably lower mean-absolute-deviation as shown in Figure 4 . We concatenated the DCT embedding provided by OE-GCN with the document embedding provided by AC-GCN and trained in an end to end joint fashion like NeuralDater. We see that even with a similar training method, the Attentive Neu-ralDater model on an average, performs 1.6% better in terms of accuracy, once again proving the efficacy of attention based models over normal models. Attentive Graph Convolution (Section 4.2.2) proves to be effective for OE-GCN, giving a 2% accuracy improvement over non-attentive T-GCN of NeuralDater (Table 3) . Similarly the efficacy of word level attention is also prominent from Table  3 . We have also analyzed our models by visualizing attentions over words and attention over graph nodes. Figure 5 shows that AC-GCN focuses on temporally informative words such as \"said\" (for tense) or time mentions like \"1995\", alongside important contextual words like \"inflation\", \"Israel\" etc. For OE-GCN, from Figure 6 we observe that \"DCT\" and time-mention '1995' grabs the highest attention. Attention between \"DCT\" and other event verbs indicating past tense are quite prominent, which helps the model to infer 1996 (which is correct) as the most likely time-stamp of the document. These analyses provide us with a good justification for the performance of our attentive models.\n",
      "7_Discussion\n",
      " Apart from empirical improvements over previous models, we also perform a qualitative analysis of the individual models. Figure 7 shows that the performance of AC-GCN improves with the length of documents, thus indicating that richer context leads to better model prediction. Figure  8 shows how the performance of OE-GCN improves with the number of event-time mentions in the document, thus further reinforcing our claim that more temporal information improves model performance. Vashishth et al. (2018) reported that their model got confused by the presence of multiple misleading time mentions. AD3 overcomes this limitation using attentive graph convolution, which successfully filters out noisy time mentions as is evident\n",
      "8_Conclusion\n",
      " We propose AD3, an ensemble model which exploits both syntactic and temporal information in a document explicitly to predict its creation time (DCT). To the best of our knowledge, this is the first application of attention based deep models for dating documents. Our experimental results demonstrate the effectiveness of our model over all previous models. We also visualize the attention weights to show that the model is able to choose what is important for the task and filter out noise inherent in language. As part of future work, we would like to incorporate external knowledge as a side information for improved time-stamping of documents.\n",
      "Crowdsourcing_with_Arbitrary_Adversaries\n",
      "OUT/Crowdsourcing_with_Arbitrary_Adversaries/\n",
      "../papers-xmls/Crowdsourcing_with_Arbitrary_Adversaries.tei.xml\n",
      "1\n",
      "I. INTRODUCTION Crowdsourcing empowers open collaborations over the Internet. A remarkable case is to gather knowledge by human intelligence tasks (HITs). In a HIT, a requester specifies a few questions and lets some workers answer, such that the requester solicits answers while the workers get paid. Since HITs were firstly minted in Amazon's MTurk [1] , they have been widely adopted, say to solicit training datasets for machine learning [2] [3] [4] . Notably, ImageNet [5] , an impactful deep learning benchmark, was created by thousands of HITs, and laid stepping stones for the deep learning paradigm. Nevertheless, both academia and industry [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] realize the broader adoption of HITs is severely impeded in practice, as a result of the serious security concerns of free-riding and falsereporting: (i) on the one hand, HITs suffer from low-quality answers, as misconducting workers or even bots would try to reap rewards without making real efforts [7, 8] ; (ii) on the other hand, many real-world practices set forth the idea of *This paper is accepted to appear in the 40th International Conference on Distributed Computing Systems, July, 2020, Singapore (ICDCS 2020). 1 In German history, a dragoon or dragoner was a lancer that particularly was light and firearmed; as an analog, our HITs protocol is super efficient and provably secure regarding the modern freelancers. allowing the requester to reject low-quality answers [9] [10] [11] , but cause quite many requesters in the wild arbitrarily reject answers in order to collect data without paying [14] . The issues of free-riding and false-reporting become the major obstacles to achieving broader adoption of HITs that are joined by mutually distrustful users [14] , and therefore raise a basic requirement of fairness in HITs, namely, the requester pays a worker, iff the worker puts forth a qualified answer. Many studies [6] [7] [8] [9] [10] [11] [12] characterize the purpose and then design proper incentives and payment policies for the needed fairness. Notwithstanding, most traditional solutions to fairness [6] [7] [8] [9] [10] [11] [12] fully trust in a de facto centralized third-party platform to enforce the payment policies for the basic fairness requirement in HITs. Unfortunately, putting trust in a single party turns out to be vulnerable and elusive in practice, as a reflection of tremendous compromises, outages and misfeasance of realworld crowdsourcing platforms [14] [15] [16] . For instance, one of the most popular crowdsourcing platform, MTurk, is biased and allows corrupted requesters to reap data without paying [14, 15] . Worse still, all well-known weaknesses of overtrusted third-parties, such as single-point failure [16] and tremendous privacy leakage [17] remain as serious vulnerabilities in the special case of crowdsourcing. Let alone the third-party platforms impose expensive handling fees, say MTurk charges a handling fee up to 45% of overall incentives [18] . New challenges in decentralization. Recognizing those drawbacks of centralized crowdsourcing, recent attempts [19, 20] initiated the decentralized crowdsourcing through the newly emerged blockchain 2 technology. Their aim is to \"simulate\" a virtual platform that is trustful to enforce the carefully designed payment policies, without suffering from the vulnerabilities of fully centralized systems. However, as shown in [21, 22] , decentralization atop open blockchain brought about a few \"new\" security challenges that can render the incentives of HITs completely ineffective [19] . Privacy as a basic requirement. In particular, due to the transparency of blockchain [21, 22] , once some answers are submitted, any malicious worker can simply copy and resubmit them to earn rewards without making any real efforts, which immediately allows free-riding and cracks the basic fairness of HITs. Namely, the transparent blockchain provides all workers a new choice: running a simple automated script to \"copy-and-paste\" other answers in the blockchain, which was infeasible in previous centralized systems. More seriously, having the new path to free-riding in mind, rational workers would wait to copy, instead of doing any real efforts. Thus sorta \"tragedy of the commons\" occurs, and no one will respond with independent answers [23] [24] [25] [26] . That said, the straightforwardly decentralized crowdsourcing arguably loses all basic utilities and fails to gather anything meaningful! So the privacy becomes indispensable in the decentralized crowdsourcing systems, instead of a bonus property. State-of-the-art & open problem. To overcome blockchain's inherent limits, prior art [19] proposes the general outsourcethen-prove framework for private decentralized HITs. It enables the requester to prove the quality of answers that are encrypted to her, without revealing the actual answers. Such the proof becomes the crux to ensure privacy, and deters both false-reporting and free-riding. Now, the blockchain needs to verify proofs, so a feasibility challenge sprouts up, considering the on-chain computational resources are too limited to support any large proof or costly verification. For above reasons, prior work relies on some generic zeroknowledge proof (zk-proof) framework that is succinct in proof size and efficient for verifying, in particular SNARK 3 [29] [30] [31] to reduce the on-chain verification cost. Nonetheless, generic zk-proofs such as SNARK inevitably inherit low performance for the convenience of achieving generality, causing that prior private decentralized HITs suffer from an unbearable off-chain proving cost and a still significant on-chain verifying expense:  Infeasible proving (off-chain). The proving of generic zkproofs (e.g., SNARK) seems inherently complex, due to the burdensome NP-reduction for generality. In particular, prior study [32] reported 56 GB memory and 2 hours are needed to prove whether an encrypted answer coincides with the majority of all encrypted submissions at a very small scale, e.g., at most eleven answers. Such a performance prevents the previous protocol from being usable by any normal requesters using regular PCs.  Costly verification (on-chain). Existing blockchains (e.g. Ethereum) are feasible to verify only few types of generic zk-proofs such as SNARK, whose verification need to compute a dozen of expensive pairings over elliptic curve [29] [30] [31] . So the on-chain verification of these zk-proofs is not only computationally costly, but also financially expensive. Currently in Ethereum, 12 pairings already spend 500k gas [33] , and verifying a SNARK proof costs even more (about half US dollar). Given the insufficiencies of the state-of-the-art, the following critical problem remains open: How to design a practical private decentralized HITs protocol for crowdsourcing human knowledge? 3 Remark that though the rise of Intel SGX becomes a seemingly enticing alternative of SNARK to go beyond many limits of blockchain by remote attestations [27] , unfortunately, recent Foreshadow attacks [28] allow the adversary to forge \"attestations\" by stealing the attestation key hardcoded in any SGX Enclave, which seriously challenges the already heavy assumption of \"trusted\" hardware, and makes it even more illusive to trust SGX in practice. Our contributions. To answer the above unresolved problem, we present a practical private decentralized HITs protocol for the major tasks of crowdsourcing human knowledge. In sum, our core technical contributions are three-fold:  To achieve the practical protocol for private decentralized HITs, we carefully explore various non-trivial optimizations to avoid the cumbersome generic-purpose zeroknowledge framework, and reduce the protocol to a special-purpose verifiable encryption. As such, we attain concrete improvements by orders of magnitude, regarding both the proving and verification: -For proving, our approach is two order of magnitude better than generic zk-proof. 4 In particular, for the same HIT, the proving cost of our protocol is only 50 MB memory and 10 ms running time. -For verifying, our result improves upon the generic solution by nearly an order of magnitude. The onchain cost of verifying a proof for answer quality is reduced to 180k gas in Ethereum (much smaller than verifying SNARK proofs and typically few US cents).  We further implement our protocol to instantiate a practical private decentralized crowdsourcing system Dragoon, which can be deployed atop many real-world blockchains such as Ethereum. We use Dragoon to launch a concrete HIT adopted by ImageNet [12] to solicit large-scale image annotations atop Ethereum. To handle the task, Dragoon attains an on-chain (handling) cost $2 US dollars at the time of writing. In comparison, for the same task, the handling fee of MTurk is at least $4 currently [18, 34] . Our result provides an insight that the on-chain handling fee (characterizing the users' financial expense) in the decentralized setting can approximate or even less than the handling fee charged by centralized platforms. This hints the de facto users can financially benefit from decentralized crowdsourcing, though it is not contradictory to the common belief [35] that decentralization is more expensive regarding the overall system's cost.  Along the way, we firstly present the ideal functionality of decentralized HIT. This rigorous security model clearly defines what a secure HIT shall be, and allows us prove security against subtle adversaries in the blockchain, due to simulation-based paradigm. In contrast, existing decentralized HITs [19, 32] have quite different property-based definitions on \"securities\", which at least causes the lack of well-defined benchmark to compare. Even worse, many of them are flawed, as fail to capture all respects of subtle adversaries in blockchain; say, they allow a corrupted requester to reap data without paying, if being given the standard ability of adversarially re-ordering message deliveries. Differently, our simulation-based security model precisely defines the security against subtle attacks in the blockchain. Challenges & our techniques. The major challenge of making private decentralized HITs practical is that the blockchain must learn the quality of some encrypted answers, namely, to obtain some properties of what a few ciphertext are encrypting. The state-of-the-art [19, 32] proposed to reduce the problem to generic zk-proofs, by observing the requester can decrypt the answers, and then prove the quality of answers to the blockchain. But such the generic approach causes impractical expenses inherently, because of the underlying heavyweight NP-reduction for generality. To conquer the above challenge, we conduct a different path that deviates from generic zk-proof frameworks to explore a concretely efficient solution. At the core of our private decentralized HITs protocol, we present a special-purpose noninteractive proof scheme to efficiently attest the quality of encrypted answers. Such the approach gets rid of heavyweight general-purpose zk-proof frameworks and then avoids the inefficiency caused by generality.\n",
      "10\n",
      "ProveQuality VerifyQuality Public: G, Gs = {s i } iG , , c = c 1 . . . c N , h    for each i in G: (a i ,  i )  ProvePKE k (c) if a i = s i :     (i, a i ,  i ) output  for each (i, a i ,  i ) in : if a i  s i : output 0 if VerifyPKE h (a i , c i , ): output 0    + 1 output 1 : 0?   |G| Proof. (sketch) The completeness is immediate to see once considering the definition of quality function, the correctness of encryption and the completeness of VPKE. To prove the upper-bound soundness, we can assume by contradiction to let an adversary break it, then the adversary can immediately be leveraged to break the soundness of VPKE, which leads up to contradiction. The special zero-knowledge is also clear to see: considering |G| and the range of s i are small constants, we can construct a P.P.T. simulator S that invokes at most polynomial number of VPKE subroutines [49] to obtain VPKE proofs, thus allowing S to internally craft a simulated PoQoEA proof. The HITs contract functionality C L hit Given accesses to L, C hit interacts with R, {Wj}, and A. Phase 1: Publish Task  Upon receiving (publish, N, B, K, range, , h, commgs) from R, leak the message and R to A, until the beginning of next clock, proceed with the delayed executions down below: -send (freeze, Pi, B) to L, if returns (frozen, F L hit , Pi, B): * store N , B, K, range, , h and commgs * initialize answers  , comms   * send (published, R, N, B, K, range, , h, commgs) to all entities, and goto phase 2-a Phase 2-a: Collect Answers (Commit phase)  Upon receiving (commit, commc j ) from Wj, leak the message and Wj to A, then proceed with the following delayed executions until the beginning of next clock, with consulting A to re-order all received commit messages: -for each received commit message (sent from Wj): * if (Wj, ) /  comms and (, commc j ) /  comms:  let comms  comms  (Wj, commc j )  if |comms| = K, send (committed, comms) to all entities, and goto the reveal phase Phase 2-b: Collect Answers (Reveal phase)  Upon entering this phase, leak all received messages and their senders to A, till the next clock period, proceed as: -for each Wj  {Wj | (Wj, )  comms}: * if receiving the message (reveal, cj, key j ) from Wj such that Open(commc j , cj, key j ) = 1:  answers  answers  (Wj, cj) * else answers  answers  (Wj, ) -send (revealed, answers) to all, and goto the next phase Phase 3: Evaluate Answers  Upon entering this phase, leak all received messages and their senders to A, till the next clock period, proceed as: -if receiving (golden, G, Gs, key gs ) from R, such that Open(commsgs, G||Gs, key gs ) = 1: * for each Wj  {Wj | (Wj, )  answers}:  if receiving (outrange, Wj, i, a (i,j) , i) from R: send (pay, Wj, B/K) to L, if a (i,j)  range or VerifyPKE h (a (i,j) , c (i,j) , i) = 0  else if receiving (evaluate, Wj, j, ) from R: send (pay, Wj, B/K) to L, if j   or VerifyQuality h (cj, j, , G, Gs) = 0  else if cj = , send (pay, Wj, B/K) to L -otherwise, for each Wj  {Wj | (Wj, )  answers}, send (pay, Wj, B/K) to L\n",
      "11\n",
      "B. HIT contract and HIT protocol Now we are ready to present our concretely efficient decentralized protocol  hit for HIT. Our design centers around a smart contract C L hit , which is formally described in Fig 4. The contract C L hit is the crux to take best advantage of the rather limited abilities of blockchain to make our protocol securely realize the ideal functionality F L hit . Thus given contract C L hit , our HITs protocol  hit can be defined among the requester, the worker and the contract, as formally illustrated in Fig 5. Informally, our HIT protocol  hit proceeds as follows: 1) Publish task. The requester R announces her public key h, and publishes a task T of N multi-choice questions to crowdsource K answers for the task. Each question in T is specified to have some options in range. The task mixes some golden standard questions, whose indexes G and ground truth G s are committed to comm gs . Also, R places B as deposit to cover her budget, which promises that a worker would get a reward of B/K, if submitting an answer beyond a specified quality standard . 2) Commit answers. Once the task is published, the workers can commit their answers (encrypted to the requester) in the task. To prevent against copy-and-paste attacks, duplicated commitments are rejected. The contract moves to the next phase, once K distinct workers commit. 3) Reveal answers. After K workers commit their answers, these workers can start to reveal their answers in form of ciphertexts encrypted to the requester. Note that the submissions of answers explicitly contain two subphases, namely, committing and revealing, which is the crux to prevent the network adversary from taking advantages by adversarially scheduling the order of submissions. 4) Evaluate answers. Eventually, the requester is supposed to instruct the blockchain to correctly pay these encrypted answers to facilitate the critical fairness. To this end, the protocol leverages our novel notion of PoQoEA. So the requester can efficiently prove to the contract to reject a certain answer, if the worker does not meet the prespecified quality standard . If an answer is out of the specified range, the requester is allowed to use verifiable encryption VPKE to reveal that to reject payment. Remark. C L hit captures the essence of smart contracts [50] in reality, as it: (i) reflects the transparency of Turing-complete smart contract that is a stateful program handling pre-specified tasks publicly; (ii) captures a contract that can access the cryptocurrency ledger to honestly deal with conditional payments; (iii) models the network adversary who is consulted to schedule the delivering order of so-far-undelivered messages.\n",
      "12\n",
      "C. Instantiating cryptographic building blocks For sake of completeness, we hereafter give the constructions of cryptographic building blocks. Let G = g be a cyclic group of prime order p, where g is a random generator of G. (Short range) verifiable encryption (VPKE) is based on exponential ElGamal. The private key k $  Z p , the public key h = g k , the encryption is Enc h (m) = (c 1 , c 2 ) = (g r , g m h r ), and the decryption is Dec k ((c 1 , c 2 )) = log(c 2 /c k 1 ), where log is to brute-force the short plaintext range to obtain m; if decryption fails to output m  range, then c 2 /c k 1 is returned. In addition, to efficiently augment the above (Enc h , Dec k ) to be verifiable, we adopt a variant of Schnorr protocol [51] with Fiat-Shamir transform in random oracle model. In detail,  ProvePKE k ((c 1 , c 2 )). Run Dec k ((c 1 , c 2 )) to obtain m  range (or g m if m /  range). Let x $  {0, 1}  . Compute A = c x 1 , C = H(A|| c2 g m ), Z = x + kC, and  = (A, Z). If m  range, output (m, ); else, output (g m , ). The protocol of HITs  hit  hit is among the requester R, the workers {Wj} and C hit Phase 1: Publish Task  Requester R: -(Enc h , Dec k )  KeyGen(1  ) -Upon receiving the parameters G, Gs, , N , range, B, K of a HIT to publish: * key sg $  {0, 1}  * commgs  Commit(G||Gs, key sg ) * send (publish, N, B, K, range, , h, commgs) to C hit Phase 2: Collect Answers  Worker Wj: -Upon receiving (published, R, N, B, K, range, , h, commgs) from C hit : * get the answer aj = (a (1,j) ,    , a (N,j) ) * cj  (Enc h (a (1,j) ),    , Enc h (a (1,N ) )) * commc j  Commit(cj, key j ), where key j $  {0, 1}  * send (commit, commc j ) to C hit -Upon receiving (committed, comms) from C hit : * if (Wj, )  comms, send (reveal, cj, key j ) to C hit Phase 3: Evaluate Answers  Requester R: -Upon receiving (revealed, answers) from C hit : * send (golden, G, Gs, key gs ) to R * for each (Wj, cj)  answers:  decrypt each item in cj to get aj = (a (1,j) , (Dec(cj, skR) ; G, Gs) < :    , a (N,j) )  if a (i,j)  aj s.t. a (i,j) /  range:  (a (i,j) , i)  ProvePKE k (c (i,j) )  send (outrange, Wj, i, a (i,j) , i) to C hit  else if j = Quality    ProveQuality k (cj, j, G, Gs)  send (evaluate, Wj, j, ) to C hit  VerifyPKE h (M, (c 1 , c 2 ), ). Parse  = (A, Z). If M  range, compute C = H(A|| c2 g M ) and verify g M C  c Z 1 A  c C 2 , output 1 if the verification passes and 0 otherwise; else M  G, compute C = H(A|| c2 M ) and verify M C  c Z 1 A  c C 2  M /  {g m } mrange , output 1 iff the verification passes and 0 otherwise. Proof of quality of encrypted answer (PoQoEA) is built by invoking the above VPKE construction in a black-box manner, due to our reduction from PoQoEA to VPKE in V-A. Commitment scheme is due to the efficient folklore construction [44, 52] : (i) Commit(msg, key) = H(msg||key); (ii) Open(comm, msg , key ) = [H(msg ||key )  comm], where [] is Iverson bracket from a proposition to 1 (true) or 0 (false). D. Security analysis Theorem 1. Conditioned on the hardness of DDH problem and static corruptions, the stand-alone instance of  hit securely realizes F hit in C L hit -hybrid, random oracle model. Proof. (sketch) Let C denote the set of corrupted parties controlled by the adversary A, and let H denote the set of rest honest parties. For any P.P.T. adversary A in the real world, we can sketch a P.P.T. simulator in the ideal world to interact with the ideal functionality F hit and corrupted parties, such that S can emulate the actions of honest parties and the contract C hit . S proceeds as follows:  Publish Task (Phase 1). If R  C, considering that the corrupted R sends the publish message to C hit in the real world, S can trivially simulate that with interacting with F hit . If R  H, when the honest R sends the publish message to F hit , S is informed and thus allows S to simulate the phase of publish task (in the real world) for that R.  Collect Answers (Phase 2). In the real world, the P.P.T. adversary A might: (i) corrupt a set of parties C up to including the requester and a set of the workers, and (ii) is also consulted to reorder the so-far-undelivered messages sent to C hit (till the next clock). The basic strategy to emulate A is that: S invokes the adversary A to obtain how A is re-ordering the commit messages (sent from workers), let W to represent the set of workers whose commit messages are scheduled as the first K to deliver; then S delays all answer messages that are not sent from the workers in W. Then, S internally simulates the ciphertexts sent via reveal messages to open commitments. If R  H, the ciphertexts can be simulated as they are indistinguishable from the uniform distribution over the ciphertext space; if R  C, S is informed about all answer submissions sent from the workers, thus can internally simulate the submissions of the workers in the real world. Moreover, if A corrupts a worker whose commit message is scheduled in the first K to deliver but does not send any the reveal message to open the commitment, the simulator S can simulate that since it can let the corrupted worker to send an answer message containing  with F hit . In addition, it is trivial to see S can internally simulate the parties as well as C hit , when the adversary A corrupts a worker to submit duplicated commitment.  Evaluate Answers (Phase 3). The simulation become clear, if considering the security requirements of commitment scheme, VPKE, and PoQoEA. If the requester R  C, the simulator S invokes A to obtain all outrange and/or evaluate messages sent to C hit , and then simulates the interactions. If the requester R  H, whenever R sends outrange and/or evaluate messages to F hit , S is informed and hence is allowed to simulate the interactions between C hit and R in the real world.\n",
      "13\n",
      "VI. Dragoon: IMPLEMENTATION & EVALUATION To demonstrate the feasibility of our protocol, we implement it to build Dragoon, and then use the system to launch a typical image annotation task for ImangeNet [12, 47] atop Ethereum. System overview. Dragoon consists of an on-chain part and an off-chain part: the on-chain smart contract is deployed in Ethereum ropsten network; the requester client and worker clients are implemented in Python 3.6. The off-chain clients are installed in a PC that uses Ubuntu 14.04 LTS and equips Intel Xeon E3-1220V2 CPU and 16 GB main memory. ImageNet's HIT task. We demonstrate our system through an ImageNet task [12, 47] , which is specified as: each task is made of 106 binary questions, 100 out of which are nongold-standard questions, while the remaining 6 questions are requester's gold-standard challenges; 4 workers are allowed to participate; if a worker cannot correctly answer at least four golden standard questions, his submission will be rejected without being paid, otherwise he deserves to get the payment. Cryptographic modules. The hash function is instantiated by keccak256. We choose the cyclic group G by using the G 1 subgroup of BN-128 elliptic curve, over which all concrete public key primitives are instantiated. Code availability. The code of our prototype is available at https://github.com/njit-bc/dragoon. An experiment instance is atop Ethereum ropsten network (https://ropsten.etherscan.io/a ddress/0x5481b096c78c8e09c1bfbf694e934637f7d66698). Implementation details. Many non-trivial on-and off-chain optimizations are particularly made for practicability. Off-chain ends. The requester end warps: (i) an Ethereum node to interact with the blockchain, e.g. publish task, download workers' submissions, etc; (ii) the prover of verifiable encryption to generate necessary proofs to instruct the contract to reward workers; (iii) a Swarm API to publish the detailed questions of each crowdsourcing task. Swarm [53] is an offchain storage network, where the questions of HIT is stored; in addition, to ensure integrity of HIT questions, the digest of the questions is committed in the contract, which significantly reduces on-chain cost, without violating securities. The worker client wraps Ethereum to interact with the blockchain to read task and submit answers, and also incorporates Swarm client to allow download task questions. On-chain optimizations. We carefully perform a few nontrivial system-level optimizations to lighten the task contract: (i) we implement all public key schemes over G 1 subgroup of BN-128 [54] , since we can use some precompiled contracts in Ethereum to do algebraic operations there cheaply [33] ; (ii) it is expensive to store ciphertexts in the contract as internal variables, so we make the contract store their 256-bit hashes instead and let the actual ciphertexts included in the chain as emitted event logs [50] . Evaluations. We conduct intensive experiments to measure the concrete performance, and discuss the system feasibilities from the on-chain side and the off-chain side. Off-chain costs. First, Dragoon enables the requester to manage only one private-public key pair throughout all her tasks, because all protocol scripts are simulatable without secret key and therefore leak nothing relevant. More importantly, the off-chain cost of proving relevant cryptographic proofs is significantly reduced by removing unnecessary generality. * Through our evaluations, generic zk-proofs are instantiated by zk-SNARK, which is the only generic zk-proof feasibly supported by existing blockchains to our knowledge. Table I clarifies the requester suffers from hindersome offchain burden of generating generic zk-proofs. In contrast, our concrete constructions remove such bottleneck and boosts decentralized HITs practically. First, the requester can generate a proof to reject a worker's submission within only a few milliseconds, which costs nearly 2 minutes if using generic zkproof. Second, the concretely efficient constructions also save in memory usage. For example, by generic zk-proof, rejecting a submission requires a peak memory usage of 10 GB, which is reduced to only 53 MB by concrete constructions. On-chain costs. We measure the critical on-chain performance from many angles including the cost of verifying zkproofs and the on-chain gas usage of the whole protocol. First, we compare the verifying cost of concrete constructions and generic zk-proofs for VPKE and PoQoEA (six golden standards) in Table II . The concrete proof is fast, even compared to generic zk-proof (SNARK) known for efficient verification. For example, in the case of ImageNet task, only 6ms is needed to verify each concrete PoQoEA proof. Moreover, the overall handling fee of running a concrete ImageNet instance is summarized in Table III . To estimate the cost of on-chain usages, we apply a gas price at 1.5  10 9 Ether per gas, and an Ether price at 115 USD per Ether, which are the safe-low price of gas [55] and the market price of Ether on March/17th/2020, respectively. Under the above exchange rate, the on-chain handling fee paid by each worker is about $0.48, which is used to submit an answer. In addition, thanks to the efficient verification of PoQoEA, the requester can spend few cents to reject each low-quality answer. The overall onchain handling cost of the entire HIT is about two US dollars. In contrast, when MTurk facilitates the same ImageNet task, it charges a handling fee at least $4 currently [18, 34] . To summarize, Dragoon is practical. Our experiment even reveals that Dragoon's on-chain handling cost can be economically cheaper than the the handling fee charged by third-party platforms such as MTurk. In addition, Dragoon is compatible with many alternative chains (e.g., Cardano, Ethereum Classic and more) other than Ethereum, as long as the blockchains are using Ethereum Virtual Machine (EVM) as the running environment of smart contracts. So our system can be deployed in these alternative chains to further reduce the handling cost.\n",
      "14\n",
      "VII. CONCLUSION We design a decentralized protocol to crowdsource human knowledge, which, to our knowledge, is the first system in its kind that realizes both rigorous security and high efficiency. Open problems. It hints that the special-purpose protocols are promising to decentralize various crowdsourcing with highsecurity assurance as well as efficiency. It immediately corresponds to a few realistic problems to explore. For example, can we design a concretely efficient protocol to decentralize participatory crowd-sensing that is minimally meaningful with the needed fairness and privacy? Such the problem is challenging, since there seems no explicit requester to \"prove\" the quality of encrypted data anymore. Unfortunately, letting the blockchain learn encrypted data's quality (without a prover) falls into the category of (multi-input) functional encryption, which is unclear how can be solved practically till today. Another fundamental problem is that we consider security due to conventional cryptographic notions, where corrupted parties are fully controlled by an adversary and honest parties follow the protocol independently. The model has an inherent drawback to explain why rational workers would not deviate (e.g. by colluding). To resolve the concern, an \"incentivecompatible\" protocol is required, so \"following the protocol\" is a Nash equilibrium to deter rational workers from deviating.\n",
      "15\n",
      "ACKNOWLEDGMENT We would like to thank the anonymous reviewers for their valuable suggestions and comments about this paper.\n",
      "2\n",
      "Statement Reformation\n",
      "3\n",
      "Abstract real-world HITs The only quality-based incentive incorporated by Amazon's MTurk The ideas behind our efficient proving scheme are a variety of special-purpose optimizations to squeeze performance by removing needless generality, such that we reduce the problem of proving encrypted answers' quality from generic-purpose zk-proof to particular verifiable encryption. As shown in Fig  1, our core ideas are highlighted as:\n",
      "4\n",
      "Concretely efficient proof of quality  Abstracting real-world HITs. The first step is to well abstract an incentive widely adopted by real-world HITs, namely, the only one incorporated by Amazon's MTurk [10] . In the incentive, some golden standard challenges (i.e., questions with known answers) [7] are mixed with other questions, so the quality of a worker is due to her performance on the golden standards. 5 We carefully formulate the problem of proving the quality of encrypted answers for the above concrete incentive. So proving the quality of a worker can be reducible to a well-defined two-party problem, in which the verifier needs to output the performance of the worker on a set of golden standard questions, given only a set of ciphertext answering these golden standards challenges. Nevertheless, solving this two-party problem is still challenging, as it needs to compute the property of what a set of ciphertext are encrypting. The generic version of the issue falls into multi-input functional encryption [36, 37] , which is well known for its hardness, and has no (nearly) practical solution so far. We thus conduct the following optimizations to further reduce the problem.  Statement reformation. The major obstacle of removing the generic-purpose cryptographic frameworks is the arithmetic relations (i.e., some relationship unrepresentable in the algebraic domain). So we dedicatedly reform the statement of proving the quality of encrypted answers, to remove all arithmetic relations. We reform the statement mainly in two ways. First, we prove the upper bound of each worker's quality instead of proving the exact number, which is a relaxation in the general cases, but does not scarify any utility in our context where the reward is an increasing function of quality. Second, we realize that given the system's public knowledge, a tiny and constant portion of each worker's answer (i.e., the part answering gold standards) is already leaked, since this little portion becomes simulatable by the public knowledge; thus we explicitly relax our goal to leak these \"already-leaked\" information. To sum up, the above reformations allows us to remove needless generality of proving answer quality, so that we can reduce the problem to standard verifiable encryption without giving up securities/utilities.  Concretely efficient proving scheme. Following the above optimizations, the problem eventually is reduced to verifiable encryption, which becomes representable in concrete algebraic relations. Along the way, we present a certain variant of verifiable encryption that is concretely tailored for the scenario of HITs where the plaintexts are short, and thus squeeze most performance out of it. This completes our special-purpose design to boost private decentralized HITs, practically.\n",
      "5\n",
      "II. OTHER RELATED WORK Besides existing private decentralized HITs [19, 32] discussed earlier, here we briefly review some pertinent generic cryptographic frameworks and discuss their insufficiencies in the concrete context of private decentralized crowdsourcing. Privacy-preserving blockchain. A variety of studies [22, 38, 39] consider the general framework for privacy-preserving blockchain and smart contract. The approaches are powerful in the sense of their generality, yet are expensive for concrete usecases in practice. For example, Hawk [22] leverages generic zk-proofs to keep blockchain private, but incurs expensive proving expenses. As such, it is unclear how to leverage these generic frameworks to design concretely efficient protocol for the special-purpose of crowdsourcing [19] . Fair MPC using blockchain. Decentralized crowdsourcing is a special-purpose fair MPC using blockchain. Kiayias, Zhou and Zikas [21] consider the generic version of fair MPC in the presence of blockchain, but it is unclear how to adopt their generic protocol in practice without expensively computational costs. Recently, increasing interests focus on special-purpose variants of fair MPC in aid of blockchain. For example, [40] [41] [42] consider poker games. But these special-purpose solutions are over-tuned for distinct scenarios and are unclear how to be used for private decentralized crowdsourcing. Multi-input functional encryption. The core problem of private decentralized crowdsourcing is to let the blockchain learn the quality of encrypted answers, which is straightforwardly reducible to multi-input functional encryption (MIFE) [36] . But MIFE relies on indistinguishability obfuscation [36] or multi-linear maps [37] , which currently we do not notice how to instantiate under standard cryptographic assumptions.\n",
      "6\n",
      "III. PRELIMINARIES Here we briefly review some relevant cryptographic notions. Following convention, we let $  to denote uniformly sampling and  c to denote computationally indistinguishable. Cryptocurrency ledger. The cryptocurrency maintained atop the blockchain instantiates a global bookkeeping ledger (e.g. denoted by L) to deal with \"coin\" transfers, transparently. It can be called out by an ideal functionality (i.e., a standard model of so-called smart contract [21, 22] ) as a subroutine to assist conditional payments. Formally, cryptocurrency L can be seen as an ideal functionality interacting with a set of parties P = {P i } and the adversary; it stores the balance b i for each P i  P, and handles the following oracle queries [22, 43] :  FreezeCoins. On input (freeze, P i , b) from an ideal functionality F (i.e. a smart contract), check whether b i  b and proceed as follows: if the check holds, let b i = b i  b and b F = b F + b, send (frozen, F, P i , b) to every entity; otherwise, reply with (nofund, P i , b).  PayCoins. On input (pay, P i , b) from an ideal functionality F (i.e. a smart contract), check whether b F  b and proceed as follows: if that is the case, let b i = b i + b and b F = b F  b, send (paid, F, P i , b) to every entity. Commitment scheme. The commitment scheme is a twophase protocol among a sender and a receiver. In the commit phase, a sender \"hides\" a string msg behind a commitment string comm with using a blinding key, namely, the sender transmits comm = Commit(msg, key) to the receiver. In the reveal phase, the receiver gets key and msg as opening for comm, and executes Open(comm, msg , key ) to output 0 (reject) or (1) accept. We require computational hiding and computational binding. The former one requires the commitments of any two strings are computationally indistinguishable. The latter one means the receiver would not accept an opening to reveal msg = msg, with except negligible probability. Decisional Diffie-Hellman (DDH). DDH problem is to tell that d = ab or d $  Z p , given (g, g a , g b , g d ) where a, b $  Z p and g is a generator of a cyclic group G of order p. The DDH assumption states {(g, g a , g b , g d )}  c {(g, g a , g b , g ab )}. We assume DDH assumption holds along with the paper. Verifiable encryption. We consider the verifiable public key encryption scheme (VPKE) consisting of a tuple of algorithms (KeyGen, Enc, Dec, ProvePKE, VerifyPKE). In short, KeyGen can set up a pair of encryption-decryption algorithms (Enc h , Dec k ), where h and k are public and private keys respectively. We let any (Enc h , Dec k )  KeyGen(1  ) is a public key encryption scheme satisfying semantic security. For presentation simplicity, we also let (Enc h , Dec k ) denote the public-secret key pair (h, k). Moreover, for any (h, k)  KeyGen(1  ), the ProvePKE k algorithm explicitly inputs the private key k and the ciphertext c, and outputs a message m with a proof ; the VerifyPKE h algorithm explicitly inputs the public key h and (m, c, ), and outputs 1/0 to accept/reject the statement that m = Dec k (c). Beside semantic security, VPKE also satisfies the following extra properties:  Completeness. Pr[VerifyPKE h (m, c, ) = 1 | (m, )  ProvePKE k (c)] = 1, for  c and (h, k)  KeyGen(1  );  Soundness. Given any (h, k)  KeyGen(1  ) and any ciphertext c, any P.P.T. adversary A cannot produce a proof  fooling VerifyPKE h to accept that c is decrypted to m if m = Dec k (c), with except negligible probability;  Zero-knowledge. The proof  can be simulated by a P.P.T. simulator on input only public knowledge m, h and c, which ensures the protocol leaks nothing more than the truthness of the statement m = Dec k (c). Random oracle. We treat the cryptographic hash function as a global and programmable random oracle [44] , and denote the hash function with H through the paper. Simulation-based paradigm. To formalize and prove security, a real world and an ideal world can be defined and compared: (i) in the real world, there is an actual protocol  among the parties, some of which can be corrupted by an adversary A; (ii) in the ideal world, an \"imaginary\" trusted ideal functionality F replaces the protocol and interacts with honest parties and a simulator S. We say that  securely realizes F, if for  P.P.T. adversary A in the real-world,  a P.P.T. simulator S in the ideal-world, s.t. the two worlds cannot be distinguished, which means: no P.P.T. distinguisher D can attain non-negligible advantage to distinguish \"the joint distribution over the outputs of honest parties and the adversary A in the real world\" from \"the joint distribution over the outputs of honest parties and the simulator S in the ideal world\". Moreover, we consider static adversary who is only allowed to corrupt some parties before the protocol starts. Protocols proven secure in the real/ideal paradigm can be composed sequentially, due to the transitivity of security reductions [45] . The advantage of simulation-based paradigm is that all desired behaviors of the protocol can be precisely described by the ideal functionality. Remarkably, the approach has been widely adopted to analyze decentralized protocols [21, 22, 40] to capture subtle adversaries in the decentralized setting.\n",
      "7\n",
      "IV. FORMALIZATION OF DECENTRALIZED HUMAN INTELLIGENT TASKS This section rigorously defines our security model, by giving the ideal functionality of Human Intelligent Tasks (HITs) that captures the security/utility requirements of the state-of-theart HITs in reality [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] . Our security modeling sets forth a clear security goal, that is: the HITs in the real world shall be as \"secure\" as the HITs in an admissible ideal world. Reviewing the HITs in reality. Let us briefly review the HITs adopted in reality [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] , before presenting our abstraction of their ideal functionality. Parties & process flow. There are two explicit roles in a HIT, i.e., the requester and some workers. 6 The requester, uniquely identified by R, can post a task T to collect a certain amount of answers. In the task, R also promises a concrete reward policy. The worker with a unique identifier W j , submits his answer a j to expect receive the reward. Task design. A HIT consists of a sequence of questions denoted by T = (q 1 ,    , q N ), where each q i is a multiple choice question and N is the number of questions in the task. The answer of each question must lay in a particular range  N  0 pre-specified when T is published. The above HIT design is based on batched choice questions, which follows real-world practices [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] to remove ambiguity, thus letting workers precisely understand the task. For example, Fei-fei Li et al. [2, 12, 47] used the technique to create the deep learning benchmark ImageNet, and Andrew Ng et al. [3] suggested it for language annotations. Answer quality. The quality of an answer is induced by a function Quality(a j ; sp), where a j = (a (1,j) ,    , a (N,j) ) is the answer submitted by worker W j , and sp is some secret parameters of requester. The output of Quality() is denoted by  j , which is said to be the quality of worker W j . The above abstraction captures the quality-based incentive mechanism adopted by real-world HITs in Amazon's MTurk [10] [11] [12] [13] . For example, a task T consists of N questions, out of which M questions are golden-standard questions that are \"secretly\" mixed. The quality of a worker can be computed, due to her accuracy in the M golden-standard questions. Formally, in the qualify function Quality(a j ; sp), the parameter sp = (G, G S ), where G [1, N ] represents the randomly chosen indexes of the golden-standard questions, and G S = {s i |s i  range} iG represents the known answers of the golden-standard questions. Following the real-world practices [10] [11] [12] [13] , the quality of an answer a j = (a (1,j) ,    , a (N,j) ) is: Quality(a j , (G, G S )) = iG [a (i,j) s i ] where [] is Iverson bracket to convert any logic proposition to 1 if the proposition is true and 0 otherwise. Defining the decentralized HITs' functionality. Now we are ready to present our security notion of HITs in the presence of cryptocurrency. We formalize the ideal functionality of HITs (denoted by F hit ) in the L-hybrid model as shown in Fig 2. Intuitively, F L hit abstracts a special-purpose multi-party secure 6 There is an implicit registration authority (RA), who is required by realworld crowdsourcing platforms e.g. MTurk to prevent adversary forging a large number of identities (a.k.a. Sybil attackers). In practice, RAs can be instantiated by (i) the platform itself (e.g., MTurk), and (ii) the certificate authority who provides authentication service. Our solution can inherit these established RAs, and we therefore omits such the implicit RAs, with assuming all identities are granted. If the participants are interested in anonymity, anonymous-yet-accountable authentication scheme [19, 46] can be used; however, those are orthogonal techniques out scope of this paper.  The blue text shows F L hit is proceeding synchronously as the adversary can delay message deliveries up to next clock period [21, 22] ; the brown text means that F L hit has to proceed asynchronously as if the adversary can arbitrarily delay messages. computation, in which: (i) a requester recruits K workers to crowdsource some knowledge, and (ii) each worker gets a payment of B/K from the requester, if submitting an answer meeting the minimal quality standard . In greater detail, the ideal functionality F hit of HITs immediately implies the following security properties:  Fairness. Our ideal functionality captures a strong notion of fairness, that means: the worker get paid, if and only if s/he puts forth a qualified answer (instead of copying and pasting somewhere else). In greater detail, the requester specifies a sequence of N multi-choice questions, which are multi-choice questions having some options in range and contain |G| gold-standard challenges. 7 For each worker, s/he has to (i) meet a pre-specified quality standard  and (ii) submit answers in the range of options, in order to receive the pre-defined payment B/K.  Audibility of gold-standards. The choose of golden standards is up to the requester, so it becomes a realistic worry that a malicious requester uses some bogus as the answers of golden standard questions. The ideal functionality aims to abstract the best prior art [14, 15] regarding this issue so far, that means the golden standards become public auditable once the HIT is done. This abstraction \"simulates\" the ad-hoc reputation systems maintained by the MTurk workers to grade the reputations of the MTurk requesters in reality [14, 15] .  Confidentiality. It means any worker cannot learn the advantage information during the course of protocol execution. Without the property, workers can copy and paste to free ride, so it is a minimal requirement to ensure the usefulness of decentralized HITs. Our ideal functionality naturally captures the property. Adversary. We consider probabilistic polynomial-time adversary in the real world. It can corrupt the requester and/or some workers statically, before the real-world protocol begins. The uncorrupted parties are said to be honest. Following the standard blockchain model [21, 22] , we also abstract the ability of the real-world adversary to control the communication (between the blockchain and honest parties) as: (i) it follows the synchrony assumption [22, 48] , namely, we let there is a global clock [22, 48] , and the adversary can delay any messages sent to the blockchain up to a-priori known time (w.l.o.g., up to the next clock); (ii) the adversary can manipulate the order of so-far-undelivered messages sent to the blockchain, which is known as the \"rushing\" adversary. Expressivity of the ideal functionality F hit . Our ideal functionality of HITs is rather expressive, as it not only captures the elegant state-of-the-art of collecting image/language/video annotations [2-4, 11-13, 47] , but also reflects the common scenario of crowdsourcing human knowledge. Consider the next example: Alice is running a small startup, and aims to provide a service to visualize the availabilities of street parkings. Unfortunately, at each moment, Alice only knows the availabilities of street parkings at quite few spots, since she cannot afford the cost of monitoring every corner around the city. The little a-priori knowledge of Alice is her \"golden standards\", and such information is too little to boost a useful service. So Alice can crowdsource more street parking information from a few workers, with using her few golden standards to control the quality of solicited data. In light of the above discussion, it is fair to say that our abstraction is expressive to capture most real-world practices of crowdsourcing human knowledge (e.g. HITs in MTurk).\n",
      "8\n",
      "V. HITS PROTOCOL AND SECURITY ANALYSIS This section elaborates our practical protocol for decentralized HITs. We begin with an important building block for proving the quality of encrypted answers. Then we showcase the smart contract functionality C hit that interacts with the workers and the requester. Later, the detailed protocol is given in the presence of C hit . We finally prove that our protocol securely realizes the ideal functionality F hit of HITs.\n",
      "9\n",
      "A. Proof of quality of encrypted answer (PoQoEA) The core building block of our novel decentralized protocol is to allow the requester efficiently prove the quality of encrypted answers. We formally define this concrete purpose to set forth the notion of PoQoEA, and then present an efficient reduction from it to verifiable encryption (VPKE). Defining PoQoEA. The problem we are addressing here is to prove that: an encrypted answer c j can be decrypted to obtain some a j s.t. the quality of a j is , without leaking anything other than c j ,  and the parameters of quality function. To capture the problem, the state-of-the-art [19, 22] adopts the standard notion of zk-proof in order to support generic quality measurements. Different from existing solutions, we particularly tailor the notion of zk-proof to obtain a fine-tuned notion of PoQoEA for the widely adopted quality function defined in IV. Namely, we consider Quality( ; G, G s ) where G is the index of gold-standards and G s = {s i } iG is the ground truth of golden standards, and aim to remove the unnecessary generality in the concrete setting. Precisely, given the quality function Quality( ; G, G s ) and any established public key encryption scheme (Enc h , Dec k )  KeyGen(1  ), we can define PoQoEA as a tuple of hereunder algorithms (ProveQuality k , VerifyQuality h ): 1) ProveQuality k (c j , , G, G s )  . Given the encrypted answer c j = (c 1,j , . . . , c N,j ), the quality , and the golden standards (G, G s ), it outputs a proof  attesting  is the quality of c j ; the algorithm explicitly takes the secret decryption key k as input; 2) VerifyQuality h (c j , , , G, G s )  0/1. It outputs 0 (reject) or 1 (accept), according to whether  is a valid proof attesting  is the actual quality of c j ; the algorithm explicitly takes the public encryption key h as input; Moreover, PoQoEA shall satisfy the following properties:  Rationale behind the finely-tuned abstraction. The notion of PoQoEA is defined to remove needless generality in the special case of HITs. Compared to the state-of-the-art notion [19] , PoQoEA is more promising to be efficiently constructed, as it brings the following definitional advantages: ) = 1   < Quality(a j ; G, G s )  a j = Dec k (c j ) |   A(G, G s , , c j , , Enc h , Dec k )]  negl(), where negl() is  We adopt upper-bound soundness to prove the upper bound of quality instead of proving the exact quality of each worker. Such the tuning stems from a basic fact that: the reward of a worker is an increasing function in quality, so the upper bound of the worker's quality exactly reflects the well-deserved reward of the worker.  Another major difference is the relaxed special zeroknowledge, which means: PoQoEA is zero-knowledge, when |G| and s i  G s are small, so anything simulatable by the gold standards can be leaked. Nevertheless, the conditions are prevalent in the special context of HITs [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] , because G represents few golden standard questions, and the range of s i represents the options of each multiple-choice question in HITs, thus both of which are small in reality. In sum, even though PoQoEA is seemingly over-tuned, it essentially coincides with the generic zk-proof of the quality of encrypted answers in the context of HITs. Construction and security analysis. Here is an efficiencydriven way to constructing PoQoEA with the quality function Quality(a j ; G, G s ) defined in IV. We reduce the problem to the standard notion of verifiable encryption. More precisely, if being given (Enc h , Dec k , ProvePKE k , VerifyPKE h ) that is an established verifiable encryption scheme, PoQoEA for (Enc h , Dec k ) can be constructed as illustrated in Fig 3.\n",
      "Using_Out-of-Domain_Data_for_Lexical_Addressee_Detection_in_Human-Human-Computer_Dialog\n",
      "OUT/Using_Out-of-Domain_Data_for_Lexical_Addressee_Detection_in_Human-Human-Computer_Dialog/\n",
      "../papers-xmls/Using_Out-of-Domain_Data_for_Lexical_Addressee_Detection_in_Human-Human-Computer_Dialog.tei.xml\n",
      "1_INTRODUCTION\n",
      " Thanks to recent advances in speech recognition and natural language understanding, VAs have become part of our daily lives. The VAs are typically activated by a wake-up word/phrase such as hi Mercedes, hey BMW, hey Siri, Alexa or ok Google. Eliminating such wake-up words in favor of allowing direct requests for assistance from the VA could significantly improve the user experience. This requires the device to have the capability to detect speech directed at it and ignore human-to-human and background speech. The problem of classifying spoken utterances into system-directed and nonsystem-directed has previously been investigated within the context of virtual assistants [1, 2, 3] and dialogue systems [4, 5] . Both, the spoken words and the way they are spoken provide cues for differentiating between system-directed and non-systemdirected speech utterances. Typically, the lexical cues are extracted from a word sequence generated by an automatic speech recognition (ASR) system. The classification can then be performed by applying two language models [6, 7] , one for each class, to the hypothesized word sequence to compute a likelihood ratio and choose the class label based on that. Alternatively, the word sequence can be input to * The author performed the work while an intern at Nuance a neural network (NN) model to either directly estimate class probabilities [8] or to generate new features for another model [1] . The non-lexical acoustic cues can be learned from features corresponding to prosodic structure [2, 6] or the short-time frequency representation of the speech signal [1, 3] . The frequency-based features are typically extracted using a sliding window of 20-25 ms. One of the challenges involved in using frame-based features for utterance classification is to represent all utterances with a fixed-dimensional vector to train a model regardless of the length of the utterance. Averaging the features over time [3] or passing the input sequence into a long short-time memory (LSTM) cell and using the last output of the LSTM cell [1] are examples of how others have dealt with this issue. In this paper, we propose a new technique based on attention to address some shortcomings of the other methods. Similar to systems developed in [1, 2, 3, 6, 9] our plan is to combine information extracted from acoustic features with lexical information for this classification task, however, the focus of this paper is only on acoustic-based classification. The acoustic features explored for this purpose are frame-based log Mel-filterbank coefficients. We favor short-term frame-based acoustic features for this task since they facilitate early detection of user's intent by gradual application of the trained model on the incoming speech. The proposed classification models are based on deep neural networks (DNN) with combination of convolutional, recurrent and feed-forward layers. In addition, the use of an attention mechanism on top of the convolutional layer as well as the recurrent layer is investigated. This paper is organized as follows. First, an overview of the models developed for this classification problem is presented in 2. A number of model architectures proposed for frame-based approach and the architecture of the utterance-based model are described in Section 3. The experimental study containing a description of the evaluation data, the model parameters, and the experimental results is provided in Section 4. Summary and conclusion are given in Section 5.\n",
      "13\n",
      "ACKNOWLEDGEMENTS The authors would like to thank two colleagues from Nuance Communications, Raymond Brueckner for helping out with the openSmile toolkit and Yasser Hifny for very insightful discussions.\n",
      "2_OVERVIEW\n",
      " Here, an overview of the two modeling approaches investigated for the system-directed versus non-system-directed classification problem is given. The two approaches are based on using frame-level and utterance-level input features. The models developed based on the frame-level input features are depicted in Figure 1 on the left and the utterance-based model in shown on the right. Several architectures are explored for the frame-based models as described in Section 3 for dealing with the variable length input sequences. The model developed for the fixed-length utterance-level features is comprised of only dense feed-forward layers.\n",
      "3_MODEL_ARCHITECTURES\n",
      " This section presents a number of model architectures for dealing with the issue of variable-length input feature representation faced in the frame-based approach. Moreover, it describes the input features and the architecture of the utterance-based model in detail. In this approach the feature vectors input to the models consist of 45 log Mel-filterbank coefficients extracted from 25 ms of acoustic signal with a frame shift of 10 ms. A speech utterance is hence represented by a sequence of feature vectors, {m 45 1 , . . . , m 45 T }, where T is the total number of frames in the utterance. All frame-based models developed here use a two-dimensional convolutional layer as input layer which outputs a set of d feature maps denoted by {E jl 1 , . . . , E jl d }. The width of the feature maps, j, is proportional to the acoustic feature vector size (i.e., 45) and their length l is proportional to T . In a realistic scenario, recorded utterances have different lengths which means T and consequently l vary from one utterance to another. This causes an issue when converting the feature-maps into a vector to pass to feed-forward layers since the input to a feed-forward layer has to have a fixed-dimension for all samples. In the following, three approaches for creating a fixedlength vector from variable-length feature-maps are presented. After creating a fixed-length vector it is input to dense feed-forward layers followed by a softmax layer as shown in Figure 1 . Global averaging across time: A simple way of generating a fixed-length representation is to take the average of each featuremap over its length l. This will transform every 2-D feature map E jl i to a vector e j i . The resulting vectors, {e j 1 , . . . , e j d }, are then concatenated and fed to a feed-forward layer as was done in [10] . Using a recurrent layer: One could obtain a fixed-length vector from variable-length feature maps by using a recurrent neural layer. This is done by first concatenating columns of all feature maps to generate l super vectors of dimension d  j. Next, the super vectors are fed to a recurrent layer one by one and the last (i.e., lth) output of the recurrent layer is used for the succeeding layer. In addition, one could use a bi-directional recurrent layer and use the last output vector of forward and backward directions to obtain a richer fixed- length representation. In the model explored here, a bi-directional LSTM layer is used for this purpose and the two resulting vectors from both directions are concatenated and used in the feed-forward layer. Using attention mechanism: Simple averaging of feature maps or passing them through a recurrent layer and using only its last output could result in losing important information. An attention mechanism could retain most of the relevant information while resolving the variable-length issue. The attention mechanism explored here is somewhat different from the traditional encode-decoder based attention introduced in [11] . It is in essence a weighted average of sequence of vectors where the weights are learned through backpropagation. This mechanism was first explored for emotion recognition in [12] and is similar to the idea of self-attention in [13] . Denoting a sequence of l vectors of dimension s by the matrix X sl , attention is computed as b l1 = f (w 1s X sl ),  i = exp(bi) l j=1 exp(bj) , i = 1, . . . , l, (1) where w is the weight vector learned through back-propagation, f is a non-linear function (here tanh), b is the attention vector, and  is the normalized attention vector. Applying attention to the input sequence results in a vector known as context vector given by c s1 = X sl  l1 . (2) As can be seen in Equation 2, the context vector dimension is independent of the length of the input sequence l. Furthermore, the attention vector  helps to put more emphasis on the parts of the input sequence X that carry the most relevant information for distinguishing the two classes. The process of computing the attention and applying it to the input sequence is shown in Figure 2 . The input sequence in this case could be the flattened feature maps or the output of the recurrent layer. As an alternative to the frame-based approach one could represent every utterance with a fixed-length feature representation prior to any modeling. This can be done by computing some functions over the frame-based features. The feature set used here was developed for INTERSPEECH ComParE emotion recognition sub-challenge [14] . It contains 6373 acoustic-features described in [15] . We used the openSmile toolkit [16] for extracting these features from the speech utterances in our corpus. Although, this feature set was originally developed for an emotion recognition task, it contains a variety of acoustic-prosodic features (F0, energy, zero crossing, mfccs) many of which are relevant for this classification problem as well. A threelayer dense feed-forward model is trained and evaluated for these features.\n",
      "4_EXPERIMENTAL_STUDY\n",
      " This section provides a description of the two data sets used for training and evaluation of the classifier. Afterwards, the model parameter are given and finally experimental results are presented and analyzed. Two datasets are used for evaluation of the proposed techniques. The first dataset denoted by D1 contains recordings from a device with virtual assistant. The recordings contain system-directed utterances including questions and commands as well as non-system directed utterances mostly consisting of people dictating phrases. These are actual recordings from multiple users in different environments. The second dataset denoted by D2 also contains virtual assistant recordings but in addition it includes, background speech, open microphone recordings and some non-speech noise in the non-systemdirected subset. The models are only trained on the training subset of D1 and the dataset D2 was only used for testing. Table 1 shows the breakdown of both datasets by class and training/validation/test. Same number of training utterances from both classes where chosen for training the models to prevent them from being biased towards one class. Prior to training the models, the frame-based and utterance-based feature vectors are normalized to have zero mean and unit standard deviation along each dimension to facilitate model convergence.  System 35k 12k 14k 7k Non-system 35k 5k 4k 127k Moreover, Adam optimizer and early stopping are used for training the models. For the frame-based models, the convolutional layer has a depth of 50 with a kernel height of 20 and width of 9 for the models without the LSTM layer and width of 5 for the models with LSTM layer. A stride of 5 is used along the time access and 3 along the log Mel-filterbank coefficients. The LSTM layer is bi-directional with 128 units in each direction. The three feed-forward layers in frame-based models each have 128 units. The best classification accuracy was obtained from the utterance-based model when three feed-forward layers of 128 units were used. All the models were trained using tensorflow toolkit [17] . In this section the classification models described in Section 2 are evaluated on D1 and D2 datasets defined in Section 4.1. Figure 3 shows the performance of the models in terms of detection error tradeoff (DET) curves. A number of observations can be made from these plots. First, using an LSTM significantly improves the model performance compared to global averaging. Moreover, adding attention to the mix yields an additional boost to the performance with and without the LSTM. Furthermore, having larger improvement with attention on D2 dataset which was not seen in training suggests that the proposed attention mechanism improves model generalization as well. To have a single point of reference to compare the models, the equal error rate metric which corresponds to equal Type I and Type II errors is measured and shown in Table 2 . The main question here is what the model is actually learning. This is not easy to answer especially when it comes to neural network models. However, the attention mechanism could help shed-  ding some light on this matter. Aligning the attention vector  with the original speech utterance, one could find out where the model is putting the most emphasis. This is done in Figure 4 for two utterances from the two classes. The word sequences associated with the utterances are also shown in the figure to identify possible correlations between the spoken words and where the model is mostly focusing on. The vertical lines correspond to start and end time of the words. The plot shows that for the system-directed utterance the attention is on both \"play\" and \"music\" while for the non-systemdirected example the attention is mostly on the words \"that\", \"fine\", and \"period\". It is interesting to note that in the training dataset the word \"period\" is spoken only when the users are dictating a phrase. In other words, this word is a strong indication that the speech utterance is of dictation style which belongs to non-system-directed class. This led us to think that maybe the model is just learning keywords and is not learning any para-linguistic information. To answer this question we looked at a number of system-directed utterances such as \"you didn't catch that\" and \"one more run after that\" that were not part of the training data and did not contain any word highly correlated with system-directed class. The model classified both of these utterances correctly with high confidence. This suggests that the model is not just learning keywords or para-linguistic information but rather a combination of both. Adding more training data from different domains would make the model less sensitive to words and more sensitive to para-linguistic information. In Figure 5 , the proposed frame-based approach is compared to the utterance-based approach described in Section 3.2 on the D1 dataset. It should be noted that the utterance-based acoustic-prosodic feature set was designed for emotion recognition and contains several features that may not be relevant for this task. Nevertheless, the gap between the two curves indicates that even without using handcrafted features and only relying on frame-based log Mel-filterbank features very good classification performance can be achieved with the proposed attention-based modeling technique.\n",
      "5_CONCLUSION\n",
      " In this paper, the problem of classifying speech utterances into system-directed and non-system-directed was addressed. A number of neural network architectures based on using convolutional and recurrent layers were investigated. It was shown that having an attention mechanism improves the classification performance whether applied directly to the output of the convolutional layer or to the output of the recurrent layer. The best performing model was built by stacking a convolutional layer, a recurrent layer and three feed-forward layers with attention applied to the output of the recurrent layer. This model achieved an EER rate of 16.25% on one test set and 15.62% on the second test set. As continuation of this work we are looking into combining direct audio classification with ASR-output based text classification for improved accuracy.\n",
      "Learning_to_Optimize_Combinatorial_Functions\n",
      "OUT/Learning_to_Optimize_Combinatorial_Functions/\n",
      "../papers-xmls/Learning_to_Optimize_Combinatorial_Functions.tei.xml\n",
      "1\n",
      "Introduction Combinatorial optimization aims to optimize an objective function over a set of feasible solutions defined on a discrete space. Numerous real-life decision-making problems can be formulated as combinatorial optimization problems (Korte et al. 2012; Trevisan 2011) . In the last decade, development of time-efficient algorithms for combinatorial optimization problems paved the way for these algorithms to be widely utilized in industry, including, but not limited to, in resource allocation (Angalakudati et al. 2014 ), efficient energy scheduling (Ngueveu, Artigues, and Lopez 2016), price optimization (Ferreira, Lee, and Simchi-Levi 2015) , sales promotion planning (Cohen et al. 2017) , etc. The last decade has, in parallel, witnessed a tremendous growth in machine learning (ML) methods, which can produce very accurate predictions by leveraging historical and contextual data. In real-world applications, not all parameters of an optimization problem are known at the time of Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. execution and predictive ML models can be used for estimation of those parameters from historical data. For instance, Cohen et al. first predicted future demand of products using an ML model and then use the predicted demand to compute the optimal promotion pricing scheme over the products through non-linear integer programming. When predictive ML is followed by optimization, it is often assumed that improvements in the quality of the predictions (with respect to some suitable evaluation metric) will result in better optimization outcomes. However, ML models make errors and the impact of prediction errors is not uniform throughout the underlying solution space, for example, overestimating the highest-valued prediction might not change a maximization problem outcome, while underestimating it can. Hence, a better prediction model may not ensure a better outcome in the optimization stage. In this regard, Ifrim, OSullivan, and Simonis (2012) experienced that a better predictive model does not always translate to optimized energy-saving schedules. The alternative is to take the effect of the errors on the optimization outcome into account during learning. In the context of linear programming problems, Elmachtoub and Grigas proposed an approach, called \"Smart Predict and Optimize\" (SPO), for training ML models by minimizing a convex surrogate loss function which considers the outcome of the optimization stage. Specifically they consider optimization problems where predictions occur as weights that are linear in the objective. In this work, we build on that approach and consider discrete combinatorial optimization problems. Indeed, the SPO loss is valid for any optimization problem with a linear objective over predictions, where the constraints implicitly define a convex region. Furthermore, any black box optimization method can be used as only its outcome is used to compute the (sub)gradient of the loss. The main challenge is the computational cost of the repeated solving of the optimization problem during training, namely once for every evaluation of the loss function on an instance. For NP-hard problems, this may quickly become infeasible. In order to scale up to large problem instances, we investigate the importance of finding the optimal discrete solution for learning, showing that continuous relax-ations are highly informative for learning. Furthermore, we investigate how to speed up the learning by transfer learning from easier-to-learn models as well as method for speeding up the solving by warmstarting from earlier solutions. Our approach outperforms the state-of-the-art Melding approach (Wilder, Dilkina, and Tambe 2019) in most cases, and for the first time we are able to show the applicability of predict-and-optimize on large scale combinatorial instances, namely from the ICON energy-cost aware scheduling challenge (Simonis et al. ).\n",
      "10\n",
      "RQ2 benefits of warmstarting As baseline we use the standard SPO-relax approach. We test warmstarting the learning by first training the network with MSE as loss function for 6 epochs, after which we continue learning with SPO-relax. We indicate this approach by MSE-warmstart. We summarizes the effect of warmstarting in Table 1 , We observe that warmstarting from MSE results in a slightly faster start in the initial seconds, but this has no benefit, nor penalty over the longer run. Warmstarting from an earlier basis, after the MIP pre-solving, did result in runtime improvements overall. We also attempted warmstarting by adding objective cuts, but this slowed down the solving of the relaxation, often doubling it, because more iterations were needed.\n",
      "11\n",
      "RQ3: SPO versus QPTL Next, we compare our approach against the state-of-the-art QP approach (QPTL) of Wilder, Dilkina, and Tambe (2019) which proposes to transform the discrete linear integer program into a continuous QP by taking the continuous relaxation and a squared L2-norm of the decision variables ||x|| 2 2 . This makes the problem quadratic and twice differentiable allowing them to use the differntiable QP solver (Donti, Amos, and Kolter 2017) . Figure 4 shows the average regret on all unweighted and weighted knapsack instances and easy scheduling instances. We can see that for unweighted knapsack, SPO-relax almost always outperforms the other methods, while QPTL performs worse than MSE-r. For weighted knapsacks, SPOrelax is best for all but the lower capacities. For these lower capacities, QPTL is better though its results worsen for higher capacities. The same narrative is reflected in Figure 5 . In Figure 5c (weighted knapsack, capacity:60) QPTL converges to a better solution than SPO. In all other cases SPO-relax produces better quality of solution and in most cases QPTL converges slower than SPO-relax. The poor quality of QPTL at higher capacities may stem from the squared norm which favors sparse solutions, while at high capacities, most items will be included and the best solutions are those that identify which items not to include. On two energy-scheduling instances SPO-relax performs better whereas for the other instance, the regrets of SPOrelax and QPTL are similar. From Figure 5f and 5e, we can see, again, SPO-relax converges faster than QPTL.\n",
      "12\n",
      "RQ4: Suitability on large, hard optimisation instances While SPO-relax performs well across the combinatorial instances used so far, these are still toy-level problems with relatively few decision variables that can be solved in a few seconds. We will use the large-scale optimization instances of the ICON challenge, for which no exact solutions are known. Hence, for this experiment we will report the regret when solving the relaxation of the problem for the test instances, rather than solving the discrete problem during testing as in the previous experiments. We impose a timelimit of 6 hours on the total time budget that SPO-relax can spend on calling the solver. This includes the time to compute and cache the ground-truth solution of Table 2 , for 5 hard scheduling instances. First, we show the test (relaxed) regret after 2, 4, 6 and 8 MSE-r epochs. The results show that the test regret slightly decreases over the epochs; thereafter, we observed, regret tends to increase. With SPO-relax, in 6 hours, it was possible to train only on 300 to 450 different instances, which is only 50 to 80% of the training instances. Table 2 shows even for a limited solving budget of 6 hour and without MSE-warmstarting, it already outperforms the MSE learned models. This shows that even on very large and hard instances that are computationally expensive to solve, training with SPOrelax on a limited time-budget is better than training in a two-stage approach with a non optimisation-directed loss.\n",
      "13\n",
      "Conclusions and future work Smart \"Predict and Optimize\" methods have shown to be able to learn from, and improve task loss. Extending these techniques to be applicable beyond toy problems, more specifically hard combinatorial problems, is essential to the applicability of this promising idea. SPO is able to outperform QPTL and lends itself to a wide applicability as it allows for the use of black-box oracles in its loss computation. We investigated the use of weaker oracles and showed that for the problems studied, learning with SPO loss while solving the relaxation leads to equal performance as solving the discrete combinatorial problem. We have shown how this opens the way to solve larger and more complex combinatorial problems, for which solving the exact solution may not be possible, let alone to do so repeatedly. In case of problems with weaker relaxations, one could consider adding cutting planes prior to solving (Ferber et al. 2019) . Moreover, further improvements could be achieved by exploiting the fact that all previously computed solutions are valid candidates. So far we have only used this for warmstarting the solver. Our work hence encourages more research into the use of weak oracles and relaxation methods, especially those that can benefit from repeated solving. One interesting direction are local search methods and other iterative refinement methods, as they can improve the solutions during the loss computation. With respect to exact methods, knowledge compilation methods such as (relaxed) BDDs could offer both a runtime improvement from faster repeat solving and employing a relaxation.\n",
      "14\n",
      "Acknowledgments We would like to thank the anonymous reviewers for the valuable comments and suggestions. This research is supported by Data-driven logistics (FWO-S007318N).\n",
      "2\n",
      "Related Work Predict-and-optimize problems arise in many applications. Current practice is to use a two-stage approach where the ML models are trained independent of the optimization problem. As a consequence, the ML models do not account for the optimization tasks (Wang et al. 2006; Mukhopadhyay et al. 2017) . In recent years there is a growing interest in decision-focused learning (Elmachtoub and Grigas 2017; Demirovi et al. 2019; , that aims to couple ML and decision making. In the context of portfolio optimization, Bengio (1997) report a deep learning model fails to improve future profit when trained with respect to a standard ML loss function, but a profit-driven loss function turns out to be effective. Kao, Roy, and Yan (2009) consider an unconstrained quadratic optimisation problem, where the predicted values appear linearly with the objective. They train a linear model with respect to a combination of prediction error and optimization loss. They do not mention how this can be applied to optimization problems with constraints. A number of works aim to exploit the fact that the KKT conditions of a quadratic program (QP) define a system of linear equations around the optimal points. For instance, Donti, Amos, and Kolter (2017) propose a framework which computes the gradient of the solution of the QP with respect to the predictions by applying the implicit function theorem to differentiate through the KKT conditions around the optimal point. Wilder, Dilkina, and Tambe (2019) use the same approach, and propose its use for linear programs by adding a small quadratic term to convert it into a concave QP. They also propose a specialisation of it for submodular functions. Our work builds on the SPO approach of Elmachtoub and Grigas (2017) , where the authors provide a framework to train an ML model, which learns with respect to the error in the optimization problem. This is investigated for linear optimization problems with a convex feasible region. We will use the approach for discrete combinatorial problems with a linear objective. They are computationally expensive to solve, e.g. often N P-hard. The decision variables and search space of these problems is discrete, meaning gradients can not be computed in a straightforward manner. However, the SPO approach remains applicable as we will see. Demirovi et al. (2019) investigate the predic-tion+optimisation problem for the knapsack problem, and prove that optimizing over predictions are as valid as stochastic optimisation over learned distributions, in case the predictions are used as weights in a linear objective. They further investigate possible learning approaches, and classified them into three groups: indirect approaches, which do not use knowledge of the optimisation problem; semi-direct approaches, which encode knowledge of the optimisation problem, such as the importance of ranking and direct approaches which encode or use the optimisation problem in the learning in some way (Demirovi et al. 2019) . Our approach is a direct approach and we examine how to combine the best of such techniques in order to scale to large and hard combinatorial problems.\n",
      "3\n",
      "Problem Formulation and Approach Optimizing a parameterized problem Traditional optimization algorithms work under the assumption that all the parameters are known precisely. But in a predict-and-optimize setup we assume some parameters of the optimization problem are not known. We formalize a combinatorial optimization problem as follows : v * ()  arg min v f (v, ) s.t. C(v, ) (1) where  defines the set of parameters (coefficients) of the optimization problem, v are the decision variables, f (v, ) is an objective to be minimized, and C(v, ) is a (set of) constraints that determine(s) the feasible region; hence v * () is an oracle that returns the optimal solution. Consider, the 0-1 knapsack, where a set of items, with their values and wights, are provided. The objective is to select a subset of items respecting a capacity constraint on the sum of weights so that the total value of the subset is maximized. The parameter set  of the problem consists of the value and weight of each item and the total capacity. The decision variable set consists of 0-1 decision variable for each item, f is a linear sum of the variables and the item values and C describing the capacity constraint. We decompose  =  s   u where  s are the set of parameters that are observed (e.g. the weights and capacity of a knapsack) and  u are the set of unobserved parameters (the value of the knapsack items). To predict the unobserved parameters  u , some attributes correlated with them are observed. We are equipped with a training dataset D : {(x 1 ,  u1 ), ..., (x n ,  un )} where x i 's are vectors of attributes correlated to  ui . An ML model m is trained to generate a prediction u = m(x; ). The model m is characterized by a set of learning parameters . E.g. in linear regression the  encompasses the slope and the intercept of the regression line. Once the predicted u are obtained,  =  s  u is used for the optimization problem. To ease notation, we will write = ( s , u ), containing both the observed parameters and predicted parameters. Recall from Equation (1) that v * () is the optimal solution using parameters. Then, the objective value of this solution is f (v * (), ). Whereas if the actual  is known a priori, one could obtain the actual optimal solution v * (). The difference in using the predicted instead of the actual values is hence measured by regret(,)  f (v * (), )  f (v * (), ) (2) Ideally the aim of the training should be to generate predictions which minimize this regret on unseen data.\n",
      "4\n",
      "Two Stage Learning First we formalize a two-stage approach, which is widely used in industry and where the prediction and the optimization are performed in a decoupled manner. First the predictive model is trained with the objective of minimizing the expected loss for a suitable choice of loss function L( u , u ) For regression, if we use squared-error as the loss function, model parameters  are estimated by minimizing the Mean Squared Error (MSE) on the training data: L M SE = 1 n n i=1 ( ui  ui ) 2 2 (3) Algorithm 1: Stochastic Batch gradient descent for the two-stage learning for regression tasks (batchsize:N ) and learning rate  repeat Sample N training datapoints for i in 1, ..., N do predict ui using current  L i  ( ui   ui ) //gradient of L M SE end L = N i=1 Li N      * L * u  until convergence; Training by gradient descent The process of estimating  to minimize the loss function is executed through stochastic gradient descent algorithms 1 ,where at each epoch,  are updated after calculating the gradient L of the loss function with respect to the predictions as shown in Algorithm 1. The advantage of the two-stage approach is that training by gradient descent is straightforward. In the prediction stage the objective is to minimize the MSE-loss to generate accurate predictions without considering their impact on the final solution. Clearly this does not require solving the optimization problem during training. Model validation and early stopping with regret It is common practice to perform model validation on a separate validation set while training the model. Early stopping (Bishop 2006) is the practice of choosing the epoch with the smallest loss on the validation set as the final output of the training procedure. The objective is to avoid overfitting on training data. The performance on the validation set is also used to select hyperparameters of the ML models (Bergstra and Bengio 2012) . Considering the final task, in our setup, is minimizing the regret, we modify the two stage learning by measuring regret on the validation set for early stopping and hyperparameter selection. We call this the MSE-r approach. It is more computationally expensive than MSE given that computing regret on the validation data for every epoch requires solving the optimization problems each time.\n",
      "5\n",
      "Smart Predict then Optimize (SPO) The major drawback of the two-stage approach is it does not aim to minimize the regret, but minimizes the error between  u and u directly. As Figure 1 shows, minimizing loss between  u and u does not necessarily result in minimization of the regret. Early stopping with regret can avoid worsening results, but can not improve the learning. The SPO framework proposed by Elmachtoub and Grigas addresses this by integrating prediction and optimization. Note, to minimize regret(,) directly we have to find the gradient of it with respect to which requires differentiating the argmin operator v * () in Eq. 1. This differentiation may not be feasible as v * () can be discontinuous in and exponential in size. Consequently we can not train an ML model to minimize the regret through gradient descent. The SPO framework integrates the optimization problem into the loop of gradient descent algorithms in a clever way. In their work, Elmachtoub and Grigas consider an optimization problem with a convex feasible region S and a linear objective : v * ()  arg min vS  v (4) where the cost vector  is not known beforehand. Following Eq. (2), the regret for such a problem when using predicted values instead of actual  is:  (v * ()  v * ()) , which as discussed is not differentiable. To make it differentiable, they use a convex surrogate upper bound of the regret function, which they name the SPO+ loss function L SP O+ (,). The gradient of L SP O+ (,) may not exist as it also involves the argmin operator. However, they have shown that v * ()  v * (2  ) is a subgradient of L SP O+ (,), that is g(,) = v * ()  v * (2  ), g(,)  L SP O+ (,) (5) The subgadient formulation is the key to bring the optimization problem into the loop of gradient descent as shown in algorithm 2. The difference between algorithm 1 and algorithm 2 is in their (sub)gradients. In Algorithm 1, the MSE gradient is the signed difference between the actual values Algorithm 2: Stochastic Batch gradient descent for the SPO approach for regression tasks (batchsize:N ) and learning rate  repeat Sample N training datapoints for i in 1, ..., N do predict ui using current  compute v * (2  ) L i  v * ()  v * (2  ) //sub-gradient end L = N i=1 Li N      * L * u  ; until convergence; and predicted ones; in Algorithm 2 the SPO subgradient is the difference of an optimization solution obtained using the actual parameter values and another solution obtained using a convex combination of the predicted values and the true values. For the 0-1 knapsack problem, the solution of a knapsack instance is a 0-1 vector of length equal to the size of the set, where 1 represents the corresponding item is selected. In this case, the subgradient is the element-wise difference between the two solutions and if the solution using the transformed predicted values is the same as the solution using actual values, all entries of the subgradient are zero. In essence, the non-zero entries in the subgradient indicate places where the two solutions contradict. Note, to compute the subgradient for this SPO approach, the optimization problem v * (2  ) needs to be solved for each training instance, while v * () can be precomputed and cached. Moreover, one training instance typically contains multiple predictions. For example, if we consider a 0-1 knapsack problem with 10 items, then one training instance always contains 10 value predictions, one for each item. Furthermore, the dataset may contain thousands of training instances of 10 values each. Hence, one iteration over the training data (one epoch) requires solving hundreds of knapsack problems. As an ML model is trained over several epochs, clearly the training process is computationally expensive.\n",
      "6\n",
      "Combinatorial problems and scaling up We observe that the SPO approach and its corresponding loss function places no restriction on the type of oracle v * () used. Given that our target task is to minimize the regret of the combinatorial problem, an oracle that solves the combinatorial optimisation problem is the most natural choice. We call this approach SPO-full. Weaker oracles Repeatedly solving combinatorial problems is computationally expensive. Hence for large and hard problems, it is necessary to look at ways to reduce the solving time. As there is no restriction on the oracle used, we consider using weaker oracles during training. NP-hard problems that have a polynomial (bounded) approximation algorithm could use the approximation algorithm in the loss as a proxy instead. For example, in case of knapsack, the greedy algorithm (Dantzig 1957) . For mixed integer programming (MIP) formulations, a natural weaker oracle to use is the continuous relaxation of the problem. While disregarding the discrete part, relaxations can often identify what part of the problem is trivial (variable assignments close to 0 or 1) from what part is non-trivial. For example for knapsack, the continuous relaxation leads to very similar solutions compared to the greedy algorithm. Note that we always use the same oracle for v * () and v * (2  ) when computing the loss. We call the approach of using the continuous relaxation as oracle v * () SPO-relax. In case of weak MIP relaxations, one can also use a cutting plane algorithm in the root node and use the resulting tighter relaxation thereof (Ferber et al. 2019) . Other weaker oracles could also be used, for example setting a time-limit on an any-time solver and using the best solution found, or a node-limit on search algorithms. In case of mixed integer programming, we can also set a gap tolerance, which means the solver does not have to prove optimality. We call this SPO-gap. For stability of the learning, it is recommended that the solution returned by the oracle does not vary much when called with (near) identical input. Apart from changing what is being solved, we also investigate ways to warmstart the learning, and to warmstart across solver calls: Warmstarting the learning We consider warmstarting the learning by transfer learning (Pratt and Jennings 1996) , that is, to train the model with an easy to compute loss function, and then continue training it with a more difficult one. In our case, we can pre-train the model using MSE as loss, which means the predictions will already be more informed when we start using an SPO loss afterwards. More elaborate learning schemes are possible, such as curriculum learning (Thrun and Pratt 2012; Pratt and Jennings 1996) where we gradually move from easier to harder to compute loss functions, e.g. by moving from SPO-relax to SPO-gap for decreasing gaps to SPO-full. As we will see later, this is not needed for the cases we studied. Warmstarting the solving When computing the loss, we must solve both v * () using the true values , and v * (2). Furthermore, we know that an optimal solution to v * () is also a valid (but potentially suboptimal) solution to v * (2  ) as only the coefficients of the objective differ. Furthermore, if this is an optimal solution to the latter than we would achieve 0-regret, hence we can expect the solution of v * () to be of decent quality for v * (2  ) too. We hence want to aide the solving of v * (2  ) by using the optimal solution of v * (). One way to do this for CP solvers is solution-guided search (Demirovc, Chu, and Stuckey 2018) . For MIP/LP we can use warmstart-ing (Yildirim and Wright 2002; Zeilinger, Jones, and Morari 2011) , that is, to use the previous solution as starting point for MIP. In case of linear programming (and hence the relaxation), we can reuse the basis of the solution. An alternative is to use the true solution to compute a bound on the objective function. Indeed, as the solution to S = v * () is valid for v * (2  ) and has an objective value of f (S, (2  )). Hence, we can use this as a bound on the objective and potentially cut away a large part of the search space. While the true solutions v * () can be cached, we must compute this solution once for each training instance (x, ) i , which may already take significant time for large problems. We observe that only the objective changes between calls to the oracle v * , and hence any previously computed solution is also a candidate solution for the other calls. We can hence use warmstarting for any solver call after the first, and from any previously computed solution so far.\n",
      "7\n",
      "Experimental Evaluation We consider three types of combinatorial problems: unweighted and weighted knapsack and energy-cost aware scheduling. Below we briefly discuss the problem formulations: Unweighted/weighted knapsack problem The knapsack problem can be formalized as argmax X V X s.t. W X  c. The values V will be predicted from data and weights W and capacity c are given. In the unweighted knapsack, all weights W are 1 and the problem is polynomial time solvable. Weighted knapsacks are NP-hard and it is known that the computational difficulty increases with the correlation between weights and values (Pisinger 2005) . We generated mildly correlated knapsacks as follows: for each of the 48 half-hour slots we assign a weight w i by sampling from the set {3, 5, 7}, then we multiply each profit value v i by its corresponding weight and include some randomness by adding Gaussian noise   N(0, 25) to each v i before multiplying by weight. In the unweighted case, we consider 9 different capacity values c, namely from 5 to 45 increasing by 5. For the weighted knapsack experiment, we consider 7 different capacity values from 30 to 210 increasing by 30. Energy-cost aware scheduling It is a resourceconstrained job scheduling problem where the goal is to minimize (predicted) energy cost, it is described in the CSPLib as problem 059 (Simonis et al. ) . In summary, we are given a number of machines and have to schedule a given number of tasks, where each task has a duration, an earliest start and a latest end, resource requirement and a power usage. Each machine has a resource capacity constraint. We omit startup/shutdown costs. No task is allowed to stretch over midnight between two days and cannot be interrupted once started, nor migrate to another machine. Time is discretized in t timeslots and a schedule has to be made over all timeslots at once. For each timeslot, a (predicted) energy cost is given and the objective is to minimize the total energy cost of running the tasks on the machines. We consider two variants of the problem: easy instances consisting of 30 minute timeslots (e.g. 48 timeslots per day), and hard instances as used in the ICON energy challenge consisting of 5 minute timeslots (288 timeslots per day). The easy instances have 3 machines and respectively 10, 15 and 20 tasks. The hard instances each have 10 machines and 200 tasks. Data Our data is drawn from the Irish Single Electricity Market Operator (SEMO) (Ifrim, OSullivan, and Simonis 2012) . This dataset consists of historical energy price data at 30-minute intervals starting from Midnight 1st November, 2011 to 31st December, 2013. Each instance of the data has calendar attributes; day-ahead estimates of weather characteristics; SEMO day-ahead forecasted energy-load, windenergy production and prices; and actual wind-speed, temperature, CO 2 intensity and price. Of the actual attributes, we keep only the actual price, and use it as a target for prediction. For the hard scheduling instances, each 5-minute timeslots have the same price as the 30-minute timeslot it belongs to, following the ICON challenge. Experimental setup For all our experiments, we use a linear model without any hidden layer as the underlying predictive model. Note, the SPO approach is a model-free approach and it is compatible wih any deep neural network; but earlier work (Ifrim, OSullivan, and Simonis 2012) showed accuracy in predictions is not effective for the downstream optimization task. For the experiments, we divide our data into three sets: training (70%), validation (10%) and test (20%), and evaluate the performance by measuring regret on the test set. Training, validation and test data covers 552, 60 and 177 days of energy data respectively. Our model is trained by batch gradient descent, where each batch corresponds to one day, that is, 48 consecutive training instances namely one for each half hour of that day. This batch together forms one set of parameters of one optimisation instance, e.g. knapsack values or schedule costs. The learning rate and momentum for each model are selected through a grid search based on the regret on the validation dataset. The best combination of parameters is then used in the experiments shown. Solving the knapsack instances takes sub-second time per optimisation instance, solving the easy scheduling problems takes 0.1 to 2 seconds per optimisation instance and for the hard scheduling problems solving just the relaxation already costs 30 to 150 seconds per optimisation instance. For the latter, this means that merely evaluating regret on the test-set of 177 optimisation instances takes about 3 hours. With 552 training instances, one epoch of training requires 9 hours. For all experiments except the hard instances, we repeat it 10 times and report on the mean and standard deviation. We use the Gurobi optimization-solver for solving the combinatorial problems, the nn module in Pytorch to implement the predictive models and the optim module in Pytorch for training the models with corresponding loss functions. Experiments were run on Intel(R) Xeon(R) CPU E3-1225 v5 @ 3.30GHz processors with 32GB memory 2 .\n",
      "8\n",
      "RQ1: exact versus weaker oracles The first research question is what the loss in accuracy of solving the full discrete problems (SPO-full) versus solving only the relaxation (SPO-relax) during training is. Together with this, we look at what the gain is in terms of reductionin-regret over time. We visualise both through the learning curves as evaluated on the test set. For all methods, we compute the exact regret on the test-set, e.g. by fully solving the instances with the predictions. Figure 2 shows the learning curves over epochs, where one epoch is one iteration over all instances of the training data, for three problem instances. We also include the regret of MSE-r as baseline. In all three case we see that MSE-r is worse, and stagnates or slightly decreases in performance over the epochs. This validates the use of MSE-r where the best epoch is chosen retrospectively based on a validation set. It also validates that the SPO-surrogate loss function captures the essence of the intended loss, namely regret. We also see, surprisingly, that SPO-relax achieves very similar performance to SPO-full on all problem instances in our experiments. This means that even though SPO can reason over exact discrete solutions, reasoning over these continuous relaxation solutions is sufficient for the loss function to guide the learning to where it matters. The real advantage of SPO-relax over SPO-full is evident from Figure 3 . Here we present the test regret not against\n",
      "9\n",
      "Instance Baseline MSE-warmstart Warmstart from earlier basis 1 6.5 (1.5) sec 8 (0.5) sec 1.5 (0.2) sec 2 7 (1.5) sec 6 (1.0) sec 1 (0.2) sec 3 10 (0.5) sec 12 (1.0) sec 2.5 (0.1) sec show SPO-relax runs, and hence converges, much quicker in time than SPO. This is thanks to the fact that solving the continuous relaxation can be done in polynomial time while the discrete problem is worst-case exponential. SPO-relax is slower than MSE-r but the difference in quality clearly justifies it. In the subsequent experiments, we will use SPO-relax only.\n",
      "Learning_a_Lexicon_and_Translation_Model_from_Phoneme_Lattices\n",
      "OUT/Learning_a_Lexicon_and_Translation_Model_from_Phoneme_Lattices/\n",
      "../papers-xmls/Learning_a_Lexicon_and_Translation_Model_from_Phoneme_Lattices.tei.xml\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-60139cd8bb02>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdivs_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheadings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0;31m#make divs_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-6a065d75466e>\u001b[0m in \u001b[0;36mmake_d\u001b[0;34m(divs_text, headings, maxx)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdivs_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t_now\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mmax_t_now\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
